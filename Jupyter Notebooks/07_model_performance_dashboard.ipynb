{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db481b1b",
   "metadata": {},
   "source": [
    "# LLM Screening Performance Dashboard\n",
    "\n",
    "Interactive visualisation comparing model performance across evaluation runs.\n",
    "\n",
    "- **Left Panel**: Model rankings sorted by F1 score\n",
    "- **Right Panel**: Confusion matrix for selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25ec234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q ipywidgets pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0a7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb37ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Run 1 models: ['Mistral Zero-Shot', 'Mistral CoT', 'Llama 3.2 Zero-Shot', 'Llama 3.2 CoT']\n",
      "Run 2 models: ['Mistral Zero-Shot', 'Mistral CoT', 'Llama 3.2 Zero-Shot', 'Llama 3.2 CoT']\n"
     ]
    }
   ],
   "source": [
    "# Load and process evaluation data\n",
    "DATA_DIR = Path.cwd().parent / \"Data\" if not (Path.cwd() / \"Data\").exists() else Path.cwd() / \"Data\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "\n",
    "# Define file mappings for each run\n",
    "RUN_FILES = {\n",
    "    'Run 1': {\n",
    "        'Mistral Zero-Shot': 'eval_mistral_zero_shot_20260115_231802.csv',\n",
    "        'Mistral CoT': 'eval_mistral_cot_20260116_003208.csv',\n",
    "        'Llama 3.2 Zero-Shot': 'eval_llama3.2_zero_shot_20260115_193605.csv',\n",
    "        'Llama 3.2 CoT': 'eval_llama3.2_cot_20260115_215209.csv'\n",
    "    },\n",
    "    'Run 2': {\n",
    "        'Mistral Zero-Shot': 'eval_mistral_zero_shot_20260116_050656.csv',\n",
    "        'Mistral CoT': 'eval_mistral_cot_20260116_073058.csv',\n",
    "        'Llama 3.2 Zero-Shot': 'eval_llama3.2_zero_shot_20260116_025453.csv',\n",
    "        'Llama 3.2 CoT': 'eval_llama3.2_cot_20260116_041136.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_eval_data(run_name, model_name):\n",
    "    \"\"\"Load evaluation data for a specific run and model.\"\"\"\n",
    "    filepath = RESULTS_DIR / RUN_FILES[run_name][model_name]\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Handle column name differences between runs\n",
    "    if 'label' in df.columns and 'true_label' not in df.columns:\n",
    "        df['true_label'] = df['label']\n",
    "    \n",
    "    # Convert predictions to binary - handle both numeric and text formats\n",
    "    def convert_prediction(x):\n",
    "        if pd.isna(x):\n",
    "            return 0  # Default to exclude for missing predictions\n",
    "        if isinstance(x, (int, float)):\n",
    "            return int(x)\n",
    "        return 1 if 'include' in str(x).lower() else 0\n",
    "    \n",
    "    df['pred_binary'] = df['prediction'].apply(convert_prediction)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def compute_metrics(run_name):\n",
    "    \"\"\"Compute metrics for all models in a given run.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name in RUN_FILES[run_name].keys():\n",
    "        df = load_eval_data(run_name, model_name)\n",
    "        \n",
    "        y_true = df['true_label']\n",
    "        y_pred = df['pred_binary']\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'F1 Score': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'N': len(df)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Pre-compute metrics for both runs\n",
    "METRICS = {\n",
    "    'Run 1': compute_metrics('Run 1'),\n",
    "    'Run 2': compute_metrics('Run 2')\n",
    "}\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Run 1 models: {list(RUN_FILES['Run 1'].keys())}\")\n",
    "print(f\"Run 2 models: {list(RUN_FILES['Run 2'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63b4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Color scheme\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',\n",
    "    'secondary': '#A23B72',\n",
    "    'accent': '#F18F01',\n",
    "    'success': '#28a745',\n",
    "    'danger': '#dc3545',\n",
    "    'light_bg': '#f8f9fa',\n",
    "    'dark_text': '#2c3e50',\n",
    "    'muted': '#6c757d'\n",
    "}\n",
    "\n",
    "MODEL_COLORS = {\n",
    "    'Mistral Zero-Shot': '#2E86AB',\n",
    "    'Mistral CoT': '#5BA3C6',\n",
    "    'Llama 3.2 Zero-Shot': '#A23B72',\n",
    "    'Llama 3.2 CoT': '#C66B9A'\n",
    "}\n",
    "\n",
    "def create_ranking_chart(ax, metrics_df, run_name):\n",
    "    \"\"\"Create a horizontal bar chart showing F1 rankings.\"\"\"\n",
    "    ax.clear()\n",
    "    \n",
    "    # Sort by F1 (already sorted, but ensure order for plotting)\n",
    "    df = metrics_df.sort_values('F1 Score', ascending=True)\n",
    "    \n",
    "    # Create bars\n",
    "    y_pos = np.arange(len(df))\n",
    "    colors = [MODEL_COLORS.get(m, COLORS['primary']) for m in df['Model']]\n",
    "    \n",
    "    bars = ax.barh(y_pos, df['F1 Score'], color=colors, edgecolor='white', linewidth=1.5, height=0.6)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, f1, prec, rec) in enumerate(zip(bars, df['F1 Score'], df['Precision'], df['Recall'])):\n",
    "        # F1 score on the bar\n",
    "        ax.text(bar.get_width() - 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{f1:.1%}', ha='right', va='center', fontweight='bold', \n",
    "                fontsize=14, color='white')\n",
    "        # Precision/Recall annotation\n",
    "        ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'P: {prec:.1%} | R: {rec:.1%}', ha='left', va='center',\n",
    "                fontsize=10, color=COLORS['muted'])\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(df['Model'], fontsize=12, fontweight='500')\n",
    "    ax.set_xlim(0, 1.30)\n",
    "    ax.set_xlabel('F1 Score', fontsize=12, fontweight='500', color=COLORS['dark_text'])\n",
    "    ax.set_title(f'Model Rankings ({run_name})', fontsize=16, fontweight='bold', \n",
    "                 color=COLORS['dark_text'], pad=15)\n",
    "    \n",
    "    # Add rank numbers on the right side\n",
    "    for i, (idx, row) in enumerate(df.iloc[::-1].iterrows()):\n",
    "        rank = len(df) - i\n",
    "        ax.text(1.22, i, f'#{rank}', ha='center', va='center', \n",
    "                fontsize=11, fontweight='bold', color=COLORS['accent'],\n",
    "                bbox=dict(boxstyle='circle,pad=0.3', facecolor='white', edgecolor=COLORS['accent']))\n",
    "    \n",
    "    # Clean up spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(left=False)\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "\n",
    "def create_confusion_matrix(ax, run_name, model_name):\n",
    "    \"\"\"Create a styled confusion matrix.\"\"\"\n",
    "    ax.clear()\n",
    "    \n",
    "    # Load data and compute confusion matrix\n",
    "    df = load_eval_data(run_name, model_name)\n",
    "    y_true = df['true_label']\n",
    "    y_pred = df['pred_binary']\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_pct = cm.astype('float') / cm.sum() * 100\n",
    "    \n",
    "    # Create custom colormap\n",
    "    cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(cm, annot=False, cmap=cmap, ax=ax, cbar=False,\n",
    "                linewidths=3, linecolor='white', square=True)\n",
    "    \n",
    "    # Add custom annotations\n",
    "    labels = [['TN', 'FP'], ['FN', 'TP']]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            count = cm[i, j]\n",
    "            pct = cm_pct[i, j]\n",
    "            label = labels[i][j]\n",
    "            \n",
    "            # Determine text color based on cell value\n",
    "            text_color = 'white' if count > cm.max() * 0.5 else COLORS['dark_text']\n",
    "            \n",
    "            ax.text(j + 0.5, i + 0.35, label, ha='center', va='center',\n",
    "                   fontsize=14, fontweight='bold', color=text_color, alpha=0.7)\n",
    "            ax.text(j + 0.5, i + 0.55, f'{count:,}', ha='center', va='center',\n",
    "                   fontsize=20, fontweight='bold', color=text_color)\n",
    "            ax.text(j + 0.5, i + 0.75, f'({pct:.1f}%)', ha='center', va='center',\n",
    "                   fontsize=11, color=text_color, alpha=0.8)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='500', color=COLORS['dark_text'], labelpad=10)\n",
    "    ax.set_ylabel('True Label', fontsize=12, fontweight='500', color=COLORS['dark_text'], labelpad=10)\n",
    "    ax.set_xticklabels(['Exclude', 'Include'], fontsize=11)\n",
    "    ax.set_yticklabels(['Exclude', 'Include'], fontsize=11, rotation=0)\n",
    "    \n",
    "    # Title with model color\n",
    "    model_color = MODEL_COLORS.get(model_name, COLORS['primary'])\n",
    "    ax.set_title(f'Confusion Matrix\\n{model_name}', fontsize=14, fontweight='bold',\n",
    "                color=model_color, pad=15)\n",
    "    \n",
    "    # Add metrics summary below\n",
    "    metrics = METRICS[run_name]\n",
    "    row = metrics[metrics['Model'] == model_name].iloc[0]\n",
    "    \n",
    "    metrics_text = f\"Accuracy: {row['Accuracy']:.1%}  |  Precision: {row['Precision']:.1%}  |  Recall: {row['Recall']:.1%}  |  F1: {row['F1 Score']:.1%}\"\n",
    "    ax.text(1, -0.18, metrics_text, ha='center', va='top', transform=ax.transAxes,\n",
    "           fontsize=10, color=COLORS['muted'], style='italic')\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6670a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10bf22c761242a18fd18bc80da87399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='text-align: center; color: #2c3e50; font-family: Segoe UI, sans-serif; mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create interactive dashboard\n",
    "\n",
    "# Widgets\n",
    "run_dropdown = widgets.Dropdown(\n",
    "    options=['Run 1', 'Run 2'],\n",
    "    value='Run 2',\n",
    "    description='Evaluation Run:',\n",
    "    style={'description_width': '100px'},\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=list(RUN_FILES['Run 2'].keys()),\n",
    "    value='Mistral Zero-Shot',\n",
    "    description='Model:',\n",
    "    style={'description_width': '100px'},\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "# Output area\n",
    "output = widgets.Output()\n",
    "\n",
    "def update_dashboard(run_name, model_name):\n",
    "    \"\"\"Update the entire dashboard.\"\"\"\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Create figure with landscape orientation\n",
    "        fig, (ax_ranking, ax_cm) = plt.subplots(1, 2, figsize=(14, 6), \n",
    "                                                 gridspec_kw={'width_ratios': [1.3, 1]})\n",
    "        fig.patch.set_facecolor('white')\n",
    "        \n",
    "        # Left: Rankings\n",
    "        create_ranking_chart(ax_ranking, METRICS[run_name], run_name)\n",
    "        \n",
    "        # Right: Confusion Matrix\n",
    "        create_confusion_matrix(ax_cm, run_name, model_name)\n",
    "        \n",
    "        plt.tight_layout(pad=3)\n",
    "        plt.show()\n",
    "\n",
    "def on_run_change(change):\n",
    "    \"\"\"Handle run dropdown change.\"\"\"\n",
    "    update_dashboard(run_dropdown.value, model_dropdown.value)\n",
    "\n",
    "def on_model_change(change):\n",
    "    \"\"\"Handle model dropdown change.\"\"\"\n",
    "    update_dashboard(run_dropdown.value, model_dropdown.value)\n",
    "\n",
    "# Link widgets to handlers\n",
    "run_dropdown.observe(on_run_change, names='value')\n",
    "model_dropdown.observe(on_model_change, names='value')\n",
    "\n",
    "# Layout\n",
    "controls = widgets.HBox(\n",
    "    [run_dropdown, model_dropdown],\n",
    "    layout=widgets.Layout(\n",
    "        justify_content='center',\n",
    "        padding='15px',\n",
    "        margin='10px 0'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Title\n",
    "title = widgets.HTML(\n",
    "    value=\"<h2 style='text-align: center; color: #2c3e50; font-family: Segoe UI, sans-serif; margin-bottom: 5px;'>\" +\n",
    "          \"ðŸ”¬ LLM Screening Performance Dashboard</h2>\" +\n",
    "          \"<p style='text-align: center; color: #6c757d; font-size: 14px;'>\" +\n",
    "          \"Compare model performance across evaluation runs</p>\"\n",
    ")\n",
    "\n",
    "# Display dashboard\n",
    "dashboard = widgets.VBox([title, controls, output])\n",
    "display(dashboard)\n",
    "\n",
    "# Initial render\n",
    "update_dashboard('Run 2', 'Mistral Zero-Shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542a03e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dashboard saved to: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\model_performance_dashboard.html\n",
      "   File size: 184.9 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/juanx/Documents/LSE-UKHSA Project/model_performance_dashboard.html')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export visualization to HTML file\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def save_dashboard_html(run_name='Run 2', model_name='Mistral Zero-Shot'):\n",
    "    \"\"\"Save the dashboard as a standalone HTML file.\"\"\"\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax_ranking, ax_cm) = plt.subplots(1, 2, figsize=(16, 7), \n",
    "                                             gridspec_kw={'width_ratios': [1.3, 1]})\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_ranking_chart(ax_ranking, METRICS[run_name], run_name)\n",
    "    create_confusion_matrix(ax_cm, run_name, model_name)\n",
    "    \n",
    "    plt.tight_layout(pad=3)\n",
    "    \n",
    "    # Save figure to bytes\n",
    "    buffer = BytesIO()\n",
    "    fig.savefig(buffer, format='png', dpi=150, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    buffer.seek(0)\n",
    "    img_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Create HTML\n",
    "    html_content = f'''<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>LLM Screening Performance Dashboard</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            background: #f8f9fa;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            align-items: center;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 1400px;\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            box-shadow: 0 4px 20px rgba(0,0,0,0.1);\n",
    "            padding: 30px;\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #2c3e50;\n",
    "            text-align: center;\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .subtitle {{\n",
    "            color: #6c757d;\n",
    "            text-align: center;\n",
    "            font-size: 14px;\n",
    "            margin-bottom: 25px;\n",
    "        }}\n",
    "        .dashboard-img {{\n",
    "            width: 100%;\n",
    "            height: auto;\n",
    "            border-radius: 10px;\n",
    "        }}\n",
    "        .footer {{\n",
    "            text-align: center;\n",
    "            margin-top: 20px;\n",
    "            color: #888;\n",
    "            font-size: 12px;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>ðŸ”¬ LLM Screening Performance Dashboard</h1>\n",
    "        <p class=\"subtitle\">Evaluation Run: {run_name} | Confusion Matrix: {model_name}</p>\n",
    "        <img class=\"dashboard-img\" src=\"data:image/png;base64,{img_base64}\" alt=\"Dashboard\">\n",
    "        <div class=\"footer\">\n",
    "            LSE-UKHSA Systematic Review Screening Project\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "    \n",
    "    # Save to root folder\n",
    "    output_path = Path.cwd().parent / \"model_performance_dashboard.html\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"âœ… Dashboard saved to: {output_path}\")\n",
    "    print(f\"   File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "    return output_path\n",
    "\n",
    "# Save the dashboard\n",
    "save_dashboard_html('Run 2', 'Mistral Zero-Shot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
