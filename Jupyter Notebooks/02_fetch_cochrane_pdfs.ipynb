{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca35fdb",
   "metadata": {},
   "source": [
    "# 02: Fetch Cochrane PDFs via Wiley TDM API\n",
    "\n",
    "## Summary\n",
    "This notebook downloads Cochrane review PDFs using the Wiley Text and Data Mining (TDM) API. The PDFs contain structured reference sections that properly categorize included vs. excluded studies - information not available in PubMed XML.\n",
    "\n",
    "**Pipeline Position:** Third notebook - downloads source PDFs for extracting categorized references.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads Cochrane review metadata from PubMed (including DOIs)\n",
    "2. Downloads PDFs for each review via Wiley TDM API\n",
    "3. Implements rate limiting and error handling for batch downloads\n",
    "4. Saves PDFs to local storage for text extraction\n",
    "\n",
    "**Input:** `Data/cochrane_pubmed_abstracts.csv`\n",
    "\n",
    "**Output:** `Data/cochrane_pdfs/*.pdf`\n",
    "\n",
    "**Requirements:**\n",
    "- Wiley TDM API token in `.env` file (WILEY_TEXT_AND_DATA_MINING_TOKEN)\n",
    "- Institutional IP access to Cochrane Library content\n",
    "\n",
    "**Important:** PDFs contain proprietary content and must NOT be uploaded to GitHub. They are excluded via .gitignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for PDF download and processing\n",
    "%pip install -q wiley-tdm python-dotenv pandas biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf40ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment and load credentials\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \".env\").exists() else notebook_dir.parent\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "WILEY_TDM_TOKEN = os.getenv(\"WILEY_TEXT_AND_DATA_MINING_TOKEN\")\n",
    "os.environ['TDM_API_TOKEN'] = WILEY_TDM_TOKEN or \"\"\n",
    "\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "PDF_DIR = DATA_DIR / \"cochrane_pdfs\"\n",
    "ABSTRACTS_CSV = DATA_DIR / \"cochrane_pubmed_abstracts.csv\"\n",
    "\n",
    "print(f\"Wiley TDM Token loaded: {'✓' if WILEY_TDM_TOKEN else '✗'}\")\n",
    "print(f\"PDF output directory: {PDF_DIR}\")\n",
    "\n",
    "if not WILEY_TDM_TOKEN:\n",
    "    raise ValueError(\"WILEY_TEXT_AND_DATA_MINING_TOKEN not set in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cochrane reviews and fetch DOIs from PubMed\n",
    "from Bio import Entrez\n",
    "import time\n",
    "import re\n",
    "\n",
    "Entrez.email = os.getenv(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.getenv(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "abstracts = pd.read_csv(ABSTRACTS_CSV, dtype={\"pmid\": str})\n",
    "print(f\"Loaded {len(abstracts):,} Cochrane reviews\")\n",
    "\n",
    "def get_doi_from_pmid(pmid):\n",
    "    \"\"\"Fetch DOI for a PubMed ID.\"\"\"\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\", retmode=\"xml\")\n",
    "        xml = handle.read()\n",
    "        if isinstance(xml, bytes):\n",
    "            xml = xml.decode('utf-8')\n",
    "        handle.close()\n",
    "        doi_match = re.search(r'<ArticleId IdType=\"doi\">([^<]+)</ArticleId>', xml)\n",
    "        return doi_match.group(1) if doi_match else None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Test with a sample\n",
    "sample_pmid = abstracts['pmid'].iloc[0]\n",
    "sample_doi = get_doi_from_pmid(sample_pmid)\n",
    "print(f\"Sample: PMID {sample_pmid} -> DOI {sample_doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02baa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wiley TDM client and test download\n",
    "from wiley_tdm import TDMClient\n",
    "\n",
    "PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "tdm = TDMClient(download_dir=str(PDF_DIR))\n",
    "\n",
    "# Test with a single download\n",
    "if sample_doi:\n",
    "    print(f\"Testing download for DOI: {sample_doi}\")\n",
    "    result = tdm.download_pdf(sample_doi)\n",
    "    print(f\"Result: {result}\")\n",
    "else:\n",
    "    print(\"Could not get DOI for test download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch download PDFs with progress tracking\n",
    "import csv\n",
    "\n",
    "# Track DOIs and download status\n",
    "DOI_CACHE_FILE = DATA_DIR / \"cochrane_dois.csv\"\n",
    "DOWNLOAD_LOG = DATA_DIR / \"pdf_download_log.csv\"\n",
    "\n",
    "def batch_get_dois(pmids, batch_size=50, sleep_time=0.5):\n",
    "    \"\"\"Fetch DOIs for a list of PMIDs in batches.\"\"\"\n",
    "    results = {}\n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch = pmids[i:i+batch_size]\n",
    "        for pmid in batch:\n",
    "            doi = get_doi_from_pmid(pmid)\n",
    "            results[pmid] = doi\n",
    "            time.sleep(sleep_time)\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"  DOI progress: {i + len(batch)}/{len(pmids)}\")\n",
    "    return results\n",
    "\n",
    "# Load or create DOI cache\n",
    "if DOI_CACHE_FILE.exists():\n",
    "    doi_df = pd.read_csv(DOI_CACHE_FILE, dtype={\"pmid\": str})\n",
    "    dois = dict(zip(doi_df['pmid'], doi_df['doi']))\n",
    "    print(f\"Loaded {len(dois):,} DOIs from cache\")\n",
    "else:\n",
    "    print(\"Fetching DOIs from PubMed (this will take a while)...\")\n",
    "    pmids = abstracts['pmid'].tolist()\n",
    "    dois = batch_get_dois(pmids)\n",
    "    doi_df = pd.DataFrame([(k, v) for k, v in dois.items()], columns=['pmid', 'doi'])\n",
    "    doi_df.to_csv(DOI_CACHE_FILE, index=False)\n",
    "    print(f\"Saved {len(dois):,} DOIs to cache\")\n",
    "\n",
    "valid_dois = {k: v for k, v in dois.items() if v is not None}\n",
    "print(f\"\\nReviews with valid DOIs: {len(valid_dois):,} ({100*len(valid_dois)/len(dois):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a25ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDFs in batches (run this cell multiple times if interrupted)\n",
    "import time\n",
    "\n",
    "# Check which PDFs already exist\n",
    "existing_pdfs = set(p.stem for p in PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Already downloaded: {len(existing_pdfs)} PDFs\")\n",
    "\n",
    "# Filter to DOIs we haven't downloaded yet\n",
    "to_download = [(pmid, doi) for pmid, doi in valid_dois.items() \n",
    "               if doi.replace(\"/\", \"-\") not in existing_pdfs and doi.replace(\"/\", \"%2F\") not in existing_pdfs]\n",
    "\n",
    "print(f\"Remaining to download: {len(to_download)}\")\n",
    "\n",
    "# Download with rate limiting (adjust MAX_DOWNLOADS for testing)\n",
    "MAX_DOWNLOADS = 100  # Set to None for all\n",
    "downloads = to_download[:MAX_DOWNLOADS] if MAX_DOWNLOADS else to_download\n",
    "\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for i, (pmid, doi) in enumerate(downloads):\n",
    "    try:\n",
    "        result = tdm.download_pdf(doi)\n",
    "        if result == \"Success\":\n",
    "            success_count += 1\n",
    "        else:\n",
    "            fail_count += 1\n",
    "    except Exception as e:\n",
    "        fail_count += 1\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Progress: {i+1}/{len(downloads)} | Success: {success_count} | Failed: {fail_count}\")\n",
    "    \n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "print(f\"\\nDownload complete: {success_count} succeeded, {fail_count} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of downloaded PDFs\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "total_size_mb = sum(p.stat().st_size for p in pdf_files) / (1024 * 1024)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total PDFs downloaded: {len(pdf_files):,}\")\n",
    "print(f\"Total storage used: {total_size_mb:.1f} MB\")\n",
    "print(f\"Average PDF size: {total_size_mb/len(pdf_files)*1024:.0f} KB\" if pdf_files else \"N/A\")\n",
    "print(f\"\\nNext step: Run notebook 03 to extract categorized references from PDFs\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
