{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55fe0ca",
   "metadata": {},
   "source": [
    "# 06: Evaluate LLMs for Systematic Review Screening\n",
    "\n",
    "## Objective\n",
    "Evaluate whether LLMs can correctly determine if a paper should be **included** or **excluded** from a Cochrane systematic review.\n",
    "\n",
    "## Task\n",
    "Given:\n",
    "- **Review context** (title + abstract of the Cochrane review - defines the screening criteria)\n",
    "- **Paper abstract** (the candidate study being screened)\n",
    "\n",
    "Predict: INCLUDE or EXCLUDE\n",
    "\n",
    "## Dataset\n",
    "- **Validation set**: `ground_truth_validation_dataset.csv` - All Cochrane groups with `cochrane_group` column for filtering\n",
    "\n",
    "## Prompt Types\n",
    "1. **Zero-shot** - Direct question without reasoning\n",
    "2. **Chain-of-thought (CoT)** - Ask the LLM to reason step-by-step before deciding\n",
    "\n",
    "## Models (10 Models - All Local via Ollama)\n",
    "\n",
    "### General-Purpose Models\n",
    "| Model | Size | Description |\n",
    "|-------|------|-------------|\n",
    "| **Llama 3.2** | 3B | Meta's efficient baseline model |\n",
    "| **Llama 3.1 8B** | 8B | Stronger instruction-following |\n",
    "| **Mistral 7B** | 7B | Strong general-purpose model |\n",
    "| **Mistral Nemo 12B** | 12B | Newer architecture, strong reasoning |\n",
    "| **Qwen 2.5 7B** | 7B | Top benchmarks, rivals GPT-3.5 |\n",
    "| **Gemma 2 9B** | 9B | Google's latest, excellent classification |\n",
    "| **Phi-3 Medium** | 14B | Microsoft's efficient model |\n",
    "\n",
    "### Biomedical-Specialized Models\n",
    "| Model | Size | Description |\n",
    "|-------|------|-------------|\n",
    "| **OpenBioLLM-8B** | 8B | Llama-3 fine-tuned, outperforms GPT-3.5 on medical |\n",
    "| **BioMistral 7B** | 7B | Mistral fine-tuned on PubMed Central |\n",
    "| **Meditron 7B** | 7B | Fine-tuned on medical guidelines & PubMed |\n",
    "\n",
    "## Output\n",
    "- `Data/results/eval_*.csv` - Predictions with LLM reasoning saved\n",
    "- Metrics: Accuracy, Precision, Recall, F1, Sensitivity, Specificity\n",
    "\n",
    "**IMPORTANT:** All inference is local via Ollama - no data sent to external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc949b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for local LLM inference\n",
    "%pip install -q ollama pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98ba0d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation set: 41,692 examples\n",
      "\n",
      "Label distribution:\n",
      "  Included: 14,738\n",
      "  Excluded: 26,954\n",
      "\n",
      "Unique reviews: 1,228\n",
      "\n",
      "Cochrane groups available (filter with cochrane_group column):\n",
      "cochrane_group\n",
      "Acute Respiratory Infections    11455\n",
      "Tobacco Addiction               10198\n",
      "Infectious Diseases              8516\n",
      "Drugs and Alcohol                6754\n",
      "Public Health                    4089\n",
      "STI                               680\n"
     ]
    }
   ],
   "source": [
    "# Setup and load data\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GROUND_TRUTH_CSV = DATA_DIR / \"ground_truth_validation_dataset.csv\"\n",
    "\n",
    "# Load validation set (all Cochrane groups, filterable by cochrane_group column)\n",
    "ground_truth = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "print(f\"Loaded validation set: {len(ground_truth):,} examples\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Included: {(ground_truth['label'] == 1).sum():,}\")\n",
    "print(f\"  Excluded: {(ground_truth['label'] == 0).sum():,}\")\n",
    "print(f\"\\nUnique reviews: {ground_truth['review_doi'].nunique():,}\")\n",
    "print(f\"\\nCochrane groups available (filter with cochrane_group column):\")\n",
    "print(ground_truth['cochrane_group'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec17e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available local models:\n",
      "  - cniongolo/biomistral:latest\n",
      "  - koesn/llama3-openbiollm-8b:latest\n",
      "  - mistral:latest\n",
      "  - llama3.2:latest\n"
     ]
    }
   ],
   "source": [
    "# Check available Ollama models\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Available local models:\")\n",
    "    if hasattr(models, 'models'):\n",
    "        for model in models.models:\n",
    "            name = model.model if hasattr(model, 'model') else str(model)\n",
    "            print(f\"  - {name}\")\n",
    "    elif isinstance(models, dict) and 'models' in models:\n",
    "        for model in models['models']:\n",
    "            name = model.get('name', model.get('model', str(model)))\n",
    "            print(f\"  - {name}\")\n",
    "    else:\n",
    "        print(f\"  Models: {models}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d08ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE ZERO-SHOT PROMPT:\n",
      "============================================================\n",
      "You are a systematic review screening assistant. Your task is to determine whether a candidate paper should be INCLUDED or EXCLUDED from a specific Cochrane systematic review.\n",
      "\n",
      "=== COCHRANE REVIEW ===\n",
      "Title: Acupuncture for smoking cessation.\n",
      "\n",
      "Abstract/Objective: Acupuncture is promoted as a treatment for smoking cessation, and is believed to reduce withdrawal symptoms. The objective of this review is to determine the effectiveness of acupuncture in smoking cessation in comparison with: a) sham acupuncture b) other interventions c) no intervention. We searched the Cochrane Tobacco Addiction Group trials register, Medline, PsycLit, Dissertation Abstracts, Health Planning and Administration, Social SciSearch, Smoking & Health, Embase, Biological Abstracts and DRUG. Randomised trials comparing a form of acupuncture with either sham acupuncture, another intervention or no intervention for smoking cessation. We extracted data in duplicate on the type of subjects, the nature of the acupuncture and control procedures, the outcome measures, method of randomisation, and completeness of follow-up. We assessed abstinence from smoking at the earliest time-point (before 6 weeks), at six months and at one year follow-up in patients smoking at baseline. We used the most rigorous definition of abstinence for each trial, and biochemically validated rates if available. Those lost to follow-up were counted as continuing to smoke. Where appropriate, we performed meta-analysis using a fixed effec...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prompt Templates - Include Review Context!\n",
    "# =============================================================================\n",
    "\n",
    "ZERO_SHOT_PROMPT = \"\"\"You are a systematic review screening assistant. Your task is to determine whether a candidate paper should be INCLUDED or EXCLUDED from a specific Cochrane systematic review.\n",
    "\n",
    "=== COCHRANE REVIEW ===\n",
    "Title: {review_title}\n",
    "\n",
    "Abstract/Objective: {review_abstract}\n",
    "\n",
    "=== CANDIDATE PAPER ===\n",
    "Title: {paper_title}\n",
    "\n",
    "Abstract: {paper_abstract}\n",
    "\n",
    "=== TASK ===\n",
    "Based on the review's objectives and inclusion criteria, should this paper be INCLUDED or EXCLUDED?\n",
    "\n",
    "Respond with only: INCLUDE or EXCLUDE\"\"\"\n",
    "\n",
    "\n",
    "COT_PROMPT = \"\"\"You are a systematic review screening assistant. Your task is to determine whether a candidate paper should be INCLUDED or EXCLUDED from a specific Cochrane systematic review.\n",
    "\n",
    "=== COCHRANE REVIEW ===\n",
    "Title: {review_title}\n",
    "\n",
    "Abstract/Objective: {review_abstract}\n",
    "\n",
    "=== CANDIDATE PAPER ===\n",
    "Title: {paper_title}\n",
    "\n",
    "Abstract: {paper_abstract}\n",
    "\n",
    "=== TASK ===\n",
    "Think step by step:\n",
    "1. What is the review looking for? (population, intervention, outcomes)\n",
    "2. What does the candidate paper study?\n",
    "3. Does the paper match the review's criteria?\n",
    "\n",
    "After your reasoning, give your final answer on a new line as: DECISION: INCLUDE or DECISION: EXCLUDE\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(row: pd.Series, use_cot: bool = False) -> str:\n",
    "    \"\"\"Create prompt with review context and paper abstract.\"\"\"\n",
    "    template = COT_PROMPT if use_cot else ZERO_SHOT_PROMPT\n",
    "    return template.format(\n",
    "        review_title=str(row['review_title'])[:500],\n",
    "        review_abstract=str(row['review_abstract'])[:2000],\n",
    "        paper_title=str(row['paper_title'])[:300],\n",
    "        paper_abstract=str(row['paper_abstract'])[:2000]\n",
    "    )\n",
    "\n",
    "# Preview a prompt\n",
    "sample = ground_truth.iloc[0]\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ZERO-SHOT PROMPT:\")\n",
    "print(\"=\" * 60)\n",
    "print(create_prompt(sample, use_cot=False)[:1500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6a9781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Evaluation Functions - Save full reasoning\n",
    "# =============================================================================\n",
    "import re\n",
    "\n",
    "def extract_decision(response: str) -> int:\n",
    "    \"\"\"Extract INCLUDE (1) or EXCLUDE (0) from LLM response.\"\"\"\n",
    "    response_upper = response.upper()\n",
    "    \n",
    "    # Look for explicit DECISION: pattern first (CoT)\n",
    "    decision_match = re.search(r'DECISION:\\s*(INCLUDE|EXCLUDE)', response_upper)\n",
    "    if decision_match:\n",
    "        return 1 if decision_match.group(1) == 'INCLUDE' else 0\n",
    "    \n",
    "    # Fall back to last occurrence\n",
    "    include_pos = response_upper.rfind('INCLUDE')\n",
    "    exclude_pos = response_upper.rfind('EXCLUDE')\n",
    "    \n",
    "    if include_pos > exclude_pos:\n",
    "        return 1\n",
    "    elif exclude_pos > include_pos:\n",
    "        return 0\n",
    "    \n",
    "    return -1  # Could not determine\n",
    "\n",
    "\n",
    "def run_evaluation(model_name: str, data: pd.DataFrame, use_cot: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Run evaluation and save full LLM reasoning.\"\"\"\n",
    "    results = []\n",
    "    prompt_type = 'cot' if use_cot else 'zero_shot'\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model_name} ({prompt_type})\"):\n",
    "        prompt = create_prompt(row, use_cot=use_cot)\n",
    "        \n",
    "        try:\n",
    "            start = time.time()\n",
    "            response = ollama.generate(model=model_name, prompt=prompt)\n",
    "            elapsed = time.time() - start\n",
    "            response_text = response.get('response', '')\n",
    "            prediction = extract_decision(response_text)\n",
    "        except Exception as e:\n",
    "            response_text = f\"ERROR: {e}\"\n",
    "            prediction = -1\n",
    "            elapsed = 0\n",
    "        \n",
    "        results.append({\n",
    "            'review_doi': row['review_doi'],\n",
    "            'study_id': row['study_id'],\n",
    "            'label': row['label'],\n",
    "            'prediction': prediction,\n",
    "            'correct': prediction == row['label'],\n",
    "            'reasoning': response_text,  # Full LLM reasoning saved!\n",
    "            'response_time_sec': round(elapsed, 2)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ea031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 360,743 samples\n",
      "  Included: 124,119\n",
      "  Excluded: 236,624\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Run Llama 3.2 Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "MODEL_NAME = \"llama3.2\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Full evaluation on entire dataset\n",
    "SAMPLE_SIZE = None  # None = full dataset\n",
    "eval_data = ground_truth.sample(n=SAMPLE_SIZE, random_state=42) if SAMPLE_SIZE else ground_truth\n",
    "\n",
    "print(f\"Evaluating on {len(eval_data):,} samples\")\n",
    "print(f\"  Included: {(eval_data['label'] == 1).sum():,}\")\n",
    "print(f\"  Excluded: {(eval_data['label'] == 0).sum():,}\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['llama3.2_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.2_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['llama3.2_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.2_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Mistral Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_NAME = \"mistral\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['mistral_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['mistral_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run OpenBioLLM-8B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# State-of-the-art biomedical LLM based on Llama-3, fine-tuned on medical data\n",
    "# Outperforms GPT-3.5 on medical benchmarks (72.5% avg accuracy)\n",
    "\n",
    "MODEL_NAME = \"koesn/llama3-openbiollm-8b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['openbiollm_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_openbiollm_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['openbiollm_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_openbiollm_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run BioMistral Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Mistral 7B fine-tuned on PubMed Central biomedical literature\n",
    "# Designed specifically for biomedical NLP tasks\n",
    "\n",
    "MODEL_NAME = \"cniongolo/biomistral\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['biomistral_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_biomistral_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['biomistral_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_biomistral_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Llama 3.1 8B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Stronger than Llama 3.2, excellent instruction-following\n",
    "\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['llama3.1_8b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.1_8b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['llama3.1_8b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.1_8b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Qwen 2.5 7B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Top benchmarks, rivals GPT-3.5, strong reasoning\n",
    "\n",
    "MODEL_NAME = \"qwen2.5:7b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['qwen2.5_7b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_qwen2.5_7b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['qwen2.5_7b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_qwen2.5_7b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a666e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Gemma 2 9B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Google's latest, excellent for classification tasks\n",
    "\n",
    "MODEL_NAME = \"gemma2:9b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['gemma2_9b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_gemma2_9b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['gemma2_9b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_gemma2_9b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Phi-3 Medium 14B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Microsoft's efficient model, punches above its weight\n",
    "\n",
    "MODEL_NAME = \"phi3:medium\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['phi3_medium_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_phi3_medium_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['phi3_medium_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_phi3_medium_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edeffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Meditron 7B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Fine-tuned on medical guidelines & PubMed (biomedical specialized)\n",
    "\n",
    "MODEL_NAME = \"meditron:7b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['meditron_7b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_meditron_7b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['meditron_7b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_meditron_7b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb15790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Mistral Nemo 12B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Newer architecture, strong reasoning capabilities\n",
    "\n",
    "MODEL_NAME = \"mistral-nemo:12b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['mistral_nemo_12b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_nemo_12b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['mistral_nemo_12b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_nemo_12b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Compare Results\n",
    "# =============================================================================\n",
    "\n",
    "comparison_rows = []\n",
    "\n",
    "for run_name, results in all_results.items():\n",
    "    valid = results[results['prediction'] != -1]\n",
    "    if len(valid) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Compute metrics\n",
    "    tn = ((valid['label'] == 0) & (valid['prediction'] == 0)).sum()\n",
    "    tp = ((valid['label'] == 1) & (valid['prediction'] == 1)).sum()\n",
    "    fn = ((valid['label'] == 1) & (valid['prediction'] == 0)).sum()\n",
    "    fp = ((valid['label'] == 0) & (valid['prediction'] == 1)).sum()\n",
    "    \n",
    "    comparison_rows.append({\n",
    "        'model': run_name,\n",
    "        'n_samples': len(valid),\n",
    "        'accuracy': accuracy_score(valid['label'], valid['prediction']),\n",
    "        'precision': precision_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'recall': recall_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'f1': f1_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,  # Same as recall\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'avg_response_time': results['response_time_sec'].mean()\n",
    "    })\n",
    "\n",
    "comparison = pd.DataFrame(comparison_rows)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: ZERO-SHOT vs CHAIN-OF-THOUGHT\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# View Sample Reasoning\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE LLM REASONING (Chain-of-Thought)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show a correct and incorrect example\n",
    "cot_results = all_results.get('llama3.2_cot')\n",
    "if cot_results is not None:\n",
    "    correct = cot_results[cot_results['correct'] == True].iloc[0] if any(cot_results['correct']) else None\n",
    "    incorrect = cot_results[cot_results['correct'] == False].iloc[0] if any(~cot_results['correct']) else None\n",
    "    \n",
    "    if correct is not None:\n",
    "        print(\"\\n✓ CORRECT PREDICTION:\")\n",
    "        print(f\"  Label: {correct['label']} | Prediction: {correct['prediction']}\")\n",
    "        print(f\"  Reasoning:\\n{correct['reasoning'][:800]}...\")\n",
    "    \n",
    "    if incorrect is not None:\n",
    "        print(\"\\n✗ INCORRECT PREDICTION:\")\n",
    "        print(f\"  Label: {incorrect['label']} | Prediction: {incorrect['prediction']}\")\n",
    "        print(f\"  Reasoning:\\n{incorrect['reasoning'][:800]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Samples evaluated: {len(eval_data):,}\")\n",
    "print(f\"Prompt types: Zero-shot, Chain-of-thought\")\n",
    "print(f\"Total experiments: 10 models × 2 prompts = 20 runs\")\n",
    "\n",
    "print(f\"\\nGeneral-Purpose Models (7):\")\n",
    "print(\"  - Llama 3.2 (3B)\")\n",
    "print(\"  - Llama 3.1 8B\")\n",
    "print(\"  - Mistral 7B\")\n",
    "print(\"  - Mistral Nemo 12B\")\n",
    "print(\"  - Qwen 2.5 7B\")\n",
    "print(\"  - Gemma 2 9B\")\n",
    "print(\"  - Phi-3 Medium 14B\")\n",
    "\n",
    "print(f\"\\nBiomedical-Specialized Models (3):\")\n",
    "print(\"  - OpenBioLLM-8B\")\n",
    "print(\"  - BioMistral 7B\")\n",
    "print(\"  - Meditron 7B\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "print(\"  - eval_<model>_zero_shot_*.csv\")\n",
    "print(\"  - eval_<model>_cot_*.csv\")\n",
    "print(\"  - model_comparison.csv\")\n",
    "print(\"\\n✓ All inference was LOCAL via Ollama - no data sent to external APIs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
