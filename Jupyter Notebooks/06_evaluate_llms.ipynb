{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55fe0ca",
   "metadata": {},
   "source": [
    "# 06: Evaluate LLMs for Systematic Review Screening\n",
    "\n",
    "## Objective\n",
    "Evaluate whether LLMs can correctly determine if a paper should be **included** or **excluded** from a Cochrane systematic review.\n",
    "\n",
    "## Task\n",
    "Given:\n",
    "- **Review context** (title + abstract of the Cochrane review - defines the screening criteria)\n",
    "- **Paper abstract** (the candidate study being screened)\n",
    "\n",
    "Predict: INCLUDE or EXCLUDE\n",
    "\n",
    "## Dataset\n",
    "- **Validation set**: `ground_truth_validation_dataset.csv` - All Cochrane groups with `cochrane_group` column for filtering\n",
    "\n",
    "## Prompt Types\n",
    "1. **Zero-shot** - Direct question without reasoning\n",
    "2. **Chain-of-thought (CoT)** - Ask the LLM to reason step-by-step before deciding\n",
    "\n",
    "## Models (10 Models - All Local via Ollama)\n",
    "\n",
    "### General-Purpose Models\n",
    "| Model | Size | Description |\n",
    "|-------|------|-------------|\n",
    "| **Llama 3.2** | 3B | Meta's efficient baseline model |\n",
    "| **Llama 3.1 8B** | 8B | Stronger instruction-following |\n",
    "| **Mistral 7B** | 7B | Strong general-purpose model |\n",
    "| **Mistral Nemo 12B** | 12B | Newer architecture, strong reasoning |\n",
    "| **Qwen 2.5 7B** | 7B | Top benchmarks, rivals GPT-3.5 |\n",
    "| **Gemma 2 9B** | 9B | Google's latest, excellent classification |\n",
    "| **Phi-3 Medium** | 14B | Microsoft's efficient model |\n",
    "\n",
    "### Biomedical-Specialized Models\n",
    "| Model | Size | Description |\n",
    "|-------|------|-------------|\n",
    "| **OpenBioLLM-8B** | 8B | Llama-3 fine-tuned, outperforms GPT-3.5 on medical |\n",
    "| **BioMistral 7B** | 7B | Mistral fine-tuned on PubMed Central |\n",
    "| **Meditron 7B** | 7B | Fine-tuned on medical guidelines & PubMed |\n",
    "\n",
    "## Output\n",
    "- `Data/results/eval_*.csv` - Predictions with LLM reasoning saved\n",
    "- Metrics: Accuracy, Precision, Recall, F1, Sensitivity, Specificity\n",
    "\n",
    "**IMPORTANT:** All inference is local via Ollama - no data sent to external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc949b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for local LLM inference\n",
    "%pip install -q ollama pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba0d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation set: 12,778 examples (Public Health)\n",
      "\n",
      "Label distribution:\n",
      "  Included: 3,704\n",
      "  Excluded: 9,074\n",
      "\n",
      "Unique reviews: 75\n",
      "\n",
      "Note: Full dataset (~360K) preserved in ground_truth_all_categories.csv\n"
     ]
    }
   ],
   "source": [
    "# Setup and load data\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GROUND_TRUTH_CSV = DATA_DIR / \"ground_truth_validation_dataset.csv\"\n",
    "\n",
    "# Load validation set (all Cochrane groups, filterable by cochrane_group column)\n",
    "ground_truth = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "print(f\"Loaded validation set: {len(ground_truth):,} examples\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Included: {(ground_truth['label'] == 1).sum():,}\")\n",
    "print(f\"  Excluded: {(ground_truth['label'] == 0).sum():,}\")\n",
    "print(f\"\\nUnique reviews: {ground_truth['review_doi'].nunique():,}\")\n",
    "print(f\"\\nCochrane groups available (filter with cochrane_group column):\")\n",
    "print(ground_truth['cochrane_group'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec17e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available local models:\n",
      "  - cniongolo/biomistral:latest\n",
      "  - koesn/llama3-openbiollm-8b:latest\n",
      "  - mistral:latest\n",
      "  - llama3.2:latest\n"
     ]
    }
   ],
   "source": [
    "# Check available Ollama models\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Available local models:\")\n",
    "    if hasattr(models, 'models'):\n",
    "        for model in models.models:\n",
    "            name = model.model if hasattr(model, 'model') else str(model)\n",
    "            print(f\"  - {name}\")\n",
    "    elif isinstance(models, dict) and 'models' in models:\n",
    "        for model in models['models']:\n",
    "            name = model.get('name', model.get('model', str(model)))\n",
    "            print(f\"  - {name}\")\n",
    "    else:\n",
    "        print(f\"  Models: {models}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d08ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE ZERO-SHOT PROMPT:\n",
      "============================================================\n",
      "You are a systematic review screening assistant. Your task is to determine whether a candidate paper should be INCLUDED or EXCLUDED from a specific Cochrane systematic review.\n",
      "\n",
      "=== COCHRANE REVIEW ===\n",
      "Title: Acupuncture for smoking cessation.\n",
      "\n",
      "Abstract/Objective: Acupuncture is promoted as a treatment for smoking cessation, and is believed to reduce withdrawal symptoms. The objective of this review is to determine the effectiveness of acupuncture in smoking cessation in comparison with: a) sham acupuncture b) other interventions c) no intervention. We searched the Cochrane Tobacco Addiction Group trials register, Medline, PsycLit, Dissertation Abstracts, Health Planning and Administration, Social SciSearch, Smoking & Health, Embase, Biological Abstracts and DRUG. Randomised trials comparing a form of acupuncture with either sham acupuncture, another intervention or no intervention for smoking cessation. We extracted data in duplicate on the type of subjects, the nature of the acupuncture and control procedures, the outcome measures, method of randomisation, and completeness of follow-up. We assessed abstinence from smoking at the earliest time-point (before 6 weeks), at six months and at one year follow-up in patients smoking at baseline. We used the most rigorous definition of abstinence for each trial, and biochemically validated rates if available. Those lost to follow-up were counted as continuing to smoke. Where appropriate, we performed meta-analysis using a fixed effec...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prompt Templates - Include Review Context!\n",
    "# =============================================================================\n",
    "\n",
    "ZERO_SHOT_PROMPT = \"\"\"You are a systematic review screening assistant. Your task is to determine whether a candidate paper should be INCLUDED or EXCLUDED from a specific Cochrane systematic review.\n",
    "\n",
    "=== COCHRANE REVIEW ===\n",
    "Title: {review_title}\n",
    "\n",
    "Abstract/Objective: {review_abstract}\n",
    "\n",
    "=== CANDIDATE PAPER ===\n",
    "Title: {paper_title}\n",
    "\n",
    "Abstract: {paper_abstract}\n",
    "\n",
    "=== TASK ===\n",
    "Based on the review's objectives and inclusion criteria, should this paper be INCLUDED or EXCLUDED?\n",
    "\n",
    "Respond with only: INCLUDE or EXCLUDE\"\"\"\n",
    "\n",
    "\n",
    "COT_PROMPT = \"\"\"You are a systematic review screening assistant. Your task is to determine whether a candidate paper should be INCLUDED or EXCLUDED from a specific Cochrane systematic review.\n",
    "\n",
    "=== COCHRANE REVIEW ===\n",
    "Title: {review_title}\n",
    "\n",
    "Abstract/Objective: {review_abstract}\n",
    "\n",
    "=== CANDIDATE PAPER ===\n",
    "Title: {paper_title}\n",
    "\n",
    "Abstract: {paper_abstract}\n",
    "\n",
    "=== TASK ===\n",
    "Think step by step:\n",
    "1. What is the review looking for? (population, intervention, outcomes)\n",
    "2. What does the candidate paper study?\n",
    "3. Does the paper match the review's criteria?\n",
    "\n",
    "After your reasoning, give your final answer on a new line as: DECISION: INCLUDE or DECISION: EXCLUDE\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(row: pd.Series, use_cot: bool = False) -> str:\n",
    "    \"\"\"Create prompt with review context and paper abstract.\"\"\"\n",
    "    template = COT_PROMPT if use_cot else ZERO_SHOT_PROMPT\n",
    "    return template.format(\n",
    "        review_title=str(row['review_title'])[:500],\n",
    "        review_abstract=str(row['review_abstract'])[:2000],\n",
    "        paper_title=str(row['paper_title'])[:300],\n",
    "        paper_abstract=str(row['paper_abstract'])[:2000]\n",
    "    )\n",
    "\n",
    "# Preview a prompt\n",
    "sample = ground_truth.iloc[0]\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ZERO-SHOT PROMPT:\")\n",
    "print(\"=\" * 60)\n",
    "print(create_prompt(sample, use_cot=False)[:1500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6a9781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Evaluation Functions - Save full reasoning\n",
    "# =============================================================================\n",
    "import re\n",
    "\n",
    "def extract_decision(response: str) -> int:\n",
    "    \"\"\"Extract INCLUDE (1) or EXCLUDE (0) from LLM response.\"\"\"\n",
    "    response_upper = response.upper()\n",
    "    \n",
    "    # Look for explicit DECISION: pattern first (CoT)\n",
    "    decision_match = re.search(r'DECISION:\\s*(INCLUDE|EXCLUDE)', response_upper)\n",
    "    if decision_match:\n",
    "        return 1 if decision_match.group(1) == 'INCLUDE' else 0\n",
    "    \n",
    "    # Fall back to last occurrence\n",
    "    include_pos = response_upper.rfind('INCLUDE')\n",
    "    exclude_pos = response_upper.rfind('EXCLUDE')\n",
    "    \n",
    "    if include_pos > exclude_pos:\n",
    "        return 1\n",
    "    elif exclude_pos > include_pos:\n",
    "        return 0\n",
    "    \n",
    "    return -1  # Could not determine\n",
    "\n",
    "\n",
    "def run_evaluation(model_name: str, data: pd.DataFrame, use_cot: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Run evaluation and save full LLM reasoning.\"\"\"\n",
    "    results = []\n",
    "    prompt_type = 'cot' if use_cot else 'zero_shot'\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model_name} ({prompt_type})\"):\n",
    "        prompt = create_prompt(row, use_cot=use_cot)\n",
    "        \n",
    "        try:\n",
    "            start = time.time()\n",
    "            response = ollama.generate(model=model_name, prompt=prompt)\n",
    "            elapsed = time.time() - start\n",
    "            response_text = response.get('response', '')\n",
    "            prediction = extract_decision(response_text)\n",
    "        except Exception as e:\n",
    "            response_text = f\"ERROR: {e}\"\n",
    "            prediction = -1\n",
    "            elapsed = 0\n",
    "        \n",
    "        results.append({\n",
    "            'review_doi': row['review_doi'],\n",
    "            'study_id': row['study_id'],\n",
    "            'label': row['label'],\n",
    "            'prediction': prediction,\n",
    "            'correct': prediction == row['label'],\n",
    "            'reasoning': response_text,  # Full LLM reasoning saved!\n",
    "            'response_time_sec': round(elapsed, 2)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391ea031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 360,743 samples\n",
      "  Included: 124,119\n",
      "  Excluded: 236,624\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Run Llama 3.2 Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "MODEL_NAME = \"llama3.2\"\n",
    "\n",
    "# Full evaluation on entire dataset\n",
    "SAMPLE_SIZE = None  # None = full dataset\n",
    "eval_data = ground_truth.sample(n=SAMPLE_SIZE, random_state=42) if SAMPLE_SIZE else ground_truth\n",
    "\n",
    "print(f\"Evaluating on {len(eval_data):,} samples\")\n",
    "print(f\"  Included: {(eval_data['label'] == 1).sum():,}\")\n",
    "print(f\"  Excluded: {(eval_data['label'] == 0).sum():,}\")\n",
    "\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caae1d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ZERO-SHOT EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62aaad34244d4ac198392d34e2328486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2 (zero_shot):   0%|          | 0/360743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZERO-SHOT EVALUATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m results_zero \u001b[38;5;241m=\u001b[39m run_evaluation(MODEL_NAME, eval_data, use_cot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m all_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3.2_zero_shot\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results_zero\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save results with reasoning\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[1;34m(model_name, data, use_cot)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mgenerate(model\u001b[38;5;241m=\u001b[39mmodel_name, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m     38\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     39\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:262\u001b[0m, in \u001b[0;36mClient.generate\u001b[1;34m(self, model, prompt, suffix, system, template, context, stream, think, logprobs, top_logprobs, raw, format, images, options, keep_alive)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\n\u001b[0;32m    234\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    235\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[0;32m    252\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    263\u001b[0m     GenerateResponse,\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    266\u001b[0m     json\u001b[38;5;241m=\u001b[39mGenerateRequest(\n\u001b[0;32m    267\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    268\u001b[0m       prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    269\u001b[0m       suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[0;32m    270\u001b[0m       system\u001b[38;5;241m=\u001b[39msystem,\n\u001b[0;32m    271\u001b[0m       template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[0;32m    272\u001b[0m       context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    273\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    274\u001b[0m       think\u001b[38;5;241m=\u001b[39mthink,\n\u001b[0;32m    275\u001b[0m       logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[0;32m    276\u001b[0m       top_logprobs\u001b[38;5;241m=\u001b[39mtop_logprobs,\n\u001b[0;32m    277\u001b[0m       raw\u001b[38;5;241m=\u001b[39mraw,\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    279\u001b[0m       images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(_copy_images(images)) \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    281\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    282\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    283\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    284\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:189\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    187\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:129\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    824\u001b[0m )\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[0;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['llama3.2_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.2_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b06018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CHAIN-OF-THOUGHT EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a632e8a7c4a479f826362f94a3f82af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama3.2 (cot):   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHAIN-OF-THOUGHT EVALUATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m results_cot \u001b[38;5;241m=\u001b[39m run_evaluation(MODEL_NAME, eval_data, use_cot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m all_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3.2_cot\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results_cot\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save results with reasoning\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[1;34m(model_name, data, use_cot)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mgenerate(model\u001b[38;5;241m=\u001b[39mmodel_name, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m     38\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     39\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:262\u001b[0m, in \u001b[0;36mClient.generate\u001b[1;34m(self, model, prompt, suffix, system, template, context, stream, think, logprobs, top_logprobs, raw, format, images, options, keep_alive)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\n\u001b[0;32m    234\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    235\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[0;32m    252\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    263\u001b[0m     GenerateResponse,\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    266\u001b[0m     json\u001b[38;5;241m=\u001b[39mGenerateRequest(\n\u001b[0;32m    267\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    268\u001b[0m       prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    269\u001b[0m       suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[0;32m    270\u001b[0m       system\u001b[38;5;241m=\u001b[39msystem,\n\u001b[0;32m    271\u001b[0m       template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[0;32m    272\u001b[0m       context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    273\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    274\u001b[0m       think\u001b[38;5;241m=\u001b[39mthink,\n\u001b[0;32m    275\u001b[0m       logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[0;32m    276\u001b[0m       top_logprobs\u001b[38;5;241m=\u001b[39mtop_logprobs,\n\u001b[0;32m    277\u001b[0m       raw\u001b[38;5;241m=\u001b[39mraw,\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    279\u001b[0m       images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(_copy_images(images)) \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    281\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    282\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    283\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    284\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:189\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    187\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:129\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    824\u001b[0m )\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[0;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\juanx\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['llama3.2_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.2_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Mistral Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_NAME = \"mistral\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['mistral_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['mistral_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run OpenBioLLM-8B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# State-of-the-art biomedical LLM based on Llama-3, fine-tuned on medical data\n",
    "# Outperforms GPT-3.5 on medical benchmarks (72.5% avg accuracy)\n",
    "\n",
    "MODEL_NAME = \"koesn/llama3-openbiollm-8b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['openbiollm_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_openbiollm_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['openbiollm_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_openbiollm_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run BioMistral Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Mistral 7B fine-tuned on PubMed Central biomedical literature\n",
    "# Designed specifically for biomedical NLP tasks\n",
    "\n",
    "MODEL_NAME = \"cniongolo/biomistral\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['biomistral_zero_shot'] = results_zero\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_biomistral_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['biomistral_cot'] = results_cot\n",
    "\n",
    "# Save results with reasoning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_biomistral_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "# Quick metrics\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Llama 3.1 8B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Stronger than Llama 3.2, excellent instruction-following\n",
    "\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['llama3.1_8b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.1_8b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['llama3.1_8b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_llama3.1_8b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Qwen 2.5 7B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Top benchmarks, rivals GPT-3.5, strong reasoning\n",
    "\n",
    "MODEL_NAME = \"qwen2.5:7b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['qwen2.5_7b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_qwen2.5_7b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['qwen2.5_7b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_qwen2.5_7b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a666e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Gemma 2 9B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Google's latest, excellent for classification tasks\n",
    "\n",
    "MODEL_NAME = \"gemma2:9b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['gemma2_9b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_gemma2_9b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['gemma2_9b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_gemma2_9b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Phi-3 Medium 14B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Microsoft's efficient model, punches above its weight\n",
    "\n",
    "MODEL_NAME = \"phi3:medium\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['phi3_medium_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_phi3_medium_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['phi3_medium_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_phi3_medium_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edeffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Meditron 7B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Fine-tuned on medical guidelines & PubMed (biomedical specialized)\n",
    "\n",
    "MODEL_NAME = \"meditron:7b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['meditron_7b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_meditron_7b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['meditron_7b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_meditron_7b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb15790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Mistral Nemo 12B Evaluation - Two prompt types\n",
    "# =============================================================================\n",
    "# Newer architecture, strong reasoning capabilities\n",
    "\n",
    "MODEL_NAME = \"mistral-nemo:12b\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATING: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Run Zero-Shot evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_zero = run_evaluation(MODEL_NAME, eval_data, use_cot=False)\n",
    "all_results['mistral_nemo_12b_zero_shot'] = results_zero\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_nemo_12b_zero_shot_{timestamp}.csv\"\n",
    "results_zero.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_zero[results_zero['prediction'] != -1]\n",
    "print(f\"\\nZero-shot results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "\n",
    "# Run Chain-of-Thought evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_cot = run_evaluation(MODEL_NAME, eval_data, use_cot=True)\n",
    "all_results['mistral_nemo_12b_cot'] = results_cot\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = RESULTS_DIR / f\"eval_mistral_nemo_12b_cot_{timestamp}.csv\"\n",
    "results_cot.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Saved to {output_file.name}\")\n",
    "\n",
    "valid = results_cot[results_cot['prediction'] != -1]\n",
    "print(f\"\\nChain-of-thought results ({len(valid)} valid predictions):\")\n",
    "print(f\"  Accuracy: {accuracy_score(valid['label'], valid['prediction']):.3f}\")\n",
    "print(f\"  Precision: {precision_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  Recall: {recall_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")\n",
    "print(f\"  F1: {f1_score(valid['label'], valid['prediction'], zero_division=0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Compare Results\n",
    "# =============================================================================\n",
    "\n",
    "comparison_rows = []\n",
    "\n",
    "for run_name, results in all_results.items():\n",
    "    valid = results[results['prediction'] != -1]\n",
    "    if len(valid) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Compute metrics\n",
    "    tn = ((valid['label'] == 0) & (valid['prediction'] == 0)).sum()\n",
    "    tp = ((valid['label'] == 1) & (valid['prediction'] == 1)).sum()\n",
    "    fn = ((valid['label'] == 1) & (valid['prediction'] == 0)).sum()\n",
    "    fp = ((valid['label'] == 0) & (valid['prediction'] == 1)).sum()\n",
    "    \n",
    "    comparison_rows.append({\n",
    "        'model': run_name,\n",
    "        'n_samples': len(valid),\n",
    "        'accuracy': accuracy_score(valid['label'], valid['prediction']),\n",
    "        'precision': precision_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'recall': recall_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'f1': f1_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,  # Same as recall\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'avg_response_time': results['response_time_sec'].mean()\n",
    "    })\n",
    "\n",
    "comparison = pd.DataFrame(comparison_rows)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: ZERO-SHOT vs CHAIN-OF-THOUGHT\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# View Sample Reasoning\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE LLM REASONING (Chain-of-Thought)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show a correct and incorrect example\n",
    "cot_results = all_results.get('llama3.2_cot')\n",
    "if cot_results is not None:\n",
    "    correct = cot_results[cot_results['correct'] == True].iloc[0] if any(cot_results['correct']) else None\n",
    "    incorrect = cot_results[cot_results['correct'] == False].iloc[0] if any(~cot_results['correct']) else None\n",
    "    \n",
    "    if correct is not None:\n",
    "        print(\"\\nâœ“ CORRECT PREDICTION:\")\n",
    "        print(f\"  Label: {correct['label']} | Prediction: {correct['prediction']}\")\n",
    "        print(f\"  Reasoning:\\n{correct['reasoning'][:800]}...\")\n",
    "    \n",
    "    if incorrect is not None:\n",
    "        print(\"\\nâœ— INCORRECT PREDICTION:\")\n",
    "        print(f\"  Label: {incorrect['label']} | Prediction: {incorrect['prediction']}\")\n",
    "        print(f\"  Reasoning:\\n{incorrect['reasoning'][:800]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Samples evaluated: {len(eval_data):,}\")\n",
    "print(f\"Prompt types: Zero-shot, Chain-of-thought\")\n",
    "print(f\"Total experiments: 10 models Ã— 2 prompts = 20 runs\")\n",
    "\n",
    "print(f\"\\nGeneral-Purpose Models (7):\")\n",
    "print(\"  - Llama 3.2 (3B)\")\n",
    "print(\"  - Llama 3.1 8B\")\n",
    "print(\"  - Mistral 7B\")\n",
    "print(\"  - Mistral Nemo 12B\")\n",
    "print(\"  - Qwen 2.5 7B\")\n",
    "print(\"  - Gemma 2 9B\")\n",
    "print(\"  - Phi-3 Medium 14B\")\n",
    "\n",
    "print(f\"\\nBiomedical-Specialized Models (3):\")\n",
    "print(\"  - OpenBioLLM-8B\")\n",
    "print(\"  - BioMistral 7B\")\n",
    "print(\"  - Meditron 7B\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "print(\"  - eval_<model>_zero_shot_*.csv\")\n",
    "print(\"  - eval_<model>_cot_*.csv\")\n",
    "print(\"  - model_comparison.csv\")\n",
    "print(\"\\nâœ“ All inference was LOCAL via Ollama - no data sent to external APIs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
