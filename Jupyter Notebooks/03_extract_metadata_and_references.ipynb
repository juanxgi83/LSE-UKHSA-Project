{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b707e9",
   "metadata": {},
   "source": [
    "# 03: Extract Metadata, References, and Search Strategies from Cochrane PDFs\n",
    "\n",
    "## Summary\n",
    "This notebook extracts metadata, references, and **search strategies** from Cochrane review PDFs using **PyMuPDF** (fitz) - a fast PDF library.\n",
    "\n",
    "### Extracted Data:\n",
    "\n",
    "**Metadata:**\n",
    "- `doi`: DOI from filename\n",
    "- `title`: Review title\n",
    "- `authors`: Author list (from abstract section)\n",
    "- `abstract`: Full abstract text\n",
    "- `review_type`: review, protocol, or withdrawn\n",
    "- `cochrane_group`: Cochrane review group\n",
    "\n",
    "**References:**\n",
    "- `category`: included, excluded, awaiting, ongoing\n",
    "- `study_id`: Author Year format\n",
    "- `authors`: Full author list\n",
    "- `title`: Reference title\n",
    "- `year`: Publication year\n",
    "- `ref_doi`: DOI of reference\n",
    "- `pmid`: PubMed ID\n",
    "- `full_citation`: Complete citation text\n",
    "\n",
    "**Search Strategies (NEW):**\n",
    "- `doi`: Review DOI\n",
    "- `database`: Database searched (e.g., \"Ovid MEDLINE\", \"PubMed\")\n",
    "- `raw_strategy`: Original search strategy text (usually Ovid syntax)\n",
    "- `pubmed_query`: Translated PubMed query (for automated search execution)\n",
    "- `translation_notes`: Any issues encountered during translation\n",
    "\n",
    "### Performance:\n",
    "- **~15-20 minutes** for 16,588 PDFs (using PyMuPDF)\n",
    "- ~50x faster than pdfplumber\n",
    "\n",
    "### Output:\n",
    "- `Data/review_metadata.csv`\n",
    "- `Data/categorized_references.csv`\n",
    "- `Data/search_strategies.csv` (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0bfce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pymupdf pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f01a42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\n",
      "PDF directory: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\cochrane_pdfs\n",
      "Found 16,618 PDFs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF - fast PDF library\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Setup paths\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "PDF_DIR = DATA_DIR / \"cochrane_pdfs\"\n",
    "\n",
    "METADATA_CSV = DATA_DIR / \"review_metadata.csv\"\n",
    "REFERENCES_CSV = DATA_DIR / \"categorized_references.csv\"\n",
    "SEARCH_STRATEGIES_CSV = DATA_DIR / \"search_strategies.csv\"\n",
    "\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PDF directory: {PDF_DIR}\")\n",
    "print(f\"Found {len(pdf_files):,} PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d47e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PyMuPDF extracted 31,979 chars in 0.040 seconds\n",
      "  Estimated time for all 16,618 PDFs: 11.0 minutes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PyMuPDF Helper Functions\n",
    "# =============================================================================\n",
    "# PyMuPDF (fitz) is 10-50x faster than pdfplumber for text extraction\n",
    "# =============================================================================\n",
    "\n",
    "def extract_text_fast(pdf_path: Path, start_page: int = 0, end_page: int = None) -> str:\n",
    "    \"\"\"Extract text from PDF pages using PyMuPDF (fast).\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    if end_page is None:\n",
    "        end_page = len(doc)\n",
    "    \n",
    "    text = \"\"\n",
    "    for i in range(start_page, min(end_page, len(doc))):\n",
    "        text += doc[i].get_text() + \"\\n\\n\"\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_pdf_page_count(pdf_path: Path) -> int:\n",
    "    \"\"\"Get page count quickly.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    count = len(doc)\n",
    "    doc.close()\n",
    "    return count\n",
    "\n",
    "\n",
    "# Quick speed test\n",
    "test_pdf = pdf_files[0]\n",
    "start = time.time()\n",
    "text = extract_text_fast(test_pdf)\n",
    "elapsed = time.time() - start\n",
    "print(f\"✓ PyMuPDF extracted {len(text):,} chars in {elapsed:.3f} seconds\")\n",
    "print(f\"  Estimated time for all {len(pdf_files):,} PDFs: {elapsed * len(pdf_files) / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5637e498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metadata extraction...\n",
      "\n",
      "10.1002/14651858.CD000004\n",
      "  Title: Abdominal decompression for suspected fetal compromise/pre-e...\n",
      "  Type: review\n",
      "  Abstract: Abdominal decompression was developed as a means of pain relief during labour. I...\n",
      "\n",
      "10.1002/14651858.CD000004.pub2\n",
      "  Title: Abdominal decompression for suspected fetal compromise/pre-e...\n",
      "  Type: review\n",
      "  Abstract: Abdominal decompression was developed as a means of pain relief during labour. I...\n",
      "\n",
      "10.1002/14651858.CD000005\n",
      "  Title: Absorbable staples for uterine incision at caesarean section...\n",
      "  Type: review\n",
      "  Abstract: Staples can be placed during the making of an incision, with the aim of decreasi...\n",
      "\n",
      "10.1002/14651858.CD000005.pub2\n",
      "  Title: Absorbable staples for uterine incision at caesarean section...\n",
      "  Type: protocol\n",
      "  Abstract: (none)\n",
      "\n",
      "10.1002/14651858.CD000006\n",
      "  Title: Absorbable synthetic versus catgut suture material for perin...\n",
      "  Type: review\n",
      "  Abstract: Approximately 70% of women will experience some degree of perineal trauma follow...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Metadata Extraction (using PyMuPDF)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_metadata(pdf_path: Path) -> Dict:\n",
    "    \"\"\"Extract review metadata from first pages of PDF.\"\"\"\n",
    "    doi = pdf_path.stem.replace(\"-\", \"/\")\n",
    "    result = {\n",
    "        'doi': doi, 'title': '', 'authors': '', 'abstract': '',\n",
    "        'review_type': '', 'cochrane_group': '',\n",
    "    }\n",
    "    try:\n",
    "        # Extract first 5 pages for metadata\n",
    "        text = extract_text_fast(pdf_path, 0, 5)\n",
    "        \n",
    "        # --- TITLE ---\n",
    "        title_patterns = [\n",
    "            r'Cochrane Database of Systematic Reviews\\s*\\n+\\s*([A-Z][^\\.]{10,200})',\n",
    "            r'CochraneDatabaseofSystematicReviews\\s*\\n*\\s*([A-Z][^\\.]{10,200})',\n",
    "            r'Review\\s*\\n+([A-Z][A-Za-z\\s\\-\\,\\:]{10,200})',\n",
    "        ]\n",
    "        for pattern in title_patterns:\n",
    "            title_match = re.search(pattern, text)\n",
    "            if title_match:\n",
    "                title = re.sub(r'\\s+', ' ', title_match.group(1)).strip()\n",
    "                if len(title) > 10 and not title.startswith('Copyright'):\n",
    "                    result['title'] = title[:500]\n",
    "                    break\n",
    "        \n",
    "        # --- AUTHORS ---\n",
    "        authors_match = re.search(\n",
    "            r'^([A-Z][a-z]+(?:\\s+[A-Z]\\.?)+(?:,\\s*[A-Z][a-z]+(?:\\s+[A-Z]\\.?)+)*)',\n",
    "            text[200:2000], re.MULTILINE\n",
    "        )\n",
    "        if authors_match:\n",
    "            result['authors'] = re.sub(r'\\s+', ' ', authors_match.group(1)).strip()[:500]\n",
    "        \n",
    "        # --- ABSTRACT ---\n",
    "        # Look for \"Background\" section content (not the table of contents with dots)\n",
    "        # Key: Find \"Background\" followed by actual paragraph text, not \"...\" dots\n",
    "        abstract_patterns = [\n",
    "            # Pattern 1: Background section with real content (no dots)\n",
    "            r'Background\\s*\\n+([A-Z][^\\.]{20,}(?:\\.\\s+[A-Z][^\\.]+)*\\.)',\n",
    "            # Pattern 2: Objectives section\n",
    "            r'Objectives\\s*\\n+([A-Z][^\\.]{20,}(?:\\.\\s+[A-Z][^\\.]+)*\\.)',\n",
    "            # Pattern 3: Summary section\n",
    "            r'Summary\\s*\\n+([A-Z][^\\.]{20,}(?:\\.\\s+[A-Z][^\\.]+)*\\.)',\n",
    "        ]\n",
    "        for pattern in abstract_patterns:\n",
    "            abs_match = re.search(pattern, text)\n",
    "            if abs_match:\n",
    "                abstract = abs_match.group(1).strip()\n",
    "                # Skip if it's mostly dots (table of contents)\n",
    "                if '...' not in abstract and len(abstract) > 50:\n",
    "                    result['abstract'] = re.sub(r'\\s+', ' ', abstract)[:3000]\n",
    "                    break\n",
    "        \n",
    "        # Fallback: Try to find any substantive text after \"Background\"\n",
    "        if not result['abstract']:\n",
    "            bg_match = re.search(r'Background[:\\s]+(.{100,1500}?)(?=\\n\\s*(?:Objectives|Methods|Search|Selection))', \n",
    "                                 text, re.IGNORECASE | re.DOTALL)\n",
    "            if bg_match:\n",
    "                abstract = bg_match.group(1).strip()\n",
    "                abstract = re.sub(r'\\.+\\s*\\d+', '', abstract)  # Remove \"... 2\" patterns\n",
    "                abstract = re.sub(r'\\s+', ' ', abstract)\n",
    "                if len(abstract) > 50 and '...' not in abstract:\n",
    "                    result['abstract'] = abstract[:3000]\n",
    "        \n",
    "        # --- REVIEW TYPE ---\n",
    "        text_lower = text.lower()[:3000]\n",
    "        if 'protocol' in text_lower:\n",
    "            result['review_type'] = 'protocol'\n",
    "        elif 'withdrawn' in text_lower:\n",
    "            result['review_type'] = 'withdrawn'\n",
    "        else:\n",
    "            result['review_type'] = 'review'\n",
    "        \n",
    "        # --- COCHRANE GROUP ---\n",
    "        group_patterns = [\n",
    "            r'Cochrane\\s+([A-Za-z\\s&]+?)\\s+Group',\n",
    "            r'Cochrane\\s+([A-Za-z\\s&]+?)\\s+Review Group',\n",
    "        ]\n",
    "        for pattern in group_patterns:\n",
    "            group_match = re.search(pattern, text)\n",
    "            if group_match:\n",
    "                result['cochrane_group'] = group_match.group(1).strip()\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test on a few PDFs\n",
    "print(\"Testing metadata extraction...\")\n",
    "for i in range(5):\n",
    "    meta = extract_metadata(pdf_files[i])\n",
    "    print(f\"\\n{meta['doi']}\")\n",
    "    print(f\"  Title: {meta['title'][:60]}...\" if meta['title'] else \"  Title: (none)\")\n",
    "    print(f\"  Type: {meta['review_type']}\")\n",
    "    print(f\"  Abstract: {meta['abstract'][:80]}...\" if meta['abstract'] else \"  Abstract: (none)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9816fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing v4 translator on first 10 PDFs ...\n",
      "Found 7 strategies, 0 translated successfully\n",
      "\n",
      "  DOI: 10.1002/14651858.CD000004\n",
      "  Database: narrative_only\n",
      "  Raw (first 200): The Cochrane Pregnancy and Childbirth Group’s Trials Register (October 2008).\n",
      "Selection criteria\n",
      "Randomised or quasi-randomised trials comparing abdominal decompression with no decompression in women \n",
      "\n",
      "  DOI: 10.1002/14651858.CD000004.pub2\n",
      "  Database: narrative_only\n",
      "  Raw (first 200): The Cochrane Pregnancy and Childbirth Group’s Trials Register (2 February 2012).\n",
      "Selection criteria\n",
      "Randomised or quasi-randomised trials comparing abdominal decompression with no decompression in wom\n",
      "\n",
      "  DOI: 10.1002/14651858.CD000005\n",
      "  Database: narrative_only\n",
      "  Raw (first 200): We searched the Cochrane Pregnancy and Childbirth Group trials register.\n",
      "Selection criteria\n",
      "Randomised and quasi-randomised trials of extending the uterine incision using a stapler compared with exten\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Search Strategy Extraction and Ovid-PubMed Translation  (v4)\n",
    "# =============================================================================\n",
    "# v3 fixes + v4 improvements:\n",
    "#  11. Pattern 2b: header-less numbered Ovid blocks (recovers ~261 narratives)\n",
    "#  12. Tighter quality filter: short queries (<100 chars) must have field tag,\n",
    "#      wildcard, or quoted phrase — filters out author names & junk\n",
    "#  13. Better field-tag regex in quality gate (includes Mesh:NoExp)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def extract_search_strategy(pdf_path: Path) -> List[Dict]:\n",
    "    \"\"\"Extract search strategies from a Cochrane PDF.\"\"\"\n",
    "    doi = pdf_path.stem.replace(\"-\", \"/\")\n",
    "    strategies = []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        for i in range(len(doc)):\n",
    "            full_text += doc[i].get_text() + \"\\n\"\n",
    "        doc.close()\n",
    "\n",
    "        # Pattern 1: numbered search blocks (require \"N.\" or \"N)\" after DB header)\n",
    "        numbered_pat = re.compile(\n",
    "            r'((?:Ovid\\s+)?(?:MEDLINE|EMBASE|PubMed|CENTRAL|PsycINFO)[^\\n]*)\\n'\n",
    "            r'((?:\\s*\\d+[\\.\\)]\\s+.+\\n)+)',\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "        for m in numbered_pat.finditer(full_text):\n",
    "            db = m.group(1).strip()\n",
    "            raw = m.group(2).strip()\n",
    "            if \"medline\" in db.lower():\n",
    "                strategies.append({\"doi\": doi, \"database\": db, \"raw_strategy\": raw})\n",
    "\n",
    "        # Pattern 1b: hit-count format  \"1 Term/ (7133)\"\n",
    "        if not strategies:\n",
    "            hc_pat = re.compile(\n",
    "                r'((?:Ovid\\s+)?(?:MEDLINE|EMBASE|PubMed|CENTRAL|PsycINFO)[^\\n]*)\\n'\n",
    "                r'((?:\\s*\\d{1,3}\\s+\\S.+\\(\\d[\\d,]*\\)\\s*\\n)+)',\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            for m in hc_pat.finditer(full_text):\n",
    "                db = m.group(1).strip()\n",
    "                raw = m.group(2).strip()\n",
    "                if \"medline\" in db.lower():\n",
    "                    strategies.append({\"doi\": doi, \"database\": db, \"raw_strategy\": raw})\n",
    "\n",
    "        # Pattern 2: Appendix-style\n",
    "        if not strategies:\n",
    "            app_pat = re.compile(\n",
    "                r'Appendix\\s*\\d*[.:]*\\s*(MEDLINE[^\\n]*)\\n'\n",
    "                r'(.{100,3000}?)'\n",
    "                r'(?=\\nAppendix|\\nW H A T|\\nH I S T O R Y|\\Z)',\n",
    "                re.IGNORECASE | re.DOTALL,\n",
    "            )\n",
    "            m = app_pat.search(full_text)\n",
    "            if m:\n",
    "                strategies.append({\n",
    "                    \"doi\": doi,\n",
    "                    \"database\": m.group(1).strip(),\n",
    "                    \"raw_strategy\": m.group(2).strip(),\n",
    "                })\n",
    "\n",
    "        # Pattern 2b (NEW): header-less numbered Ovid blocks anywhere in text\n",
    "        # Recovers ~261 strategies previously classified as narrative_only\n",
    "        if not strategies:\n",
    "            ovid_block_pat = re.compile(\n",
    "                r'((?:^\\s*\\d{1,3}[\\.\\)]\\s+.+(?:\\.(tw|ti|ab|sh|mp|kf|kw|pt|fs|hw)\\.|/|\\bexp\\s).+\\n){4,})',\n",
    "                re.IGNORECASE | re.MULTILINE,\n",
    "            )\n",
    "            m = ovid_block_pat.search(full_text)\n",
    "            if m:\n",
    "                strategies.append({\n",
    "                    \"doi\": doi,\n",
    "                    \"database\": \"MEDLINE (inferred from Ovid syntax)\",\n",
    "                    \"raw_strategy\": m.group(0).strip(),\n",
    "                })\n",
    "\n",
    "        # Pattern 3: narrative-only (kept for stats)\n",
    "        if not strategies:\n",
    "            meth_pat = re.compile(\n",
    "                r'(Electronic searches|Search methods)[^\\n]*\\n'\n",
    "                r'(.{100,2000}?)'\n",
    "                r'(?=Data collection|Selection of studies)',\n",
    "                re.IGNORECASE | re.DOTALL,\n",
    "            )\n",
    "            m = meth_pat.search(full_text)\n",
    "            if m:\n",
    "                strategies.append({\n",
    "                    \"doi\": doi,\n",
    "                    \"database\": \"narrative_only\",\n",
    "                    \"raw_strategy\": m.group(2).strip(),\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        strategies.append({\n",
    "            \"doi\": doi, \"database\": \"error\",\n",
    "            \"raw_strategy\": f\"ERROR: {str(e)[:100]}\",\n",
    "        })\n",
    "\n",
    "    return strategies\n",
    "\n",
    "\n",
    "# =====================  helper: range expansion  ============================\n",
    "\n",
    "def expand_range_expr(expr: str) -> List[int]:\n",
    "    \"\"\"Expand '1-4', '1-4,6', '1-4, 6-8' into sorted list of ints.\"\"\"\n",
    "    nums: List[int] = []\n",
    "    for part in re.split(r'[,\\s]+', expr.strip()):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if '-' in part:\n",
    "            try:\n",
    "                a, b = part.split('-', 1)\n",
    "                nums.extend(range(int(a), int(b) + 1))\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                nums.append(int(part))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return sorted(set(nums))\n",
    "\n",
    "\n",
    "# =====================  pre-processing  =====================================\n",
    "\n",
    "def preprocess_raw_strategy(raw: str) -> str:\n",
    "    \"\"\"Fix common OCR errors and strip hit-count suffixes.\"\"\"\n",
    "    raw = re.sub(r'\\b0r/', 'or/', raw)\n",
    "    raw = re.sub(r'\\b0R/', 'OR/', raw)\n",
    "    raw = re.sub(r'\\s+\\(\\d[\\d,]*\\)\\s*$', '', raw, flags=re.MULTILINE)\n",
    "    return raw\n",
    "\n",
    "\n",
    "# =====================  main translator  ====================================\n",
    "\n",
    "def translate_ovid_to_pubmed(raw_strategy: str) -> Tuple[str, str]:\n",
    "    \"\"\"Translate an Ovid MEDLINE search block to PubMed syntax (v4).\"\"\"\n",
    "    if not raw_strategy or len(raw_strategy) < 10:\n",
    "        return \"\", \"empty_strategy\"\n",
    "\n",
    "    notes: list = []\n",
    "    clean_raw = preprocess_raw_strategy(raw_strategy)\n",
    "\n",
    "    # Parse numbered lines\n",
    "    lines: Dict[int, str] = {}\n",
    "    line_pat = re.compile(r'^\\s*(\\d{1,3})(?:[.\\)]\\s+|\\s{1,4})(.+)$', re.MULTILINE)\n",
    "    for m in line_pat.finditer(clean_raw):\n",
    "        lines[int(m.group(1))] = m.group(2).strip()\n",
    "\n",
    "    # Fallback for un-numbered strategies\n",
    "    if not lines:\n",
    "        fq = translate_fallback_strategy(raw_strategy, notes)\n",
    "        if fq:\n",
    "            return fq, \"; \".join(notes) if notes else \"fallback_parse\"\n",
    "        return \"\", \"no_numbered_lines\"\n",
    "\n",
    "    # Translate each line\n",
    "    translated: Dict[int, str] = {}\n",
    "    for num, content in lines.items():\n",
    "        translated[num] = translate_ovid_line(content, notes)\n",
    "\n",
    "    # Pick best final line\n",
    "    final_line = find_best_final_line(lines, translated)\n",
    "\n",
    "    # Resolve references\n",
    "    pubmed_query = resolve_line_references(translated, final_line, notes)\n",
    "\n",
    "    # Fallback: if resolution is empty, OR all non-empty translated terms\n",
    "    if not pubmed_query.strip():\n",
    "        non_empty = [\n",
    "            t for _, t in sorted(translated.items())\n",
    "            if t.strip() and not is_line_reference(t)\n",
    "        ]\n",
    "        if non_empty:\n",
    "            pubmed_query = \" OR \".join(f\"({t})\" for t in non_empty)\n",
    "            notes.append(\"final_line_empty_used_fallback\")\n",
    "\n",
    "    pubmed_query = clean_pubmed_query(pubmed_query)\n",
    "\n",
    "    # Quality gate\n",
    "    if pubmed_query and not is_valid_search_query(pubmed_query):\n",
    "        notes.append(\"filtered_not_a_query\")\n",
    "        return \"\", \"filtered_not_a_query\"\n",
    "\n",
    "    # PubMed hard limit ~4000 chars\n",
    "    if len(pubmed_query) > 4000:\n",
    "        notes.append(\"query_truncated\")\n",
    "        pubmed_query = simplify_long_query(pubmed_query, 4000)\n",
    "\n",
    "    return pubmed_query, \"; \".join(notes) if notes else \"success\"\n",
    "\n",
    "\n",
    "# =====================  final-line heuristic  ===============================\n",
    "\n",
    "def find_best_final_line(original_lines: dict, translated_lines: dict) -> int:\n",
    "    \"\"\"Return the last line that combines earlier lines (or max line number).\"\"\"\n",
    "    for ln in sorted(original_lines.keys(), reverse=True):\n",
    "        raw = original_lines[ln]\n",
    "        if re.match(r'^(?:or|and)/\\d', raw, re.IGNORECASE):\n",
    "            return ln\n",
    "        if is_line_reference(translated_lines.get(ln, '')):\n",
    "            return ln\n",
    "        if re.match(r'^\\d[\\d\\-\\s,]*(or|and|not)\\s+\\d', raw, re.IGNORECASE):\n",
    "            return ln\n",
    "    return max(original_lines.keys())\n",
    "\n",
    "\n",
    "# =====================  quality gate (v4 — tighter)  ========================\n",
    "\n",
    "def is_valid_search_query(query: str) -> bool:\n",
    "    \"\"\"Reject obvious non-search text (database names, author lists, etc.).\n",
    "\n",
    "    v4: short queries (<100 chars) must contain a PubMed field tag, wildcard,\n",
    "    or quoted phrase.  This filters ~978 junk short queries while preserving\n",
    "    legitimate single-concept searches.\n",
    "    \"\"\"\n",
    "    if len(query) < 5:\n",
    "        return False\n",
    "    has_field = bool(re.search(\n",
    "        r'\\[(?:Mesh|Mesh:NoExp|tiab|ti|pt|sh|la|ta)\\]', query, re.IGNORECASE))\n",
    "    has_bool  = bool(re.search(r'\\b(?:AND|OR|NOT)\\b', query))\n",
    "    has_quote = '\"' in query\n",
    "    has_wild  = '*' in query\n",
    "\n",
    "    # Short queries: require a concrete search indicator\n",
    "    if len(query) < 100:\n",
    "        return has_field or has_wild or has_quote\n",
    "\n",
    "    # Longer queries: booleans alone are also fine\n",
    "    return has_field or has_bool or has_quote or has_wild\n",
    "\n",
    "\n",
    "# =====================  fallback (non-numbered)  ============================\n",
    "\n",
    "def translate_fallback_strategy(raw_strategy: str, notes: list) -> str:\n",
    "    \"\"\"Best-effort parser for strategies without numbered lines.\"\"\"\n",
    "    notes.append(\"fallback_parse\")\n",
    "    terms: list = []\n",
    "\n",
    "    # PubMed-style: term[field]\n",
    "    for m in re.finditer(\n",
    "        r'([\"\\w\\*]+(?:\\s+[\"\\w\\*]+)*)\\s*\\[(tiab|ti|ab|mesh|pt|all fields)\\]',\n",
    "        raw_strategy, re.IGNORECASE,\n",
    "    ):\n",
    "        term = m.group(1).strip()\n",
    "        fmap = {'tiab': 'tiab', 'ti': 'ti', 'ab': 'tiab', 'mesh': 'Mesh', 'pt': 'pt'}\n",
    "        terms.append(f'{term}[{fmap.get(m.group(2).lower(), \"tiab\")}]')\n",
    "\n",
    "    # Ovid-style: term.field.\n",
    "    for m in re.finditer(\n",
    "        r'([A-Za-z\\*\\?]+(?:\\s+[A-Za-z\\*\\?]+)*)\\.(tw|ti|ab|mp|sh|kf|kw)\\.',\n",
    "        raw_strategy, re.IGNORECASE,\n",
    "    ):\n",
    "        term = m.group(1).strip()\n",
    "        fmap = {'tw': 'tiab', 'ti': 'ti', 'ab': 'tiab', 'mp': 'tiab',\n",
    "                'sh': 'Mesh', 'kf': 'tiab', 'kw': 'tiab'}\n",
    "        term = term.replace('$', '*').replace(':', '*').replace('?', '*')\n",
    "        terms.append(f'{term}[{fmap.get(m.group(2).lower(), \"tiab\")}]')\n",
    "\n",
    "    # exp MeSH/\n",
    "    for m in re.finditer(r'exp\\s+([^/\\n]+)/', raw_strategy, re.IGNORECASE):\n",
    "        terms.append(f'\"{m.group(1).strip()}\"[Mesh]')\n",
    "\n",
    "    if terms:\n",
    "        seen: set = set()\n",
    "        unique = []\n",
    "        for t in terms:\n",
    "            key = t.lower()\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique.append(t)\n",
    "        return \" OR \".join(unique)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# =====================  per-line translator  ================================\n",
    "\n",
    "def translate_ovid_line(line: str, notes: list) -> str:\n",
    "    \"\"\"Translate a single Ovid search line to PubMed syntax.\"\"\"\n",
    "    original = line.strip()\n",
    "\n",
    "    # -- combination patterns: or/N-M, and/N-M\n",
    "    combo = re.match(r'^(or|and)/(.+)$', original, re.IGNORECASE)\n",
    "    if combo:\n",
    "        op = combo.group(1).upper()\n",
    "        nums = expand_range_expr(combo.group(2))\n",
    "        if nums:\n",
    "            return f\" {op} \".join(str(n) for n in nums)\n",
    "\n",
    "    # -- field-tagged line refs: \"1-4.kf.\" , \"1-8.mp.\"\n",
    "    fref = re.match(r'^([\\d\\-,\\s]+)\\.(kf|kw|tw|ti|ab|mp|sh)\\.$', original, re.IGNORECASE)\n",
    "    if fref:\n",
    "        nums = expand_range_expr(fref.group(1))\n",
    "        if nums:\n",
    "            return \" OR \".join(str(n) for n in nums)\n",
    "\n",
    "    # -- pure line references (including NOT): \"9 not 10\", \"1-5 or 7\"\n",
    "    if is_line_reference(original):\n",
    "        return original\n",
    "\n",
    "    # -- LIMIT commands\n",
    "    if original.lower().startswith('limit'):\n",
    "        if 'human' in original.lower():\n",
    "            if 'humans_limit_converted' not in notes:\n",
    "                notes.append('humans_limit_converted')\n",
    "            return '\"humans\"[Mesh]'\n",
    "        if 'english' in original.lower():\n",
    "            if 'english_limit_converted' not in notes:\n",
    "                notes.append('english_limit_converted')\n",
    "            return '\"english\"[la]'\n",
    "        if 'limit_skipped' not in notes:\n",
    "            notes.append('limit_skipped')\n",
    "        return \"\"\n",
    "\n",
    "    line = original\n",
    "\n",
    "    # 1. MeSH with subheadings\n",
    "    line = re.sub(r'exp\\s+([^/]+)/[a-z,\\s]+', r'\"\\1\"[Mesh]', line, flags=re.IGNORECASE)\n",
    "    # 2. Explode MeSH\n",
    "    line = re.sub(r'exp\\s+([^/]+)/', r'\"\\1\"[Mesh]', line, flags=re.IGNORECASE)\n",
    "    # 3. Non-exp MeSH (not or/ and/ not/)\n",
    "    line = re.sub(\n",
    "        r'(?<![\"\\w])(?!(?:or|and|not)/)([A-Za-z][A-Za-z\\s\\-,]+)/',\n",
    "        r'\"\\1\"[Mesh:NoExp]', line, flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    # 4. Field tags\n",
    "    line = re.sub(r'\\.tw\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.ti\\.', '[ti]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.ab\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.pt\\.', '[pt]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.sh\\.', '[Mesh]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.kf\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.kw\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.hw\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.fs\\.', '[sh]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.nm\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.rn\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.jw\\.', '[ta]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.ot\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.ti,ab\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\.ab,ti\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "\n",
    "    # 5. .mp. -> [tiab]\n",
    "    if '.mp.' in line.lower():\n",
    "        line = re.sub(r'\\.mp\\.', '[tiab]', line, flags=re.IGNORECASE)\n",
    "        if 'mp_approximated' not in notes:\n",
    "            notes.append('mp_approximated')\n",
    "\n",
    "    # 6. Truncation\n",
    "    line = re.sub(r'(\\w+)[\\:\\$]', r'\\1*', line)\n",
    "\n",
    "    # 7. Wildcard\n",
    "    if '?' in line:\n",
    "        line = re.sub(r'\\?', '*', line)\n",
    "        if 'wildcard_approximated' not in notes:\n",
    "            notes.append('wildcard_approximated')\n",
    "\n",
    "    # 8. Adjacency\n",
    "    if re.search(r'\\badj\\d*\\b', line, re.IGNORECASE):\n",
    "        line = re.sub(r'\\s+adj\\d*\\s+', ' AND ', line, flags=re.IGNORECASE)\n",
    "        if 'adjacency_converted_to_AND' not in notes:\n",
    "            notes.append('adjacency_converted_to_AND')\n",
    "\n",
    "    # 9. NEAR\n",
    "    if re.search(r'\\bnear/?\\d*\\b', line, re.IGNORECASE):\n",
    "        line = re.sub(r'\\s+near/?\\d*\\s+', ' AND ', line, flags=re.IGNORECASE)\n",
    "        if 'near_converted_to_AND' not in notes:\n",
    "            notes.append('near_converted_to_AND')\n",
    "\n",
    "    # 10. Boolean -> uppercase\n",
    "    line = re.sub(r'\\bor\\b',  'OR',  line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\band\\b', 'AND', line, flags=re.IGNORECASE)\n",
    "    line = re.sub(r'\\bnot\\b', 'NOT', line, flags=re.IGNORECASE)\n",
    "\n",
    "    # 11. Smart quotes -> straight\n",
    "    line = line.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "# =====================  line-reference detection  ===========================\n",
    "\n",
    "def is_line_reference(content: str) -> bool:\n",
    "    \"\"\"True when content is purely refs to other lines (with OR/AND/NOT).\"\"\"\n",
    "    content = content.strip()\n",
    "    if re.match(r'^[\\d\\s\\-,]+(?:(?:or|and|not)[\\d\\s\\-,]+)+$', content, re.IGNORECASE):\n",
    "        return True\n",
    "    if re.match(r'^\\d+\\s*-\\s*\\d+$', content):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# =====================  reference resolver  =================================\n",
    "\n",
    "def resolve_line_references(\n",
    "    translated_lines: Dict[int, str], target_line: int, notes: list\n",
    ") -> str:\n",
    "    \"\"\"Recursively resolve line refs into concrete query text.\"\"\"\n",
    "\n",
    "    def resolve(line_num: int, visited: set) -> str:\n",
    "        if line_num in visited:\n",
    "            if 'circular_reference' not in notes:\n",
    "                notes.append('circular_reference')\n",
    "            return \"\"\n",
    "        visited.add(line_num)\n",
    "\n",
    "        content = translated_lines.get(line_num, \"\")\n",
    "        if not content.strip():\n",
    "            return \"\"\n",
    "\n",
    "        if is_line_reference(content):\n",
    "            parts = re.split(r'\\s+(or|and|not)\\s+', content, flags=re.IGNORECASE)\n",
    "            resolved_parts: list = []\n",
    "            operators: list = []\n",
    "\n",
    "            for p in parts:\n",
    "                p = p.strip()\n",
    "                if p.upper() in ('OR', 'AND', 'NOT'):\n",
    "                    operators.append(p.upper())\n",
    "                else:\n",
    "                    line_nums = expand_range_expr(p)\n",
    "                    subs = []\n",
    "                    for ln in line_nums:\n",
    "                        r = resolve(ln, visited.copy())\n",
    "                        if r:\n",
    "                            subs.append(f\"({r})\")\n",
    "                    if subs:\n",
    "                        resolved_parts.append(\n",
    "                            \" OR \".join(subs) if len(subs) > 1 else subs[0]\n",
    "                        )\n",
    "\n",
    "            if not resolved_parts:\n",
    "                return \"\"\n",
    "\n",
    "            result = resolved_parts[0]\n",
    "            for i, op in enumerate(operators):\n",
    "                if i + 1 < len(resolved_parts):\n",
    "                    result = f\"({result}) {op} ({resolved_parts[i + 1]})\"\n",
    "            return result\n",
    "        else:\n",
    "            return content\n",
    "\n",
    "    return resolve(target_line, set())\n",
    "\n",
    "\n",
    "# =====================  query clean-up  =====================================\n",
    "\n",
    "def clean_pubmed_query(query: str) -> str:\n",
    "    \"\"\"Tidy translated PubMed query.\"\"\"\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()\n",
    "    query = re.sub(r'\"\"', '\"', query)\n",
    "\n",
    "    for _ in range(3):\n",
    "        query = re.sub(r'\\(\\s*\\)', '', query)\n",
    "\n",
    "    query = re.sub(r'\\(\\s*(?:OR|AND|NOT)\\s*\\)', '', query)\n",
    "    query = re.sub(r'^\\s*(?:OR|AND)\\s+', '', query)\n",
    "    query = re.sub(r'\\s+(?:OR|AND)\\s*$', '', query)\n",
    "    query = re.sub(r'(?:OR|AND)\\s+(?:OR|AND)', 'OR', query)\n",
    "\n",
    "    o, c = query.count('('), query.count(')')\n",
    "    if o > c:\n",
    "        query += ')' * (o - c)\n",
    "    elif c > o:\n",
    "        query = '(' * (c - o) + query\n",
    "\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "def simplify_long_query(query: str, max_length: int) -> str:\n",
    "    \"\"\"Trim a query that exceeds PubMed's char limit.\"\"\"\n",
    "    s = re.sub(r'\\([^()]*\\[All Fields\\][^()]*\\)\\s*(?:OR|AND)?\\s*', '', query)\n",
    "    if len(s) <= max_length:\n",
    "        return clean_pubmed_query(s)\n",
    "    s = re.sub(r'\\([^()]*\\[sh\\][^()]*\\)\\s*(?:OR|AND)?\\s*', '', s)\n",
    "    if len(s) <= max_length:\n",
    "        return clean_pubmed_query(s)\n",
    "    trunc = s[:max_length]\n",
    "    lp = trunc.rfind(')')\n",
    "    if lp > max_length * 0.7:\n",
    "        trunc = trunc[:lp + 1]\n",
    "    return clean_pubmed_query(trunc)\n",
    "\n",
    "\n",
    "# ====  Quick smoke test  ====================================================\n",
    "print(\"Testing v4 translator on first 10 PDFs ...\")\n",
    "test_strategies = []\n",
    "for pdf in pdf_files[:10]:\n",
    "    test_strategies.extend(extract_search_strategy(pdf))\n",
    "\n",
    "translated_ok = 0\n",
    "for s in test_strategies:\n",
    "    if s[\"database\"] != \"narrative_only\":\n",
    "        q, n = translate_ovid_to_pubmed(s[\"raw_strategy\"])\n",
    "        s[\"pubmed_query\"] = q\n",
    "        s[\"translation_notes\"] = n\n",
    "        if q:\n",
    "            translated_ok += 1\n",
    "\n",
    "print(f\"Found {len(test_strategies)} strategies, {translated_ok} translated successfully\")\n",
    "for s in test_strategies[:3]:\n",
    "    print(f\"\\n  DOI: {s['doi']}\")\n",
    "    print(f\"  Database: {s['database']}\")\n",
    "    print(f\"  Raw (first 200): {s['raw_strategy'][:200]}\")\n",
    "    if \"pubmed_query\" in s:\n",
    "        print(f\"  PubMed: {s.get('pubmed_query','')[:200]}\")\n",
    "        print(f\"  Notes: {s.get('translation_notes','')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1966dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing reference extraction on first 5 PDFs...\n",
      "10.1002-14651858.CD000004.pdf: 4 refs, type: review\n",
      "  [included] Blecher 1967\n",
      "  Authors: Blecher JA\n",
      "  Title: Aspects of the physiology of decompression and its\n",
      "10.1002-14651858.CD000004.pub2.pdf: 4 refs, type: review\n",
      "  [included] Blecher 1967\n",
      "  Authors: Blecher JA\n",
      "  Title: Aspects of the physiology of decompression and its\n",
      "10.1002-14651858.CD000005.pdf: 4 refs, type: review\n",
      "  [included] Dargent 1990\n",
      "  Authors: Dargent D, Audra G, Noblot G\n",
      "  Title: Utilization de la pince POLY CS 57 pour l’operatio\n",
      "10.1002-14651858.CD000005.pub2.pdf: 0 refs, type: no_refs\n",
      "10.1002-14651858.CD000006.pdf: 11 refs, type: review\n",
      "  [included] Banninger 1978\n",
      "  Authors: Banninger U, Buhrig H, Schreiner WE\n",
      "  Title: A comparison between chromic catgut and polyglycol\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Reference Extraction (using PyMuPDF)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_references_from_pdf(pdf_path: Path) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"Extract categorized references from PDF.\"\"\"\n",
    "    doi = pdf_path.stem.replace(\"-\", \"/\")\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        \n",
    "        # Check first page for protocol/withdrawn\n",
    "        first_text = doc[0].get_text().lower() if total_pages > 0 else \"\"\n",
    "        if 'protocol' in first_text[:1500]:\n",
    "            doc.close()\n",
    "            return [], 'protocol'\n",
    "        if 'withdrawn' in first_text[:1500]:\n",
    "            doc.close()\n",
    "            return [], 'withdrawn'\n",
    "        \n",
    "        # Find reference pages - search from page 2 onwards\n",
    "        ref_text = \"\"\n",
    "        in_refs = False\n",
    "        \n",
    "        for i in range(2, total_pages):\n",
    "            page_text = doc[i].get_text()\n",
    "            page_lower = page_text.lower()\n",
    "            \n",
    "            # Start capturing when we find reference section markers\n",
    "            if not in_refs:\n",
    "                if any(marker in page_lower for marker in [\n",
    "                    'references to studies included',\n",
    "                    'references to studies excluded',\n",
    "                    '{published data only}',\n",
    "                    '{unpublished data only}'\n",
    "                ]):\n",
    "                    in_refs = True\n",
    "            \n",
    "            if in_refs:\n",
    "                ref_text += page_text + \"\\n\"\n",
    "                # Stop if we hit characteristics section\n",
    "                if 'characteristics of included' in page_lower:\n",
    "                    break\n",
    "                if 'characteristics of excluded' in page_lower:\n",
    "                    break\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        if not ref_text:\n",
    "            return [], 'no_refs'\n",
    "        \n",
    "        # Parse references\n",
    "        references = parse_references(ref_text, doi)\n",
    "        return references, 'review' if references else 'no_refs'\n",
    "        \n",
    "    except Exception as e:\n",
    "        return [], f'error: {str(e)[:30]}'\n",
    "\n",
    "\n",
    "def parse_references(text: str, review_doi: str) -> List[Dict]:\n",
    "    \"\"\"Parse references with structure: AuthorName Year {datatype} followed by citation.\"\"\"\n",
    "    references = []\n",
    "    \n",
    "    # Define section markers\n",
    "    sections = [\n",
    "        ('included', r'references\\s*to\\s*studies\\s*included'),\n",
    "        ('excluded', r'references\\s*to\\s*studies\\s*excluded'),\n",
    "        ('awaiting', r'references\\s*to\\s*studies\\s*awaiting'),\n",
    "        ('ongoing', r'references\\s*to\\s*ongoing\\s*studies'),\n",
    "    ]\n",
    "    \n",
    "    for category, pattern in sections:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            start = match.end()\n",
    "            \n",
    "            # Find end of section\n",
    "            end = len(text)\n",
    "            end_patterns = [\n",
    "                r'references\\s*to\\s*studies\\s*(included|excluded|awaiting)',\n",
    "                r'references\\s*to\\s*ongoing',\n",
    "                r'additional\\s+references',\n",
    "                r'characteristics\\s*of',\n",
    "                r'\\*\\s*indicates',\n",
    "            ]\n",
    "            for ep in end_patterns:\n",
    "                end_match = re.search(ep, text[start:], re.IGNORECASE)\n",
    "                if end_match and end_match.start() > 10:  # Avoid matching at very start\n",
    "                    end = min(end, start + end_match.start())\n",
    "            \n",
    "            section_text = text[start:end]\n",
    "            \n",
    "            # Pattern: \"AuthorName Year {published/unpublished data only}\"\n",
    "            # Can have whitespace/newlines between parts\n",
    "            study_pattern = re.compile(\n",
    "                r'([A-Z][A-Za-z\\'\\-]+(?:\\s+et\\s+al)?)\\s+(\\d{4}[a-z]?)\\s*\\{(published|unpublished)',\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            \n",
    "            for m in study_pattern.finditer(section_text):\n",
    "                author_id = m.group(1).strip()\n",
    "                year = m.group(2)\n",
    "                \n",
    "                # Get citation text after \"data only}\"\n",
    "                cite_start = m.end()\n",
    "                data_only_end = section_text.find('}', cite_start)\n",
    "                if data_only_end > 0:\n",
    "                    cite_start = data_only_end + 1\n",
    "                \n",
    "                # Find end of this citation (next study ID or section end)\n",
    "                next_study = study_pattern.search(section_text[cite_start:])\n",
    "                cite_end = cite_start + (next_study.start() if next_study else 2000)\n",
    "                \n",
    "                raw_citation = section_text[cite_start:cite_end].strip()\n",
    "                \n",
    "                # Clean up citation\n",
    "                citation = re.sub(r'\\n+', ' ', raw_citation)\n",
    "                citation = re.sub(r'\\s+', ' ', citation).strip()\n",
    "                \n",
    "                # Parse fields from citation\n",
    "                # Format: \"Authors. Title. Journal Year;Vol:Pages.\"\n",
    "                parts = citation.split('. ')\n",
    "                authors = parts[0].strip() if parts else \"\"\n",
    "                title = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                \n",
    "                # Extract DOI\n",
    "                doi_match = re.search(r'(10\\.\\d{4,}/[^\\s\\]\\)]+)', citation)\n",
    "                ref_doi = doi_match.group(1).rstrip('.,;])') if doi_match else \"\"\n",
    "                \n",
    "                # Extract PMID\n",
    "                pmid_match = re.search(r'(?:PMID[:\\s]*|PubMed[:\\s]*|\\[PM[:\\s]*)(\\d{6,9})', citation, re.IGNORECASE)\n",
    "                pmid = pmid_match.group(1) if pmid_match else \"\"\n",
    "                \n",
    "                references.append({\n",
    "                    'review_doi': review_doi,\n",
    "                    'category': category,\n",
    "                    'study_id': f\"{author_id} {year}\",\n",
    "                    'year': year,\n",
    "                    'authors': authors[:500],\n",
    "                    'title': title[:500],\n",
    "                    'ref_doi': ref_doi[:100],\n",
    "                    'pmid': pmid,\n",
    "                    'full_citation': citation[:1000],\n",
    "                })\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Testing reference extraction on first 5 PDFs...\")\n",
    "for i, pdf in enumerate(pdf_files[:5]):\n",
    "    refs, rtype = extract_references_from_pdf(pdf)\n",
    "    print(f\"{pdf.name}: {len(refs)} refs, type: {rtype}\")\n",
    "    if refs:\n",
    "        r = refs[0]\n",
    "        print(f\"  [{r['category']}] {r['study_id']}\")\n",
    "        print(f\"  Authors: {r['authors'][:50]}\")\n",
    "        print(f\"  Title: {r['title'][:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7158e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from 16,618 PDFs...\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44cdb63a01f47f19c69e764b1fe6593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting:   0%|          | 0/16618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  2,000 done | 5.1/sec | 47.6min left | refs: 72,677 | strategies: 1,930\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  4,000 done | 4.7/sec | 44.3min left | refs: 149,822 | strategies: 3,881\n",
      "  6,000 done | 4.6/sec | 38.8min left | refs: 228,003 | strategies: 5,905\n",
      "  8,000 done | 4.4/sec | 32.3min left | refs: 304,284 | strategies: 7,944\n",
      "  10,000 done | 4.3/sec | 25.4min left | refs: 375,177 | strategies: 10,033\n",
      "  12,000 done | 4.2/sec | 18.3min left | refs: 445,584 | strategies: 12,120\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  14,000 done | 4.0/sec | 10.9min left | refs: 519,830 | strategies: 14,261\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  16,000 done | 3.8/sec | 2.7min left | refs: 619,745 | strategies: 16,394\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "Time: 72.8 minutes (263ms per PDF)\n",
      "\n",
      "Document types: {'review': 14947, 'no_refs': 1306, 'protocol': 364, 'withdrawn': 1}\n",
      "Reference categories: {'included': 211000, 'excluded': 387757, 'awaiting': 22657, 'ongoing': 8618}\n",
      "Total references: 630,032\n",
      "\n",
      "Search strategy databases: {'narrative_only': 8801, 'MEDLINE;': 24, 'MEDLINE,': 98, 'MEDLINE': 6653, 'MEDLINE:': 267, 'CENTRAL)': 11, 'Central': 57, 'Medline,': 14, 'Medline': 163, 'CENTRAL,': 50, 'Ovid': 333, 'MEDLINE/EMBASE': 7, 'EMBASE': 4, 'MEDLINE(R)': 50, 'medline': 33, 'MEDLINE*/CENTRAL': 1, 'MEDLINE/CENTRAL': 2, 'MEDLINE,1966': 2, 'OVID': 69, 'Medline.': 1, 'CENTRAL).': 2, 'MEDLINE)': 8, 'CENTRAL),': 8, 'CENTRAL': 24, 'MEDLINE.': 13, 'MEDLINE(Ovid)': 5, 'Medline[sb]': 83, 'MEDLINEdatabase': 1, 'MEDLINE(1966-july2001),Cancerlit(1993-july': 1, 'MEDLINE(R': 1, 'MEDLINE/PubMed': 24, 'MEDLINE/CCTR': 1, 'MEDLINEusingthe': 1, 'Medline);': 1, 'medline.tw.': 11, 'MEDLINE(Ovid)(see': 1, 'MEDLINE]': 12, 'Medline]': 7, 'MEDLINE/Embase': 1, 'MEDLINE/PubMed,': 1, 'MEDLINE);': 1, 'MEDLINE).': 3, 'EMBASE,': 5, 'MedLine:': 20, 'Medline[SB])': 1, 'MEDLINEandmodiﬁedforotherdatabasesto': 1, 'MEDLINE-': 1, 'PubMed': 4, 'medline.ti,ab.': 1, 'CENTRAL;': 3, 'MEDLINE(OvidSP': 1, 'MEDLINE(OVID)': 1, 'medlineplus/mplusdictionary.html': 2, 'Medline;': 1, 'MEDLINErecords:': 1, 'medlineplus.gov/': 1, 'medlinex00ae.cr.': 1, 'MEDLINE:1966': 1, 'Medline:': 2, 'MEDLINE(1966': 1, 'Embase': 1, 'MEDLINEandthe': 1, 'medline.tw': 1, 'MEDlINE': 1, 'MEDLINE/OVID': 1, 'MEDLINEsearch': 1, 'MEDLINE/Ovid': 4, 'embase]/lim': 1, 'MEDLINE(OvidSP)': 1, 'MEDLINE[sb]': 2, 'MEDLINE/Premedline': 1, 'Medline/Premedline': 1, 'MEDLINE®': 2, 'pubmednotmedline[sb])': 1, 'medline*': 1, 'pubmednotmedline[sb]': 1, 'MEDLINE-In-Process': 1, 'Medline_final': 1, 'PubMed/MEDLINE': 1, 'medline.st.': 1, 'MEDLINE/Embase/PsycINFO': 1, 'Pubmed/MEDLINE': 1, 'pubmed/medline\"': 1, 'medline[sb]': 1, 'medline.ti.': 1, 'medline:ti': 1}\n",
      "Total search strategies: 16,929\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXTRACTION - All 16,588 PDFs\n",
    "# =============================================================================\n",
    "# Estimated time: ~15-20 minutes\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Extracting from {len(pdf_files):,} PDFs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "all_metadata = []\n",
    "all_references = []\n",
    "all_search_strategies = []\n",
    "stats = Counter()\n",
    "category_counts = Counter()\n",
    "strategy_stats = Counter()\n",
    "\n",
    "for i, pdf_path in enumerate(tqdm(pdf_files, desc=\"Extracting\")):\n",
    "    # Metadata\n",
    "    meta = extract_metadata(pdf_path)\n",
    "    all_metadata.append(meta)\n",
    "    \n",
    "    # References\n",
    "    refs, rtype = extract_references_from_pdf(pdf_path)\n",
    "    all_references.extend(refs)\n",
    "    stats[rtype] += 1\n",
    "    \n",
    "    for ref in refs:\n",
    "        category_counts[ref['category']] += 1\n",
    "    \n",
    "    # Search strategies (NEW)\n",
    "    strategies = extract_search_strategy(pdf_path)\n",
    "    for strat in strategies:\n",
    "        # Translate Ovid to PubMed\n",
    "        if strat['database'] != 'narrative_only' and strat['database'] != 'error':\n",
    "            pubmed_query, notes = translate_ovid_to_pubmed(strat['raw_strategy'])\n",
    "            strat['pubmed_query'] = pubmed_query\n",
    "            strat['translation_notes'] = notes\n",
    "        else:\n",
    "            strat['pubmed_query'] = ''\n",
    "            strat['translation_notes'] = strat['database']\n",
    "        all_search_strategies.append(strat)\n",
    "        strategy_stats[strat['database'].split()[0] if strat['database'] else 'unknown'] += 1\n",
    "    \n",
    "    # Progress every 2000\n",
    "    if (i + 1) % 2000 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed\n",
    "        remaining = (len(pdf_files) - i - 1) / rate\n",
    "        print(f\"  {i+1:,} done | {rate:.1f}/sec | {remaining/60:.1f}min left | refs: {len(all_references):,} | strategies: {len(all_search_strategies):,}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"EXTRACTION COMPLETE\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"Time: {elapsed/60:.1f} minutes ({elapsed/len(pdf_files)*1000:.0f}ms per PDF)\")\n",
    "print(f\"\\nDocument types: {dict(stats)}\")\n",
    "print(f\"Reference categories: {dict(category_counts)}\")\n",
    "print(f\"Total references: {len(all_references):,}\")\n",
    "print(f\"\\nSearch strategy databases: {dict(strategy_stats)}\")\n",
    "print(f\"Total search strategies: {len(all_search_strategies):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5764c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16,618 metadata → review_metadata.csv\n",
      "Saved 630,032 references → categorized_references.csv\n",
      "Saved 16,929 search strategies → search_strategies.csv\n",
      "\n",
      "Version tracking:\n",
      "  Reviews with CD number: 16,405\n",
      "  Unique CD numbers:      9,755\n",
      "  Latest versions:        9,968\n",
      "  Superseded (old):       6,650\n",
      "  Version distribution:   {1: np.int64(2828), 2: np.int64(8435), 3: np.int64(3424), 4: np.int64(1323), 5: np.int64(435), 6: np.int64(128), 7: np.int64(33), 8: np.int64(8), 9: np.int64(3), 10: np.int64(1)}\n",
      "\n",
      "References by category:\n",
      "category\n",
      "excluded    387757\n",
      "included    211000\n",
      "awaiting     22657\n",
      "ongoing       8618\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Search strategies by database:\n",
      "database\n",
      "narrative_only                         8801\n",
      "MEDLINE search strategy                1436\n",
      "MEDLINE                                 566\n",
      "MEDLINE (inferred from Ovid syntax)     387\n",
      "MEDLINE (Ovid) search strategy          370\n",
      "MEDLINE Ovid search strategy            303\n",
      "MEDLINE (OvidSP) search strategy        274\n",
      "MEDLINE (Ovid)                          186\n",
      "MEDLINE (OVID) search strategy          171\n",
      "MEDLINE Ovid                             93\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Translation success: 6,199/16,929 (36.6%)\n",
      "\n",
      "Translation notes breakdown:\n",
      "translation_notes\n",
      "narrative_only                                       8801\n",
      "success                                              1922\n",
      "adjacency_converted_to_AND                           1657\n",
      "filtered_not_a_query                                 1334\n",
      "mp_approximated; adjacency_converted_to_AND           651\n",
      "mp_approximated                                       561\n",
      "no_numbered_lines                                     548\n",
      "wildcard_approximated; adjacency_converted_to_AND     249\n",
      "adjacency_converted_to_AND; wildcard_approximated     213\n",
      "wildcard_approximated                                 187\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SAVE RESULTS\n",
    "meta_df = pd.DataFrame(all_metadata)\n",
    "refs_df = pd.DataFrame(all_references)\n",
    "strategies_df = pd.DataFrame(all_search_strategies)\n",
    "\n",
    "# ── Add version tracking columns ───────────────────────────────────────────────\n",
    "# Cochrane DOIs follow pattern: 10.1002/14651858.CD000004.pub2\n",
    "# Extract CD number and publication version for deduplication downstream\n",
    "import re as _re\n",
    "_version_parts = meta_df['doi'].str.extract(r'(CD\\d+)(?:\\.pub(\\d+))?', flags=_re.I)\n",
    "meta_df['cd_number'] = _version_parts[0].str.upper()\n",
    "meta_df['version'] = _version_parts[1].fillna(1).astype(int)\n",
    "\n",
    "# Flag latest version of each review\n",
    "_has_cd = meta_df[meta_df['cd_number'].notna()]\n",
    "_latest_idx = _has_cd.groupby('cd_number')['version'].idxmax()\n",
    "meta_df['is_latest_version'] = False\n",
    "meta_df.loc[_latest_idx, 'is_latest_version'] = True\n",
    "# Reviews without CD numbers are treated as latest by default\n",
    "meta_df.loc[meta_df['cd_number'].isna(), 'is_latest_version'] = True\n",
    "\n",
    "meta_df.to_csv(METADATA_CSV, index=False)\n",
    "refs_df.to_csv(REFERENCES_CSV, index=False)\n",
    "strategies_df.to_csv(SEARCH_STRATEGIES_CSV, index=False)\n",
    "\n",
    "print(f\"Saved {len(meta_df):,} metadata → {METADATA_CSV.name}\")\n",
    "print(f\"Saved {len(refs_df):,} references → {REFERENCES_CSV.name}\")\n",
    "print(f\"Saved {len(strategies_df):,} search strategies → {SEARCH_STRATEGIES_CSV.name}\")\n",
    "\n",
    "# Version tracking summary\n",
    "_latest_count = meta_df['is_latest_version'].sum()\n",
    "_superseded = len(meta_df) - _latest_count\n",
    "print(f\"\\nVersion tracking:\")\n",
    "print(f\"  Reviews with CD number: {_has_cd.shape[0]:,}\")\n",
    "print(f\"  Unique CD numbers:      {_has_cd['cd_number'].nunique():,}\")\n",
    "print(f\"  Latest versions:        {_latest_count:,}\")\n",
    "print(f\"  Superseded (old):       {_superseded:,}\")\n",
    "print(f\"  Version distribution:   {dict(meta_df['version'].value_counts().sort_index())}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nReferences by category:\")\n",
    "if len(refs_df) > 0:\n",
    "    print(refs_df['category'].value_counts())\n",
    "\n",
    "print(f\"\\nSearch strategies by database:\")\n",
    "if len(strategies_df) > 0:\n",
    "    print(strategies_df['database'].value_counts().head(10))\n",
    "\n",
    "# Translation success rate\n",
    "if len(strategies_df) > 0:\n",
    "    has_pubmed = (strategies_df['pubmed_query'].str.len() > 0).sum()\n",
    "    print(f\"\\nTranslation success: {has_pubmed:,}/{len(strategies_df):,} ({has_pubmed/len(strategies_df)*100:.1f}%)\")\n",
    "    print(f\"\\nTranslation notes breakdown:\")\n",
    "    print(strategies_df['translation_notes'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f779cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== no_numbered_lines failures: 548 ===\n",
      "\n",
      "DOI: 10.1002/14651858.CD000009.pub2\n",
      "Database: MEDLINE, EMBASE, BIOSIS Previews, PsycINFO, Science and Social Sciences Citation Index, AMED and CISCOM. Date of last search January\n",
      "Raw (first 300 chars):\n",
      "2005.\n",
      "Selection criteria\n",
      "------------------------------------------------------------\n",
      "DOI: 10.1002/14651858.CD000029.pub3\n",
      "Database: MEDLINE (Ovid) (June 1998 to May\n",
      "Raw (first 300 chars):\n",
      "2013) (Appendix 2), and EMBASE (Ovid) (June 1998 to May\n",
      "2013) (Appendix 3).\n",
      "------------------------------------------------------------\n",
      "DOI: 10.1002/14651858.CD000031.pub3\n",
      "Database: MEDLINE and EMBASE (via Ovid, 9th June\n",
      "Raw (first 300 chars):\n",
      "2009) using the medication name and 'smoking' as a free text\n",
      "------------------------------------------------------------\n",
      "DOI: 10.1002/14651858.CD000039.pub2\n",
      "Database: MEDLINE (1966 to April\n",
      "Raw (first 300 chars):\n",
      "2008) (Appendix 1), EMBASE (1980 to April 2008) (Appendix\n",
      "------------------------------------------------------------\n",
      "DOI: 10.1002/14651858.CD000057.pub2\n",
      "Database: CENTRAL) (The Cochrane Library 2009, Issue 2), MEDLINE (1966 to June 2009), EMBASE (1980 to June\n",
      "Raw (first 300 chars):\n",
      "2009) and reference lists of articles.\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== narrative_only: 8801 ===\n",
      "  With Ovid terms (.tw., .ti., etc.): 1\n",
      "  With MeSH terms: 8\n",
      "  With PubMed terms: 0\n",
      "\n",
      "--- Narrative entries that actually contain structured search terms ---\n",
      "\n",
      "DOI: 10.1002/14651858.CD000308\n",
      "Raw (first 400 chars):\n",
      "Searches were made of the Cochrane Central Register of Controlled Trials (CENTRAL, The Cochrane Library) (dexamethasone and extub*),\n",
      "MEDLINE (MeSH search terms \"dexamethasone\", \"extubat*\" and \"exp infant, newborn\"), previous reviews including cross references, ab-\n",
      "stracts of conferences and symposia proceedings, expert informants and journal handsearching mainly in the English language. These\n",
      "sear\n",
      "------------------------------------------------------------\n",
      "\n",
      "DOI: 10.1002/14651858.CD001454\n",
      "Raw (first 400 chars):\n",
      "We used the standard search method of the Cochrane Neonatal Review Group. We searched MEDLINE, EMBASE and the Cochrane\n",
      "Central Register of Controlled Trials (CENTRAL, The Cochrane Library), using the following keywords: <exp respiratory distress\n",
      "syndrome> and <exp diuretics>. These searches were updated in April 2003. In addition, we searched the abstract books of the American\n",
      "Thoracic Society and\n",
      "------------------------------------------------------------\n",
      "\n",
      "DOI: 10.1002/14651858.CD001454.pub2\n",
      "Raw (first 400 chars):\n",
      "The standard search method of the Cochrane Neonatal Review Group was used. MEDLINE, EMBASE and the Cochrane Central\n",
      "Register of Controlled Trials (CENTRAL, The Cochrane Library) were searched using the following keywords: <exp respiratory distress\n",
      "syndrome> and <exp diuretics>. These searches were updated in April 2003 and March 2007. In addition, the abstract books of the\n",
      "American Thoracic Societ\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Reviews with zero search strategies: 794 ===\n",
      "\n",
      "=== 'success' but empty/very short query: 20 ===\n",
      "\n",
      "=== Query length stats (n=6199) ===\n",
      "  Min: 6\n",
      "  Median: 238\n",
      "  Mean: 427\n",
      "  Max: 3970\n",
      "  >4000 chars: 0\n",
      "  <50 chars: 653\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSE REMAINING FAILURES\n",
    "# =============================================================================\n",
    "# Let's understand what's still failing and whether improvement is possible\n",
    "# =============================================================================\n",
    "\n",
    "# 1. What do the 123 no_numbered_lines failures look like?\n",
    "no_num = [s for s in all_search_strategies if s['translation_notes'] == 'no_numbered_lines']\n",
    "print(f\"=== no_numbered_lines failures: {len(no_num)} ===\\n\")\n",
    "for s in no_num[:5]:\n",
    "    print(f\"DOI: {s['doi']}\")\n",
    "    print(f\"Database: {s['database']}\")\n",
    "    print(f\"Raw (first 300 chars):\\n{s['raw_strategy'][:300]}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# 2. What are the narrative_only strategies? Are any actually structured?\n",
    "narrative = [s for s in all_search_strategies if s['translation_notes'] == 'narrative_only']\n",
    "print(f\"\\n=== narrative_only: {len(narrative)} ===\")\n",
    "# Check if any contain Ovid-like terms\n",
    "has_ovid_terms = 0\n",
    "has_mesh = 0\n",
    "has_pubmed_terms = 0\n",
    "for s in narrative:\n",
    "    raw = s['raw_strategy'].lower()\n",
    "    if '.tw.' in raw or '.ti.' in raw or '.ab.' in raw or '.mp.' in raw:\n",
    "        has_ovid_terms += 1\n",
    "    if '[mesh]' in raw or 'exp ' in raw:\n",
    "        has_mesh += 1\n",
    "    if '[tiab]' in raw or '[ti]' in raw:\n",
    "        has_pubmed_terms += 1\n",
    "\n",
    "print(f\"  With Ovid terms (.tw., .ti., etc.): {has_ovid_terms}\")\n",
    "print(f\"  With MeSH terms: {has_mesh}\")\n",
    "print(f\"  With PubMed terms: {has_pubmed_terms}\")\n",
    "\n",
    "# Show a few that have structured content\n",
    "print(\"\\n--- Narrative entries that actually contain structured search terms ---\")\n",
    "shown = 0\n",
    "for s in narrative:\n",
    "    raw = s['raw_strategy'].lower()\n",
    "    if '.tw.' in raw or '.ti.' in raw or 'exp ' in raw or '[mesh]' in raw:\n",
    "        print(f\"\\nDOI: {s['doi']}\")\n",
    "        print(f\"Raw (first 400 chars):\\n{s['raw_strategy'][:400]}\")\n",
    "        print(\"-\" * 60)\n",
    "        shown += 1\n",
    "        if shown >= 3:\n",
    "            break\n",
    "\n",
    "# 3. How many reviews have NO strategy at all?\n",
    "all_dois = set(f.stem.replace(\"-\", \"/\") for f in pdf_files)\n",
    "strategy_dois = set(s['doi'] for s in all_search_strategies)\n",
    "no_strategy = all_dois - strategy_dois\n",
    "print(f\"\\n=== Reviews with zero search strategies: {len(no_strategy)} ===\")\n",
    "\n",
    "# 4. Translation quality check - are any translations empty despite \"success\"?\n",
    "empty_success = [s for s in all_search_strategies \n",
    "                 if 'success' in s.get('translation_notes', '') \n",
    "                 and len(s.get('pubmed_query', '')) < 10]\n",
    "print(f\"\\n=== 'success' but empty/very short query: {len(empty_success)} ===\")\n",
    "\n",
    "# 5. Query length distribution for successful translations\n",
    "successful = [s for s in all_search_strategies if len(s.get('pubmed_query', '')) > 0]\n",
    "lengths = [len(s['pubmed_query']) for s in successful]\n",
    "print(f\"\\n=== Query length stats (n={len(successful)}) ===\")\n",
    "print(f\"  Min: {min(lengths)}\")\n",
    "print(f\"  Median: {sorted(lengths)[len(lengths)//2]}\")\n",
    "print(f\"  Mean: {sum(lengths)/len(lengths):.0f}\")\n",
    "print(f\"  Max: {max(lengths)}\")\n",
    "print(f\"  >4000 chars: {sum(1 for l in lengths if l > 4000)}\")\n",
    "print(f\"  <50 chars: {sum(1 for l in lengths if l < 50)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
