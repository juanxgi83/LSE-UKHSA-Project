{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b707e9",
   "metadata": {},
   "source": [
    "# 03: Extract Metadata and References from Cochrane PDFs\n",
    "\n",
    "## Summary\n",
    "This notebook extracts metadata and references from Cochrane review PDFs using **PyMuPDF** (fitz) - a fast PDF library.\n",
    "\n",
    "### Extracted Data:\n",
    "\n",
    "**Metadata:**\n",
    "- `doi`: DOI from filename\n",
    "- `title`: Review title\n",
    "- `authors`: Author list (from abstract section)\n",
    "- `abstract`: Full abstract text\n",
    "- `review_type`: review, protocol, or withdrawn\n",
    "- `cochrane_group`: Cochrane review group\n",
    "\n",
    "**References:**\n",
    "- `category`: included, excluded, awaiting, ongoing\n",
    "- `study_id`: Author Year format\n",
    "- `authors`: Full author list\n",
    "- `title`: Reference title\n",
    "- `year`: Publication year\n",
    "- `ref_doi`: DOI of reference\n",
    "- `pmid`: PubMed ID\n",
    "- `full_citation`: Complete citation text\n",
    "\n",
    "### Performance:\n",
    "- **~15-20 minutes** for 16,588 PDFs (using PyMuPDF)\n",
    "- ~50x faster than pdfplumber\n",
    "\n",
    "### Output:\n",
    "- `Data/review_metadata.csv`\n",
    "- `Data/categorized_references.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0bfce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pymupdf pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f01a42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\n",
      "PDF directory: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\cochrane_pdfs\n",
      "Found 16,588 PDFs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF - fast PDF library\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Setup paths\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "PDF_DIR = DATA_DIR / \"cochrane_pdfs\"\n",
    "\n",
    "METADATA_CSV = DATA_DIR / \"review_metadata.csv\"\n",
    "REFERENCES_CSV = DATA_DIR / \"categorized_references.csv\"\n",
    "\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PDF directory: {PDF_DIR}\")\n",
    "print(f\"Found {len(pdf_files):,} PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d47e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PyMuPDF extracted 31,979 chars in 0.035 seconds\n",
      "  Estimated time for all 16,588 PDFs: 9.6 minutes\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PyMuPDF Helper Functions\n",
    "# =============================================================================\n",
    "# PyMuPDF (fitz) is 10-50x faster than pdfplumber for text extraction\n",
    "# =============================================================================\n",
    "\n",
    "def extract_text_fast(pdf_path: Path, start_page: int = 0, end_page: int = None) -> str:\n",
    "    \"\"\"Extract text from PDF pages using PyMuPDF (fast).\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    if end_page is None:\n",
    "        end_page = len(doc)\n",
    "    \n",
    "    text = \"\"\n",
    "    for i in range(start_page, min(end_page, len(doc))):\n",
    "        text += doc[i].get_text() + \"\\n\\n\"\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_pdf_page_count(pdf_path: Path) -> int:\n",
    "    \"\"\"Get page count quickly.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    count = len(doc)\n",
    "    doc.close()\n",
    "    return count\n",
    "\n",
    "\n",
    "# Quick speed test\n",
    "test_pdf = pdf_files[0]\n",
    "start = time.time()\n",
    "text = extract_text_fast(test_pdf)\n",
    "elapsed = time.time() - start\n",
    "print(f\"✓ PyMuPDF extracted {len(text):,} chars in {elapsed:.3f} seconds\")\n",
    "print(f\"  Estimated time for all {len(pdf_files):,} PDFs: {elapsed * len(pdf_files) / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5637e498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metadata extraction...\n",
      "\n",
      "10.1002/14651858.CD000004\n",
      "  Title: Abdominal decompression for suspected fetal compromise/pre-e...\n",
      "  Type: review\n",
      "  Abstract: Abdominal decompression was developed as a means of pain relief during labour. I...\n",
      "\n",
      "10.1002/14651858.CD000004.pub2\n",
      "  Title: Abdominal decompression for suspected fetal compromise/pre-e...\n",
      "  Type: review\n",
      "  Abstract: Abdominal decompression was developed as a means of pain relief during labour. I...\n",
      "\n",
      "10.1002/14651858.CD000005\n",
      "  Title: Absorbable staples for uterine incision at caesarean section...\n",
      "  Type: review\n",
      "  Abstract: Staples can be placed during the making of an incision, with the aim of decreasi...\n",
      "\n",
      "10.1002/14651858.CD000005.pub2\n",
      "  Title: Absorbable staples for uterine incision at caesarean section...\n",
      "  Type: protocol\n",
      "  Abstract: (none)\n",
      "\n",
      "10.1002/14651858.CD000006\n",
      "  Title: Absorbable synthetic versus catgut suture material for perin...\n",
      "  Type: review\n",
      "  Abstract: Approximately 70% of women will experience some degree of perineal trauma follow...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Metadata Extraction (using PyMuPDF)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_metadata(pdf_path: Path) -> Dict:\n",
    "    \"\"\"Extract review metadata from first pages of PDF.\"\"\"\n",
    "    doi = pdf_path.stem.replace(\"-\", \"/\")\n",
    "    result = {\n",
    "        'doi': doi, 'title': '', 'authors': '', 'abstract': '',\n",
    "        'review_type': '', 'cochrane_group': '',\n",
    "    }\n",
    "    try:\n",
    "        # Extract first 5 pages for metadata\n",
    "        text = extract_text_fast(pdf_path, 0, 5)\n",
    "        \n",
    "        # --- TITLE ---\n",
    "        title_patterns = [\n",
    "            r'Cochrane Database of Systematic Reviews\\s*\\n+\\s*([A-Z][^\\.]{10,200})',\n",
    "            r'CochraneDatabaseofSystematicReviews\\s*\\n*\\s*([A-Z][^\\.]{10,200})',\n",
    "            r'Review\\s*\\n+([A-Z][A-Za-z\\s\\-\\,\\:]{10,200})',\n",
    "        ]\n",
    "        for pattern in title_patterns:\n",
    "            title_match = re.search(pattern, text)\n",
    "            if title_match:\n",
    "                title = re.sub(r'\\s+', ' ', title_match.group(1)).strip()\n",
    "                if len(title) > 10 and not title.startswith('Copyright'):\n",
    "                    result['title'] = title[:500]\n",
    "                    break\n",
    "        \n",
    "        # --- AUTHORS ---\n",
    "        authors_match = re.search(\n",
    "            r'^([A-Z][a-z]+(?:\\s+[A-Z]\\.?)+(?:,\\s*[A-Z][a-z]+(?:\\s+[A-Z]\\.?)+)*)',\n",
    "            text[200:2000], re.MULTILINE\n",
    "        )\n",
    "        if authors_match:\n",
    "            result['authors'] = re.sub(r'\\s+', ' ', authors_match.group(1)).strip()[:500]\n",
    "        \n",
    "        # --- ABSTRACT ---\n",
    "        # Look for \"Background\" section content (not the table of contents with dots)\n",
    "        # Key: Find \"Background\" followed by actual paragraph text, not \"...\" dots\n",
    "        abstract_patterns = [\n",
    "            # Pattern 1: Background section with real content (no dots)\n",
    "            r'Background\\s*\\n+([A-Z][^\\.]{20,}(?:\\.\\s+[A-Z][^\\.]+)*\\.)',\n",
    "            # Pattern 2: Objectives section\n",
    "            r'Objectives\\s*\\n+([A-Z][^\\.]{20,}(?:\\.\\s+[A-Z][^\\.]+)*\\.)',\n",
    "            # Pattern 3: Summary section\n",
    "            r'Summary\\s*\\n+([A-Z][^\\.]{20,}(?:\\.\\s+[A-Z][^\\.]+)*\\.)',\n",
    "        ]\n",
    "        for pattern in abstract_patterns:\n",
    "            abs_match = re.search(pattern, text)\n",
    "            if abs_match:\n",
    "                abstract = abs_match.group(1).strip()\n",
    "                # Skip if it's mostly dots (table of contents)\n",
    "                if '...' not in abstract and len(abstract) > 50:\n",
    "                    result['abstract'] = re.sub(r'\\s+', ' ', abstract)[:3000]\n",
    "                    break\n",
    "        \n",
    "        # Fallback: Try to find any substantive text after \"Background\"\n",
    "        if not result['abstract']:\n",
    "            bg_match = re.search(r'Background[:\\s]+(.{100,1500}?)(?=\\n\\s*(?:Objectives|Methods|Search|Selection))', \n",
    "                                 text, re.IGNORECASE | re.DOTALL)\n",
    "            if bg_match:\n",
    "                abstract = bg_match.group(1).strip()\n",
    "                abstract = re.sub(r'\\.+\\s*\\d+', '', abstract)  # Remove \"... 2\" patterns\n",
    "                abstract = re.sub(r'\\s+', ' ', abstract)\n",
    "                if len(abstract) > 50 and '...' not in abstract:\n",
    "                    result['abstract'] = abstract[:3000]\n",
    "        \n",
    "        # --- REVIEW TYPE ---\n",
    "        text_lower = text.lower()[:3000]\n",
    "        if 'protocol' in text_lower:\n",
    "            result['review_type'] = 'protocol'\n",
    "        elif 'withdrawn' in text_lower:\n",
    "            result['review_type'] = 'withdrawn'\n",
    "        else:\n",
    "            result['review_type'] = 'review'\n",
    "        \n",
    "        # --- COCHRANE GROUP ---\n",
    "        group_patterns = [\n",
    "            r'Cochrane\\s+([A-Za-z\\s&]+?)\\s+Group',\n",
    "            r'Cochrane\\s+([A-Za-z\\s&]+?)\\s+Review Group',\n",
    "        ]\n",
    "        for pattern in group_patterns:\n",
    "            group_match = re.search(pattern, text)\n",
    "            if group_match:\n",
    "                result['cochrane_group'] = group_match.group(1).strip()\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test on a few PDFs\n",
    "print(\"Testing metadata extraction...\")\n",
    "for i in range(5):\n",
    "    meta = extract_metadata(pdf_files[i])\n",
    "    print(f\"\\n{meta['doi']}\")\n",
    "    print(f\"  Title: {meta['title'][:60]}...\" if meta['title'] else \"  Title: (none)\")\n",
    "    print(f\"  Type: {meta['review_type']}\")\n",
    "    print(f\"  Abstract: {meta['abstract'][:80]}...\" if meta['abstract'] else \"  Abstract: (none)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1966dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing reference extraction on first 5 PDFs...\n",
      "10.1002-14651858.CD000004.pdf: 4 refs, type: review\n",
      "  [included] Blecher 1967\n",
      "  Authors: Blecher JA\n",
      "  Title: Aspects of the physiology of decompression and its\n",
      "10.1002-14651858.CD000004.pub2.pdf: 4 refs, type: review\n",
      "  [included] Blecher 1967\n",
      "  Authors: Blecher JA\n",
      "  Title: Aspects of the physiology of decompression and its\n",
      "10.1002-14651858.CD000005.pdf: 4 refs, type: review\n",
      "  [included] Dargent 1990\n",
      "  Authors: Dargent D, Audra G, Noblot G\n",
      "  Title: Utilization de la pince POLY CS 57 pour l’operatio\n",
      "10.1002-14651858.CD000005.pub2.pdf: 0 refs, type: no_refs\n",
      "10.1002-14651858.CD000006.pdf: 11 refs, type: review\n",
      "  [included] Banninger 1978\n",
      "  Authors: Banninger U, Buhrig H, Schreiner WE\n",
      "  Title: A comparison between chromic catgut and polyglycol\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Reference Extraction (using PyMuPDF)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_references_from_pdf(pdf_path: Path) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"Extract categorized references from PDF.\"\"\"\n",
    "    doi = pdf_path.stem.replace(\"-\", \"/\")\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        \n",
    "        # Check first page for protocol/withdrawn\n",
    "        first_text = doc[0].get_text().lower() if total_pages > 0 else \"\"\n",
    "        if 'protocol' in first_text[:1500]:\n",
    "            doc.close()\n",
    "            return [], 'protocol'\n",
    "        if 'withdrawn' in first_text[:1500]:\n",
    "            doc.close()\n",
    "            return [], 'withdrawn'\n",
    "        \n",
    "        # Find reference pages - search from page 2 onwards\n",
    "        ref_text = \"\"\n",
    "        in_refs = False\n",
    "        \n",
    "        for i in range(2, total_pages):\n",
    "            page_text = doc[i].get_text()\n",
    "            page_lower = page_text.lower()\n",
    "            \n",
    "            # Start capturing when we find reference section markers\n",
    "            if not in_refs:\n",
    "                if any(marker in page_lower for marker in [\n",
    "                    'references to studies included',\n",
    "                    'references to studies excluded',\n",
    "                    '{published data only}',\n",
    "                    '{unpublished data only}'\n",
    "                ]):\n",
    "                    in_refs = True\n",
    "            \n",
    "            if in_refs:\n",
    "                ref_text += page_text + \"\\n\"\n",
    "                # Stop if we hit characteristics section\n",
    "                if 'characteristics of included' in page_lower:\n",
    "                    break\n",
    "                if 'characteristics of excluded' in page_lower:\n",
    "                    break\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        if not ref_text:\n",
    "            return [], 'no_refs'\n",
    "        \n",
    "        # Parse references\n",
    "        references = parse_references(ref_text, doi)\n",
    "        return references, 'review' if references else 'no_refs'\n",
    "        \n",
    "    except Exception as e:\n",
    "        return [], f'error: {str(e)[:30]}'\n",
    "\n",
    "\n",
    "def parse_references(text: str, review_doi: str) -> List[Dict]:\n",
    "    \"\"\"Parse references with structure: AuthorName Year {datatype} followed by citation.\"\"\"\n",
    "    references = []\n",
    "    \n",
    "    # Define section markers\n",
    "    sections = [\n",
    "        ('included', r'references\\s*to\\s*studies\\s*included'),\n",
    "        ('excluded', r'references\\s*to\\s*studies\\s*excluded'),\n",
    "        ('awaiting', r'references\\s*to\\s*studies\\s*awaiting'),\n",
    "        ('ongoing', r'references\\s*to\\s*ongoing\\s*studies'),\n",
    "    ]\n",
    "    \n",
    "    for category, pattern in sections:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            start = match.end()\n",
    "            \n",
    "            # Find end of section\n",
    "            end = len(text)\n",
    "            end_patterns = [\n",
    "                r'references\\s*to\\s*studies\\s*(included|excluded|awaiting)',\n",
    "                r'references\\s*to\\s*ongoing',\n",
    "                r'additional\\s+references',\n",
    "                r'characteristics\\s*of',\n",
    "                r'\\*\\s*indicates',\n",
    "            ]\n",
    "            for ep in end_patterns:\n",
    "                end_match = re.search(ep, text[start:], re.IGNORECASE)\n",
    "                if end_match and end_match.start() > 10:  # Avoid matching at very start\n",
    "                    end = min(end, start + end_match.start())\n",
    "            \n",
    "            section_text = text[start:end]\n",
    "            \n",
    "            # Pattern: \"AuthorName Year {published/unpublished data only}\"\n",
    "            # Can have whitespace/newlines between parts\n",
    "            study_pattern = re.compile(\n",
    "                r'([A-Z][A-Za-z\\'\\-]+(?:\\s+et\\s+al)?)\\s+(\\d{4}[a-z]?)\\s*\\{(published|unpublished)',\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            \n",
    "            for m in study_pattern.finditer(section_text):\n",
    "                author_id = m.group(1).strip()\n",
    "                year = m.group(2)\n",
    "                \n",
    "                # Get citation text after \"data only}\"\n",
    "                cite_start = m.end()\n",
    "                data_only_end = section_text.find('}', cite_start)\n",
    "                if data_only_end > 0:\n",
    "                    cite_start = data_only_end + 1\n",
    "                \n",
    "                # Find end of this citation (next study ID or section end)\n",
    "                next_study = study_pattern.search(section_text[cite_start:])\n",
    "                cite_end = cite_start + (next_study.start() if next_study else 2000)\n",
    "                \n",
    "                raw_citation = section_text[cite_start:cite_end].strip()\n",
    "                \n",
    "                # Clean up citation\n",
    "                citation = re.sub(r'\\n+', ' ', raw_citation)\n",
    "                citation = re.sub(r'\\s+', ' ', citation).strip()\n",
    "                \n",
    "                # Parse fields from citation\n",
    "                # Format: \"Authors. Title. Journal Year;Vol:Pages.\"\n",
    "                parts = citation.split('. ')\n",
    "                authors = parts[0].strip() if parts else \"\"\n",
    "                title = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                \n",
    "                # Extract DOI\n",
    "                doi_match = re.search(r'(10\\.\\d{4,}/[^\\s\\]\\)]+)', citation)\n",
    "                ref_doi = doi_match.group(1).rstrip('.,;])') if doi_match else \"\"\n",
    "                \n",
    "                # Extract PMID\n",
    "                pmid_match = re.search(r'(?:PMID[:\\s]*|PubMed[:\\s]*|\\[PM[:\\s]*)(\\d{6,9})', citation, re.IGNORECASE)\n",
    "                pmid = pmid_match.group(1) if pmid_match else \"\"\n",
    "                \n",
    "                references.append({\n",
    "                    'review_doi': review_doi,\n",
    "                    'category': category,\n",
    "                    'study_id': f\"{author_id} {year}\",\n",
    "                    'year': year,\n",
    "                    'authors': authors[:500],\n",
    "                    'title': title[:500],\n",
    "                    'ref_doi': ref_doi[:100],\n",
    "                    'pmid': pmid,\n",
    "                    'full_citation': citation[:1000],\n",
    "                })\n",
    "    \n",
    "    return references\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Testing reference extraction on first 5 PDFs...\")\n",
    "for i, pdf in enumerate(pdf_files[:5]):\n",
    "    refs, rtype = extract_references_from_pdf(pdf)\n",
    "    print(f\"{pdf.name}: {len(refs)} refs, type: {rtype}\")\n",
    "    if refs:\n",
    "        r = refs[0]\n",
    "        print(f\"  [{r['category']}] {r['study_id']}\")\n",
    "        print(f\"  Authors: {r['authors'][:50]}\")\n",
    "        print(f\"  Title: {r['title'][:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c52833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE VALIDATION\n",
      "======================================================================\n",
      "\n",
      "✓ Processed 100 PDFs in 5.7 seconds (57ms per PDF)\n",
      "\n",
      "======================================================================\n",
      "METADATA EXTRACTION VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Fields extracted: ['doi', 'title', 'authors', 'abstract', 'review_type', 'cochrane_group']\n",
      "  doi: 100/100 non-empty (100%)\n",
      "  title: 100/100 non-empty (100%)\n",
      "  authors: 97/100 non-empty (97%)\n",
      "  abstract: 87/100 non-empty (87%)\n",
      "  review_type: 100/100 non-empty (100%)\n",
      "  cochrane_group: 93/100 non-empty (93%)\n",
      "\n",
      "Review types distribution:\n",
      "review_type\n",
      "review       89\n",
      "protocol      8\n",
      "withdrawn     3\n",
      "\n",
      "--- Sample Metadata (first PDF with title) ---\n",
      "DOI: 10.1002/14651858.CD000004\n",
      "Title: Abdominal decompression for suspected fetal compromise/pre-eclampsia (Review) Ho...\n",
      "Type: review\n",
      "Cochrane Group: Pregnancy and Childbirth\n",
      "Abstract: Abdominal decompression was developed as a means of pain relief during labour. It has also been used for complications of pregnancy, and in healthy pr...\n",
      "\n",
      "======================================================================\n",
      "REFERENCE EXTRACTION VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Extraction results: {'review': 86, 'protocol': 0, 'withdrawn': 0, 'no_refs': 14, 'error': 0}\n",
      "Total references extracted: 4162\n",
      "\n",
      "References by category: {'included': 1824, 'excluded': 2215, 'awaiting': 66, 'ongoing': 57}\n",
      "\n",
      "Reference fields: ['review_doi', 'category', 'study_id', 'year', 'authors', 'title', 'ref_doi', 'pmid', 'full_citation']\n",
      "  review_doi: 4162/4162 non-empty (100%)\n",
      "  category: 4162/4162 non-empty (100%)\n",
      "  study_id: 4162/4162 non-empty (100%)\n",
      "  year: 4162/4162 non-empty (100%)\n",
      "  authors: 4162/4162 non-empty (100%)\n",
      "  title: 4156/4162 non-empty (100%)\n",
      "  ref_doi: 20/4162 non-empty (0%)\n",
      "  pmid: 5/4162 non-empty (0%)\n",
      "  full_citation: 4162/4162 non-empty (100%)\n",
      "\n",
      "--- Sample References by Category ---\n",
      "\n",
      "[INCLUDED] Blecher 1967\n",
      "  Authors: Blecher JA...\n",
      "  Title: Aspects of the physiology of decompression and its usage in ...\n",
      "  DOI: \n",
      "  PMID: \n",
      "\n",
      "[EXCLUDED] Coppola 1985\n",
      "  Authors: Coppola F, Battioni M, Vessichelli R, Daoh KS, Bacchi- Moden...\n",
      "  Title: Auxologic results of abdominal decompression in growth disor...\n",
      "  DOI: \n",
      "  PMID: \n",
      "\n",
      "[AWAITING] Hemsley 1997\n",
      "  Authors: Hemsley L...\n",
      "  Title: personal communication December 2 1997....\n",
      "  DOI: \n",
      "  PMID: \n",
      "\n",
      "[ONGOING] Leung 2012\n",
      "  Authors: Leung L, Neufeld T, Marin S...\n",
      "  Title: Eﬀect of self-administered auricular acupressure on smoking ...\n",
      "  DOI: \n",
      "  PMID: \n",
      "\n",
      "======================================================================\n",
      "PDF FORMAT VALIDATION (Old vs New)\n",
      "======================================================================\n",
      "\n",
      "Format check (first 20 PDFs):\n",
      "  Old format (no spaces): 0\n",
      "  New format (with spaces): 20\n",
      "\n",
      "======================================================================\n",
      "TIME ESTIMATE FOR FULL EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "Total PDFs: 16,588\n",
      "Estimated time: 15.7 minutes\n",
      "\n",
      "✓ VALIDATION COMPLETE - Ready for full extraction\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE VALIDATION\n",
    "# =============================================================================\n",
    "# Testing on 100 PDFs to verify:\n",
    "# 1. Both old format (no spaces) and new format (with spaces) work\n",
    "# 2. All metadata fields are extracted properly\n",
    "# 3. All reference fields are extracted properly\n",
    "# 4. All reference categories are captured (included, excluded, awaiting, ongoing)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPREHENSIVE VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test on 100 PDFs\n",
    "test_pdfs = pdf_files[:100]\n",
    "start = time.time()\n",
    "\n",
    "all_test_meta = []\n",
    "all_test_refs = []\n",
    "stats = {'review': 0, 'protocol': 0, 'withdrawn': 0, 'no_refs': 0, 'error': 0}\n",
    "category_counts = {'included': 0, 'excluded': 0, 'awaiting': 0, 'ongoing': 0}\n",
    "\n",
    "for pdf_path in test_pdfs:\n",
    "    # Metadata\n",
    "    meta = extract_metadata(pdf_path)\n",
    "    all_test_meta.append(meta)\n",
    "    \n",
    "    # References\n",
    "    refs, rtype = extract_references_from_pdf(pdf_path)\n",
    "    all_test_refs.extend(refs)\n",
    "    stats[rtype.split(':')[0]] = stats.get(rtype.split(':')[0], 0) + 1\n",
    "    \n",
    "    for ref in refs:\n",
    "        category_counts[ref['category']] = category_counts.get(ref['category'], 0) + 1\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n✓ Processed 100 PDFs in {elapsed:.1f} seconds ({elapsed/100*1000:.0f}ms per PDF)\")\n",
    "\n",
    "# --- METADATA VALIDATION ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METADATA EXTRACTION VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "meta_df = pd.DataFrame(all_test_meta)\n",
    "print(f\"\\nFields extracted: {list(meta_df.columns)}\")\n",
    "\n",
    "for col in meta_df.columns:\n",
    "    non_empty = (meta_df[col].astype(str).str.len() > 0).sum()\n",
    "    print(f\"  {col}: {non_empty}/100 non-empty ({non_empty}%)\")\n",
    "\n",
    "print(\"\\nReview types distribution:\")\n",
    "print(meta_df['review_type'].value_counts().to_string())\n",
    "\n",
    "# Show sample metadata\n",
    "print(\"\\n--- Sample Metadata (first PDF with title) ---\")\n",
    "for _, row in meta_df.iterrows():\n",
    "    if row['title']:\n",
    "        print(f\"DOI: {row['doi']}\")\n",
    "        print(f\"Title: {row['title'][:80]}...\")\n",
    "        print(f\"Type: {row['review_type']}\")\n",
    "        print(f\"Cochrane Group: {row['cochrane_group']}\")\n",
    "        print(f\"Abstract: {row['abstract'][:150]}...\")\n",
    "        break\n",
    "\n",
    "# --- REFERENCE VALIDATION ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REFERENCE EXTRACTION VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nExtraction results: {stats}\")\n",
    "print(f\"Total references extracted: {len(all_test_refs)}\")\n",
    "print(f\"\\nReferences by category: {category_counts}\")\n",
    "\n",
    "if all_test_refs:\n",
    "    refs_df = pd.DataFrame(all_test_refs)\n",
    "    print(f\"\\nReference fields: {list(refs_df.columns)}\")\n",
    "    \n",
    "    for col in refs_df.columns:\n",
    "        non_empty = (refs_df[col].astype(str).str.len() > 0).sum()\n",
    "        pct = non_empty / len(refs_df) * 100\n",
    "        print(f\"  {col}: {non_empty}/{len(refs_df)} non-empty ({pct:.0f}%)\")\n",
    "    \n",
    "    # Show sample from each category\n",
    "    print(\"\\n--- Sample References by Category ---\")\n",
    "    for cat in ['included', 'excluded', 'awaiting', 'ongoing']:\n",
    "        cat_refs = [r for r in all_test_refs if r['category'] == cat]\n",
    "        if cat_refs:\n",
    "            r = cat_refs[0]\n",
    "            print(f\"\\n[{cat.upper()}] {r['study_id']}\")\n",
    "            print(f\"  Authors: {r['authors'][:60]}...\")\n",
    "            print(f\"  Title: {r['title'][:60]}...\")\n",
    "            print(f\"  DOI: {r['ref_doi']}\")\n",
    "            print(f\"  PMID: {r['pmid']}\")\n",
    "\n",
    "# --- FORMAT VALIDATION ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PDF FORMAT VALIDATION (Old vs New)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for old format (no spaces: \"CochraneDatabaseofSystematicReviews\")\n",
    "old_format = 0\n",
    "new_format = 0\n",
    "for pdf_path in test_pdfs[:20]:\n",
    "    text = extract_text_fast(pdf_path, 0, 1)\n",
    "    if 'CochraneDatabaseofSystematicReviews' in text:\n",
    "        old_format += 1\n",
    "    elif 'Cochrane Database of Systematic Reviews' in text:\n",
    "        new_format += 1\n",
    "\n",
    "print(f\"\\nFormat check (first 20 PDFs):\")\n",
    "print(f\"  Old format (no spaces): {old_format}\")\n",
    "print(f\"  New format (with spaces): {new_format}\")\n",
    "\n",
    "# --- TIME ESTIMATE ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TIME ESTIMATE FOR FULL EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "total_pdfs = len(pdf_files)\n",
    "estimated_time = elapsed / 100 * total_pdfs\n",
    "print(f\"\\nTotal PDFs: {total_pdfs:,}\")\n",
    "print(f\"Estimated time: {estimated_time/60:.1f} minutes\")\n",
    "print(f\"\\n✓ VALIDATION COMPLETE - Ready for full extraction\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7158e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from 16,588 PDFs...\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d39f949dd4444a3a7baa0cc036e682c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting:   0%|          | 0/16588 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  2,000 done | 14.7/sec | 16.5min left | refs: 72,723\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  4,000 done | 13.5/sec | 15.5min left | refs: 149,836\n",
      "  6,000 done | 12.8/sec | 13.8min left | refs: 228,029\n",
      "  8,000 done | 12.3/sec | 11.6min left | refs: 304,302\n",
      "  10,000 done | 12.0/sec | 9.1min left | refs: 375,397\n",
      "  12,000 done | 11.7/sec | 6.6min left | refs: 445,722\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  14,000 done | 11.2/sec | 3.8min left | refs: 520,155\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "  16,000 done | 10.7/sec | 0.9min left | refs: 619,709\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "Time: 25.6 minutes (92ms per PDF)\n",
      "\n",
      "Document types: {'review': 14942, 'no_refs': 1294, 'protocol': 351, 'withdrawn': 1}\n",
      "Reference categories: {'included': 210887, 'excluded': 387426, 'awaiting': 22630, 'ongoing': 8618}\n",
      "Total references: 629,561\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXTRACTION - All 16,588 PDFs\n",
    "# =============================================================================\n",
    "# Estimated time: ~15-20 minutes\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Extracting from {len(pdf_files):,} PDFs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "all_metadata = []\n",
    "all_references = []\n",
    "stats = Counter()\n",
    "category_counts = Counter()\n",
    "\n",
    "for i, pdf_path in enumerate(tqdm(pdf_files, desc=\"Extracting\")):\n",
    "    # Metadata\n",
    "    meta = extract_metadata(pdf_path)\n",
    "    all_metadata.append(meta)\n",
    "    \n",
    "    # References\n",
    "    refs, rtype = extract_references_from_pdf(pdf_path)\n",
    "    all_references.extend(refs)\n",
    "    stats[rtype] += 1\n",
    "    \n",
    "    for ref in refs:\n",
    "        category_counts[ref['category']] += 1\n",
    "    \n",
    "    # Progress every 2000\n",
    "    if (i + 1) % 2000 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed\n",
    "        remaining = (len(pdf_files) - i - 1) / rate\n",
    "        print(f\"  {i+1:,} done | {rate:.1f}/sec | {remaining/60:.1f}min left | refs: {len(all_references):,}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"EXTRACTION COMPLETE\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"Time: {elapsed/60:.1f} minutes ({elapsed/len(pdf_files)*1000:.0f}ms per PDF)\")\n",
    "print(f\"\\nDocument types: {dict(stats)}\")\n",
    "print(f\"Reference categories: {dict(category_counts)}\")\n",
    "print(f\"Total references: {len(all_references):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5764c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16,588 metadata → review_metadata.csv\n",
      "Saved 629,561 references → categorized_references.csv\n",
      "\n",
      "References by category:\n",
      "category\n",
      "excluded    387426\n",
      "included    210887\n",
      "awaiting     22630\n",
      "ongoing       8618\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SAVE RESULTS\n",
    "meta_df = pd.DataFrame(all_metadata)\n",
    "refs_df = pd.DataFrame(all_references)\n",
    "\n",
    "meta_df.to_csv(METADATA_CSV, index=False)\n",
    "refs_df.to_csv(REFERENCES_CSV, index=False)\n",
    "\n",
    "print(f\"Saved {len(meta_df):,} metadata → {METADATA_CSV.name}\")\n",
    "print(f\"Saved {len(refs_df):,} references → {REFERENCES_CSV.name}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nReferences by category:\")\n",
    "if len(refs_df) > 0:\n",
    "    print(refs_df['category'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
