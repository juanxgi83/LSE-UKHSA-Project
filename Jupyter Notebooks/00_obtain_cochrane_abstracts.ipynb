{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Cochrane Reviews from PubMed\n",
    "\n",
    "**Summary:** In this notebook, I download all Cochrane systematic reviews and their reference lists from PubMed. Cochrane reviews are high-quality systematic reviews of health research, and they cite the papers that were \"included\" in each review after screening.\n",
    "\n",
    "**What I do:**\n",
    "1. I search PubMed for all Cochrane Database of Systematic Reviews articles with abstracts\n",
    "2. I fetch the abstracts and metadata for each review (~17,000 reviews)\n",
    "3. I fetch the reference lists to get the cited papers (~1.2 million reference edges)\n",
    "4. I save both datasets to CSV files\n",
    "\n",
    "**Output files:**\n",
    "- `cochrane_pubmed_abstracts.csv` - Cochrane review abstracts and metadata\n",
    "- `cochrane_pubmed_references.csv` - Links between reviews and their cited papers\n",
    "\n",
    "**Requirements:** You need to set up a `.env` file with your NCBI credentials (NCBI_EMAIL and optionally NCBI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# I install the required packages for accessing PubMed\n",
    "%pip install -q biopython python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\.env\n",
      "NCBI_EMAIL present: yes\n"
     ]
    }
   ],
   "source": [
    "# I set up the environment, load credentials, and configure the PubMed query\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from Bio import Entrez\n",
    "import csv\n",
    "import time\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \".env\").exists() else notebook_dir.parent\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "Entrez.email = os.getenv(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.getenv(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "print(f\"Loaded .env from: {env_path}\")\n",
    "print(f\"NCBI_EMAIL present: {'yes' if Entrez.email else 'no'}\")\n",
    "\n",
    "QUERY = '(\"Cochrane Database Syst Rev\"[Journal]) AND hasabstract[text]'\n",
    "OUT_CSV = project_root / \"Data\" / \"cochrane_pubmed_abstracts.csv\"\n",
    "OUT_REF_CSV = project_root / \"Data\" / \"cochrane_pubmed_references.csv\"\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "SLEEP = 0.9\n",
    "MAX_RECORDS = None\n",
    "\n",
    "if not Entrez.email or \"example.com\" in Entrez.email:\n",
    "    raise ValueError(f\"NCBI_EMAIL not set. Create a .env file at {env_path} with your email.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define all the helper functions to search PubMed, fetch records, and parse the results\n",
    "from urllib.error import HTTPError\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO\n",
    "from Bio import Medline\n",
    "\n",
    "MAX_PUBMED_RETRIEVAL = 9500\n",
    "\n",
    "def esearch_count(query: str) -> int:\n",
    "    rec = Entrez.read(Entrez.esearch(db=\"pubmed\", term=query, retmax=0))\n",
    "    return int(rec[\"Count\"])\n",
    "\n",
    "def split_query_by_year(query: str, start_year: int, end_year: int) -> list:\n",
    "    stack = [(start_year, end_year)]\n",
    "    slices = []\n",
    "    while stack:\n",
    "        s, e = stack.pop()\n",
    "        date_clause = f'(\"{s}\"[PDAT] : \"{e}\"[PDAT])'\n",
    "        q = f\"({query}) AND {date_clause}\"\n",
    "        cnt = esearch_count(q)\n",
    "        if cnt <= MAX_PUBMED_RETRIEVAL:\n",
    "            slices.append((q, s, e))\n",
    "        else:\n",
    "            if e - s <= 1:\n",
    "                slices.append((q, s, e))\n",
    "            else:\n",
    "                mid = (s + e) // 2\n",
    "                stack.append((s, mid))\n",
    "                stack.append((mid + 1, e))\n",
    "    return slices\n",
    "\n",
    "def esearch_all_ids_with_slices(query: str, max_records=None, start_year: int = 1900, end_year: int = 2035):\n",
    "    slices = split_query_by_year(query, start_year, end_year)\n",
    "    pmids = []\n",
    "    total = 0\n",
    "    for q, s, e in slices:\n",
    "        rec0 = Entrez.read(Entrez.esearch(db=\"pubmed\", term=q, retmax=0))\n",
    "        count_slice = int(rec0[\"Count\"])\n",
    "        limit_slice = count_slice if max_records is None else min(count_slice, max_records - total)\n",
    "        for start in range(0, limit_slice, 1000):\n",
    "            retmax = min(1000, limit_slice - start)\n",
    "            rec = Entrez.read(Entrez.esearch(db=\"pubmed\", term=q, retstart=start, retmax=retmax))\n",
    "            pmids.extend(rec[\"IdList\"])\n",
    "            time.sleep(SLEEP)\n",
    "        total += limit_slice\n",
    "        if max_records is not None and total >= max_records:\n",
    "            break\n",
    "    return total, pmids\n",
    "\n",
    "def efetch_medline_by_ids(id_chunk):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_chunk), rettype=\"medline\", retmode=\"text\")\n",
    "            return handle.read()\n",
    "        except HTTPError:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            time.sleep(SLEEP * (attempt + 2))\n",
    "\n",
    "def efetch_xml_by_ids(id_chunk):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_chunk), rettype=\"xml\", retmode=\"xml\")\n",
    "            return handle.read()\n",
    "        except HTTPError:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            time.sleep(SLEEP * (attempt + 2))\n",
    "\n",
    "def medline_to_rows(medline_text: str):\n",
    "    for record in Medline.parse(StringIO(medline_text)):\n",
    "        yield {\n",
    "            \"pmid\": record.get(\"PMID\", \"\"),\n",
    "            \"title\": record.get(\"TI\", \"\"),\n",
    "            \"abstract\": record.get(\"AB\", \"\"),\n",
    "            \"journal\": record.get(\"JT\", \"\"),\n",
    "            \"year\": record.get(\"DP\", \"\").split(\" \")[0],\n",
    "            \"authors\": \"; \".join(record.get(\"AU\", [])),\n",
    "        }\n",
    "\n",
    "def parse_references_from_xml(xml_text: str):\n",
    "    root = ET.fromstring(xml_text)\n",
    "    for article in root.findall(\".//PubmedArticle\"):\n",
    "        citing_pmid = article.findtext(\".//MedlineCitation/PMID\") or \"\"\n",
    "        for ref in article.findall(\".//ReferenceList/Reference\"):\n",
    "            ref_pmid = ref.findtext(\".//ArticleIdList/ArticleId[@IdType='pubmed']\") or \"\"\n",
    "            ref_doi = ref.findtext(\".//ArticleIdList/ArticleId[@IdType='doi']\") or \"\"\n",
    "            ref_title = ref.findtext(\"Citation\") or \"\"\n",
    "            if citing_pmid and (ref_pmid or ref_doi or ref_title):\n",
    "                yield {\"citing_pmid\": citing_pmid, \"ref_pmid\": ref_pmid, \"ref_doi\": ref_doi, \"ref_title\": ref_title}\n",
    "\n",
    "def write_references_from_ids(pmids, batch_size: int, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"citing_pmid\", \"ref_pmid\", \"ref_doi\", \"ref_title\"])\n",
    "        writer.writeheader()\n",
    "        for i in range(0, len(pmids), batch_size):\n",
    "            chunk = pmids[i : i + batch_size]\n",
    "            xml_chunk = efetch_xml_by_ids(chunk)\n",
    "            for row in parse_references_from_xml(xml_chunk):\n",
    "                writer.writerow(row)\n",
    "            time.sleep(SLEEP)\n",
    "\n",
    "def write_pubmed_to_csv(query: str, out_path: Path, batch_size: int, max_records=None, refs_out_path: Path = None):\n",
    "    count, pmids = esearch_all_ids_with_slices(query, max_records=max_records)\n",
    "    print(f\"Found {count} records; fetching {len(pmids)} IDs...\")\n",
    "    \n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"pmid\", \"title\", \"abstract\", \"journal\", \"year\", \"authors\"])\n",
    "        writer.writeheader()\n",
    "        for i in range(0, len(pmids), batch_size):\n",
    "            chunk = pmids[i : i + batch_size]\n",
    "            medline_chunk = efetch_medline_by_ids(chunk)\n",
    "            for row in medline_to_rows(medline_chunk):\n",
    "                writer.writerow(row)\n",
    "            time.sleep(SLEEP)\n",
    "    \n",
    "    if refs_out_path:\n",
    "        print(\"Fetching reference lists (XML)...\")\n",
    "        write_references_from_ids(pmids, batch_size, refs_out_path)\n",
    "    \n",
    "    print(f\"Saved abstracts to {out_path.resolve()}\")\n",
    "    if refs_out_path:\n",
    "        print(f\"Saved references to {refs_out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files already exist - skipping download.\n",
      "\n",
      "Abstracts preview:\n",
      "       pmid                                              title  \\\n",
      "0  41527994  Surgical interventions for treating vesicovagi...   \n",
      "1  41524153  Physiology- versus angiography-guided percutan...   \n",
      "2  41510790     Cladribine for people with multiple sclerosis.   \n",
      "3  41510785  Oral iron supplements for children in malaria-...   \n",
      "4  41500513                           Exercise for depression.   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  This is a protocol for a Cochrane Review (inte...   \n",
      "1  This is a protocol for a Cochrane Review (inte...   \n",
      "2  RATIONALE: Multiple sclerosis (MS) is a chroni...   \n",
      "3  RATIONALE: Iron deficiency anaemia is a common...   \n",
      "4  RATIONALE: Depression is a common cause of mor...   \n",
      "\n",
      "                                       journal  year  \\\n",
      "0  The Cochrane database of systematic reviews  2026   \n",
      "1  The Cochrane database of systematic reviews  2026   \n",
      "2  The Cochrane database of systematic reviews  2026   \n",
      "3  The Cochrane database of systematic reviews  2026   \n",
      "4  The Cochrane database of systematic reviews  2026   \n",
      "\n",
      "                                             authors  \n",
      "0  Okada Y; Matsushita T; Hasegawa T; Noma H; Ota...  \n",
      "1  Higuchi S; Yamaji N; Noma H; Ito M; Yokota Y; ...  \n",
      "2  Celani MG; Orso M; Melis M; Ercolani MV; Canti...  \n",
      "3  Itzkovich M; Neuberger A; Harris I; Yahav D; P...  \n",
      "4  Clegg AJ; Hill JE; Mullin DS; Harris C; Smith ...  \n",
      "\n",
      "References preview:\n",
      "   citing_pmid  ref_pmid ref_doi  \\\n",
      "0     41527994       NaN     NaN   \n",
      "1     41527994       NaN     NaN   \n",
      "2     41527994       NaN     NaN   \n",
      "3     41527994       NaN     NaN   \n",
      "4     41527994       NaN     NaN   \n",
      "\n",
      "                                           ref_title  \n",
      "0  Hillary CJ, Osman NI, Hilton P, Chapple CR. Th...  \n",
      "1  Hilton P, Ward A. Epidemiological and surgical...  \n",
      "2  Ahmed S, Genadry R, Asiamah B, Liang M, Tripat...  \n",
      "3  World Health Organization (WHO). International...  \n",
      "4  Adler AJ, Ronsmans C, Calvert C, Filippi V. Es...  \n"
     ]
    }
   ],
   "source": [
    "# I run the download - this fetches all Cochrane reviews and their references (skip if files exist)\n",
    "import pandas as pd\n",
    "\n",
    "if OUT_CSV.exists() and OUT_REF_CSV.exists():\n",
    "    print(\"Data files already exist - skipping download.\")\n",
    "else:\n",
    "    write_pubmed_to_csv(\n",
    "        QUERY,\n",
    "        OUT_CSV,\n",
    "        BATCH_SIZE,\n",
    "        max_records=MAX_RECORDS,\n",
    "        refs_out_path=OUT_REF_CSV,\n",
    "    )\n",
    "\n",
    "print(\"\\nAbstracts preview:\")\n",
    "print(pd.read_csv(OUT_CSV).head())\n",
    "\n",
    "if OUT_REF_CSV.exists():\n",
    "    print(\"\\nReferences preview:\")\n",
    "    print(pd.read_csv(OUT_REF_CSV).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
