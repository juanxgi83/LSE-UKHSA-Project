{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00: Obtain Cochrane Review Abstracts from PubMed\n",
    "\n",
    "## Summary\n",
    "This notebook fetches Cochrane Systematic Review abstracts from PubMed/NCBI using the Entrez API.\n",
    "\n",
    "**Pipeline Position:** First notebook - obtains source data for the project.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Queries PubMed for Cochrane reviews using official publication type filters\n",
    "2. Fetches full abstract metadata (title, abstract, authors, DOI, year)\n",
    "3. Saves data to CSV for downstream processing\n",
    "\n",
    "**Output:** `Data/cochrane_pubmed_abstracts.csv`\n",
    "\n",
    "**Requirements:**\n",
    "- NCBI API key (optional but recommended for rate limits)\n",
    "- `.env` file with NCBI_EMAIL and optionally NCBI_API_KEY\n",
    "\n",
    "**Note:** This notebook ONLY fetches abstracts. Reference extraction is handled separately via Wiley TDM PDFs because PubMed XML references are not categorised (included vs excluded studies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q biopython python-dotenv pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and load environment variables\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \".env\").exists() else notebook_dir.parent\n",
    "env_path = project_root / \".env\"\n",
    "\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "Entrez.email = os.getenv(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.getenv(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"NCBI email configured: {bool(Entrez.email)}\")\n",
    "print(f\"NCBI API key configured: {bool(Entrez.api_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search query for Cochrane Systematic Reviews\n",
    "SEARCH_QUERY = '\"Cochrane Database Syst Rev\"[Journal] AND systematic review[pt]'\n",
    "\n",
    "# Perform search to get count\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=SEARCH_QUERY, retmax=0)\n",
    "result = Entrez.read(handle)\n",
    "handle.close()\n",
    "\n",
    "total_count = int(result[\"Count\"])\n",
    "print(f\"Total Cochrane reviews found: {total_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all PMIDs in batches\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "all_pmids = []\n",
    "for start in tqdm(range(0, total_count, BATCH_SIZE), desc=\"Fetching PMIDs\"):\n",
    "    handle = Entrez.esearch(\n",
    "        db=\"pubmed\", \n",
    "        term=SEARCH_QUERY, \n",
    "        retstart=start, \n",
    "        retmax=BATCH_SIZE\n",
    "    )\n",
    "    result = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    all_pmids.extend(result[\"IdList\"])\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"Retrieved {len(all_pmids):,} PMIDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to parse PubMed record metadata\n",
    "\n",
    "def parse_pubmed_record(article):\n",
    "    \"\"\"Extract key fields from a PubMed article record.\"\"\"\n",
    "    medline = article.get('MedlineCitation', {})\n",
    "    article_data = medline.get('Article', {})\n",
    "    \n",
    "    pmid = str(medline.get('PMID', ''))\n",
    "    title = article_data.get('ArticleTitle', '')\n",
    "    \n",
    "    # Extract abstract\n",
    "    abstract_data = article_data.get('Abstract', {})\n",
    "    abstract_texts = abstract_data.get('AbstractText', [])\n",
    "    if isinstance(abstract_texts, list):\n",
    "        abstract = ' '.join([str(t) for t in abstract_texts])\n",
    "    else:\n",
    "        abstract = str(abstract_texts)\n",
    "    \n",
    "    # Extract DOI\n",
    "    doi = ''\n",
    "    article_ids = article.get('PubmedData', {}).get('ArticleIdList', [])\n",
    "    for aid in article_ids:\n",
    "        if aid.attributes.get('IdType') == 'doi':\n",
    "            doi = str(aid)\n",
    "            break\n",
    "    \n",
    "    # Extract year\n",
    "    pub_date = article_data.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {})\n",
    "    year = pub_date.get('Year', '')\n",
    "    if not year:\n",
    "        medline_date = pub_date.get('MedlineDate', '')\n",
    "        if medline_date:\n",
    "            year = medline_date[:4]\n",
    "    \n",
    "    return {\n",
    "        'pmid': pmid,\n",
    "        'title': title,\n",
    "        'abstract': abstract,\n",
    "        'doi': doi,\n",
    "        'year': year\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full metadata in batches\n",
    "FETCH_BATCH = 200\n",
    "\n",
    "records = []\n",
    "for i in tqdm(range(0, len(all_pmids), FETCH_BATCH), desc=\"Fetching metadata\"):\n",
    "    batch = all_pmids[i:i + FETCH_BATCH]\n",
    "    \n",
    "    handle = Entrez.efetch(\n",
    "        db=\"pubmed\", \n",
    "        id=\",\".join(batch), \n",
    "        rettype=\"xml\"\n",
    "    )\n",
    "    fetched = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    for article in fetched.get('PubmedArticle', []):\n",
    "        records.append(parse_pubmed_record(article))\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"Fetched metadata for {len(records):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "output_file = DATA_DIR / \"cochrane_pubmed_abstracts.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved {len(df):,} Cochrane review abstracts to {output_file.name}\")\n",
    "print(f\"\\nDataset preview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick quality check\n",
    "print(f\"Records with abstracts: {df['abstract'].notna().sum():,}\")\n",
    "print(f\"Records with DOIs: {df['doi'].notna().sum():,}\")\n",
    "print(f\"Year range: {df['year'].min()} - {df['year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00: Download Cochrane Review Abstracts from PubMed\n",
    "\n",
    "## Summary\n",
    "This notebook downloads all Cochrane systematic review abstracts from PubMed. Cochrane reviews are gold-standard systematic reviews of health research that evaluate which studies should be included or excluded based on predefined criteria.\n",
    "\n",
    "**Pipeline Position:** First notebook - obtains the Cochrane reviews that will be used to fetch included/excluded studies via Wiley TDM.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Searches PubMed for all Cochrane Database of Systematic Reviews articles with abstracts\n",
    "2. Fetches abstracts and metadata for each review (~17,000 reviews)\n",
    "3. Saves to CSV for use in downstream notebooks\n",
    "\n",
    "**Output:** `Data/cochrane_pubmed_abstracts.csv`\n",
    "\n",
    "**Note:** Reference extraction is NOT done here. The PubMed XML reference list does not properly categorize included/excluded studies. Use notebook 02 to fetch categorized references via Wiley TDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for PubMed access\n",
    "%pip install -q biopython python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment: load credentials and configure paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from Bio import Entrez\n",
    "import csv\n",
    "import time\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \".env\").exists() else notebook_dir.parent\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "Entrez.email = os.getenv(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.getenv(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "print(f\"Loaded .env from: {env_path}\")\n",
    "print(f\"NCBI_EMAIL present: {'yes' if Entrez.email else 'no'}\")\n",
    "\n",
    "QUERY = '(\"Cochrane Database Syst Rev\"[Journal]) AND hasabstract[text]'\n",
    "OUT_CSV = project_root / \"Data\" / \"cochrane_pubmed_abstracts.csv\"\n",
    "BATCH_SIZE = 50\n",
    "SLEEP = 0.9\n",
    "MAX_RECORDS = None  # Set to small number for testing\n",
    "\n",
    "if not Entrez.email or \"example.com\" in Entrez.email:\n",
    "    raise ValueError(f\"NCBI_EMAIL not set. Create a .env file at {env_path} with your email.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for searching and fetching PubMed records\n",
    "from urllib.error import HTTPError\n",
    "from io import StringIO\n",
    "from Bio import Medline\n",
    "\n",
    "MAX_PUBMED_RETRIEVAL = 9500\n",
    "\n",
    "def esearch_count(query: str) -> int:\n",
    "    rec = Entrez.read(Entrez.esearch(db=\"pubmed\", term=query, retmax=0))\n",
    "    return int(rec[\"Count\"])\n",
    "\n",
    "def split_query_by_year(query: str, start_year: int, end_year: int) -> list:\n",
    "    stack = [(start_year, end_year)]\n",
    "    slices = []\n",
    "    while stack:\n",
    "        s, e = stack.pop()\n",
    "        date_clause = f'(\"{s}\"[PDAT] : \"{e}\"[PDAT])'\n",
    "        q = f\"({query}) AND {date_clause}\"\n",
    "        cnt = esearch_count(q)\n",
    "        if cnt <= MAX_PUBMED_RETRIEVAL:\n",
    "            slices.append((q, s, e))\n",
    "        else:\n",
    "            if e - s <= 1:\n",
    "                slices.append((q, s, e))\n",
    "            else:\n",
    "                mid = (s + e) // 2\n",
    "                stack.append((s, mid))\n",
    "                stack.append((mid + 1, e))\n",
    "    return slices\n",
    "\n",
    "def esearch_all_ids(query: str, max_records=None, start_year: int = 1900, end_year: int = 2035):\n",
    "    slices = split_query_by_year(query, start_year, end_year)\n",
    "    pmids = []\n",
    "    total = 0\n",
    "    for q, s, e in slices:\n",
    "        rec0 = Entrez.read(Entrez.esearch(db=\"pubmed\", term=q, retmax=0))\n",
    "        count_slice = int(rec0[\"Count\"])\n",
    "        limit_slice = count_slice if max_records is None else min(count_slice, max_records - total)\n",
    "        for start in range(0, limit_slice, 1000):\n",
    "            retmax = min(1000, limit_slice - start)\n",
    "            rec = Entrez.read(Entrez.esearch(db=\"pubmed\", term=q, retstart=start, retmax=retmax))\n",
    "            pmids.extend(rec[\"IdList\"])\n",
    "            time.sleep(SLEEP)\n",
    "        total += limit_slice\n",
    "        if max_records is not None and total >= max_records:\n",
    "            break\n",
    "    return total, pmids\n",
    "\n",
    "def efetch_medline(id_chunk):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_chunk), rettype=\"medline\", retmode=\"text\")\n",
    "            return handle.read()\n",
    "        except HTTPError:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            time.sleep(SLEEP * (attempt + 2))\n",
    "\n",
    "def medline_to_rows(medline_text: str):\n",
    "    for record in Medline.parse(StringIO(medline_text)):\n",
    "        yield {\n",
    "            \"pmid\": record.get(\"PMID\", \"\"),\n",
    "            \"title\": record.get(\"TI\", \"\"),\n",
    "            \"abstract\": record.get(\"AB\", \"\"),\n",
    "            \"journal\": record.get(\"JT\", \"\"),\n",
    "            \"year\": record.get(\"DP\", \"\").split(\" \")[0],\n",
    "            \"authors\": \"; \".join(record.get(\"AU\", [])),\n",
    "        }\n",
    "\n",
    "def write_pubmed_to_csv(query: str, out_path: Path, batch_size: int, max_records=None):\n",
    "    count, pmids = esearch_all_ids(query, max_records=max_records)\n",
    "    print(f\"Found {count} records; fetching {len(pmids)} IDs...\")\n",
    "    \n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"pmid\", \"title\", \"abstract\", \"journal\", \"year\", \"authors\"])\n",
    "        writer.writeheader()\n",
    "        for i in range(0, len(pmids), batch_size):\n",
    "            chunk = pmids[i : i + batch_size]\n",
    "            medline_chunk = efetch_medline(chunk)\n",
    "            for row in medline_to_rows(medline_chunk):\n",
    "                writer.writerow(row)\n",
    "            time.sleep(SLEEP)\n",
    "            if (i // batch_size) % 20 == 0:\n",
    "                print(f\"  Progress: {i + len(chunk)}/{len(pmids)}\")\n",
    "    \n",
    "    print(f\"Saved abstracts to {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the download (skips if file already exists)\n",
    "import pandas as pd\n",
    "\n",
    "if OUT_CSV.exists():\n",
    "    print(\"Data file already exists - skipping download.\")\n",
    "else:\n",
    "    write_pubmed_to_csv(QUERY, OUT_CSV, BATCH_SIZE, max_records=MAX_RECORDS)\n",
    "\n",
    "print(\"\\nAbstracts preview:\")\n",
    "df = pd.read_csv(OUT_CSV)\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Cochrane Reviews from PubMed\n",
    "\n",
    "**Summary:** In this notebook, I download all Cochrane systematic reviews and their reference lists from PubMed. Cochrane reviews are high-quality systematic reviews of health research, and they cite the papers that were \"included\" in each review after screening.\n",
    "\n",
    "**What I do:**\n",
    "1. I search PubMed for all Cochrane Database of Systematic Reviews articles with abstracts\n",
    "2. I fetch the abstracts and metadata for each review (~17,000 reviews)\n",
    "3. I fetch the reference lists to get the cited papers (~1.2 million reference edges)\n",
    "4. I save both datasets to CSV files\n",
    "\n",
    "**Output files:**\n",
    "- `cochrane_pubmed_abstracts.csv` - Cochrane review abstracts and metadata\n",
    "- `cochrane_pubmed_references.csv` - Links between reviews and their cited papers\n",
    "\n",
    "**Requirements:** You need to set up a `.env` file with your NCBI credentials (NCBI_EMAIL and optionally NCBI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# I install the required packages for accessing PubMed\n",
    "%pip install -q biopython python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\.env\n",
      "NCBI_EMAIL present: yes\n"
     ]
    }
   ],
   "source": [
    "# I set up the environment, load credentials, and configure the PubMed query\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from Bio import Entrez\n",
    "import csv\n",
    "import time\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \".env\").exists() else notebook_dir.parent\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "Entrez.email = os.getenv(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.getenv(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "print(f\"Loaded .env from: {env_path}\")\n",
    "print(f\"NCBI_EMAIL present: {'yes' if Entrez.email else 'no'}\")\n",
    "\n",
    "QUERY = '(\"Cochrane Database Syst Rev\"[Journal]) AND hasabstract[text]'\n",
    "OUT_CSV = project_root / \"Data\" / \"cochrane_pubmed_abstracts.csv\"\n",
    "OUT_REF_CSV = project_root / \"Data\" / \"cochrane_pubmed_references.csv\"\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "SLEEP = 0.9\n",
    "MAX_RECORDS = None\n",
    "\n",
    "if not Entrez.email or \"example.com\" in Entrez.email:\n",
    "    raise ValueError(f\"NCBI_EMAIL not set. Create a .env file at {env_path} with your email.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define all the helper functions to search PubMed, fetch records, and parse the results\n",
    "from urllib.error import HTTPError\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO\n",
    "from Bio import Medline\n",
    "\n",
    "MAX_PUBMED_RETRIEVAL = 9500\n",
    "\n",
    "def esearch_count(query: str) -> int:\n",
    "    rec = Entrez.read(Entrez.esearch(db=\"pubmed\", term=query, retmax=0))\n",
    "    return int(rec[\"Count\"])\n",
    "\n",
    "def split_query_by_year(query: str, start_year: int, end_year: int) -> list:\n",
    "    stack = [(start_year, end_year)]\n",
    "    slices = []\n",
    "    while stack:\n",
    "        s, e = stack.pop()\n",
    "        date_clause = f'(\"{s}\"[PDAT] : \"{e}\"[PDAT])'\n",
    "        q = f\"({query}) AND {date_clause}\"\n",
    "        cnt = esearch_count(q)\n",
    "        if cnt <= MAX_PUBMED_RETRIEVAL:\n",
    "            slices.append((q, s, e))\n",
    "        else:\n",
    "            if e - s <= 1:\n",
    "                slices.append((q, s, e))\n",
    "            else:\n",
    "                mid = (s + e) // 2\n",
    "                stack.append((s, mid))\n",
    "                stack.append((mid + 1, e))\n",
    "    return slices\n",
    "\n",
    "def esearch_all_ids_with_slices(query: str, max_records=None, start_year: int = 1900, end_year: int = 2035):\n",
    "    slices = split_query_by_year(query, start_year, end_year)\n",
    "    pmids = []\n",
    "    total = 0\n",
    "    for q, s, e in slices:\n",
    "        rec0 = Entrez.read(Entrez.esearch(db=\"pubmed\", term=q, retmax=0))\n",
    "        count_slice = int(rec0[\"Count\"])\n",
    "        limit_slice = count_slice if max_records is None else min(count_slice, max_records - total)\n",
    "        for start in range(0, limit_slice, 1000):\n",
    "            retmax = min(1000, limit_slice - start)\n",
    "            rec = Entrez.read(Entrez.esearch(db=\"pubmed\", term=q, retstart=start, retmax=retmax))\n",
    "            pmids.extend(rec[\"IdList\"])\n",
    "            time.sleep(SLEEP)\n",
    "        total += limit_slice\n",
    "        if max_records is not None and total >= max_records:\n",
    "            break\n",
    "    return total, pmids\n",
    "\n",
    "def efetch_medline_by_ids(id_chunk):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_chunk), rettype=\"medline\", retmode=\"text\")\n",
    "            return handle.read()\n",
    "        except HTTPError:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            time.sleep(SLEEP * (attempt + 2))\n",
    "\n",
    "def efetch_xml_by_ids(id_chunk):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_chunk), rettype=\"xml\", retmode=\"xml\")\n",
    "            return handle.read()\n",
    "        except HTTPError:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            time.sleep(SLEEP * (attempt + 2))\n",
    "\n",
    "def medline_to_rows(medline_text: str):\n",
    "    for record in Medline.parse(StringIO(medline_text)):\n",
    "        yield {\n",
    "            \"pmid\": record.get(\"PMID\", \"\"),\n",
    "            \"title\": record.get(\"TI\", \"\"),\n",
    "            \"abstract\": record.get(\"AB\", \"\"),\n",
    "            \"journal\": record.get(\"JT\", \"\"),\n",
    "            \"year\": record.get(\"DP\", \"\").split(\" \")[0],\n",
    "            \"authors\": \"; \".join(record.get(\"AU\", [])),\n",
    "        }\n",
    "\n",
    "def parse_references_from_xml(xml_text: str):\n",
    "    root = ET.fromstring(xml_text)\n",
    "    for article in root.findall(\".//PubmedArticle\"):\n",
    "        citing_pmid = article.findtext(\".//MedlineCitation/PMID\") or \"\"\n",
    "        for ref in article.findall(\".//ReferenceList/Reference\"):\n",
    "            ref_pmid = ref.findtext(\".//ArticleIdList/ArticleId[@IdType='pubmed']\") or \"\"\n",
    "            ref_doi = ref.findtext(\".//ArticleIdList/ArticleId[@IdType='doi']\") or \"\"\n",
    "            ref_title = ref.findtext(\"Citation\") or \"\"\n",
    "            if citing_pmid and (ref_pmid or ref_doi or ref_title):\n",
    "                yield {\"citing_pmid\": citing_pmid, \"ref_pmid\": ref_pmid, \"ref_doi\": ref_doi, \"ref_title\": ref_title}\n",
    "\n",
    "def write_references_from_ids(pmids, batch_size: int, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"citing_pmid\", \"ref_pmid\", \"ref_doi\", \"ref_title\"])\n",
    "        writer.writeheader()\n",
    "        for i in range(0, len(pmids), batch_size):\n",
    "            chunk = pmids[i : i + batch_size]\n",
    "            xml_chunk = efetch_xml_by_ids(chunk)\n",
    "            for row in parse_references_from_xml(xml_chunk):\n",
    "                writer.writerow(row)\n",
    "            time.sleep(SLEEP)\n",
    "\n",
    "def write_pubmed_to_csv(query: str, out_path: Path, batch_size: int, max_records=None, refs_out_path: Path = None):\n",
    "    count, pmids = esearch_all_ids_with_slices(query, max_records=max_records)\n",
    "    print(f\"Found {count} records; fetching {len(pmids)} IDs...\")\n",
    "    \n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"pmid\", \"title\", \"abstract\", \"journal\", \"year\", \"authors\"])\n",
    "        writer.writeheader()\n",
    "        for i in range(0, len(pmids), batch_size):\n",
    "            chunk = pmids[i : i + batch_size]\n",
    "            medline_chunk = efetch_medline_by_ids(chunk)\n",
    "            for row in medline_to_rows(medline_chunk):\n",
    "                writer.writerow(row)\n",
    "            time.sleep(SLEEP)\n",
    "    \n",
    "    if refs_out_path:\n",
    "        print(\"Fetching reference lists (XML)...\")\n",
    "        write_references_from_ids(pmids, batch_size, refs_out_path)\n",
    "    \n",
    "    print(f\"Saved abstracts to {out_path.resolve()}\")\n",
    "    if refs_out_path:\n",
    "        print(f\"Saved references to {refs_out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files already exist - skipping download.\n",
      "\n",
      "Abstracts preview:\n",
      "       pmid                                              title  \\\n",
      "0  41527994  Surgical interventions for treating vesicovagi...   \n",
      "1  41524153  Physiology- versus angiography-guided percutan...   \n",
      "2  41510790     Cladribine for people with multiple sclerosis.   \n",
      "3  41510785  Oral iron supplements for children in malaria-...   \n",
      "4  41500513                           Exercise for depression.   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  This is a protocol for a Cochrane Review (inte...   \n",
      "1  This is a protocol for a Cochrane Review (inte...   \n",
      "2  RATIONALE: Multiple sclerosis (MS) is a chroni...   \n",
      "3  RATIONALE: Iron deficiency anaemia is a common...   \n",
      "4  RATIONALE: Depression is a common cause of mor...   \n",
      "\n",
      "                                       journal  year  \\\n",
      "0  The Cochrane database of systematic reviews  2026   \n",
      "1  The Cochrane database of systematic reviews  2026   \n",
      "2  The Cochrane database of systematic reviews  2026   \n",
      "3  The Cochrane database of systematic reviews  2026   \n",
      "4  The Cochrane database of systematic reviews  2026   \n",
      "\n",
      "                                             authors  \n",
      "0  Okada Y; Matsushita T; Hasegawa T; Noma H; Ota...  \n",
      "1  Higuchi S; Yamaji N; Noma H; Ito M; Yokota Y; ...  \n",
      "2  Celani MG; Orso M; Melis M; Ercolani MV; Canti...  \n",
      "3  Itzkovich M; Neuberger A; Harris I; Yahav D; P...  \n",
      "4  Clegg AJ; Hill JE; Mullin DS; Harris C; Smith ...  \n",
      "\n",
      "References preview:\n",
      "   citing_pmid  ref_pmid ref_doi  \\\n",
      "0     41527994       NaN     NaN   \n",
      "1     41527994       NaN     NaN   \n",
      "2     41527994       NaN     NaN   \n",
      "3     41527994       NaN     NaN   \n",
      "4     41527994       NaN     NaN   \n",
      "\n",
      "                                           ref_title  \n",
      "0  Hillary CJ, Osman NI, Hilton P, Chapple CR. Th...  \n",
      "1  Hilton P, Ward A. Epidemiological and surgical...  \n",
      "2  Ahmed S, Genadry R, Asiamah B, Liang M, Tripat...  \n",
      "3  World Health Organization (WHO). International...  \n",
      "4  Adler AJ, Ronsmans C, Calvert C, Filippi V. Es...  \n"
     ]
    }
   ],
   "source": [
    "# I run the download - this fetches all Cochrane reviews and their references (skip if files exist)\n",
    "import pandas as pd\n",
    "\n",
    "if OUT_CSV.exists() and OUT_REF_CSV.exists():\n",
    "    print(\"Data files already exist - skipping download.\")\n",
    "else:\n",
    "    write_pubmed_to_csv(\n",
    "        QUERY,\n",
    "        OUT_CSV,\n",
    "        BATCH_SIZE,\n",
    "        max_records=MAX_RECORDS,\n",
    "        refs_out_path=OUT_REF_CSV,\n",
    "    )\n",
    "\n",
    "print(\"\\nAbstracts preview:\")\n",
    "print(pd.read_csv(OUT_CSV).head())\n",
    "\n",
    "if OUT_REF_CSV.exists():\n",
    "    print(\"\\nReferences preview:\")\n",
    "    print(pd.read_csv(OUT_REF_CSV).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
