{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f09361",
   "metadata": {},
   "source": [
    "# 04: Fetch Abstracts for Referenced Studies\n",
    "\n",
    "## Objective\n",
    "Match our extracted references to PubMed records and fetch abstracts for LLM evaluation.\n",
    "\n",
    "## Matching Strategy (CrossRef-enhanced)\n",
    "\n",
    "This workflow uses a 3-phase approach for maximum match rate:\n",
    "\n",
    "1. **Direct Extraction** - Extract DOI/PMID directly from reference text using regex\n",
    "2. **CrossRef API** - For references without DOI, query CrossRef's bibliographic search\n",
    "3. **PubMed Lookup** - Use DOIs to fetch PMIDs, then batch-fetch abstracts\n",
    "\n",
    "## Why CrossRef?\n",
    "- CrossRef API (`query.bibliographic`) is specifically designed to match reference strings to DOIs\n",
    "- Handles fuzzy matching internally (typos, ligatures, formatting issues)\n",
    "- Has 130M+ DOIs - broader coverage than PubMed-only search\n",
    "- DOI → PMID lookup is highly reliable (~95%+ when the paper exists in PubMed)\n",
    "\n",
    "## Output\n",
    "- `Data/referenced_paper_abstracts.csv` - Abstracts with match confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3896d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q biopython pandas tqdm requests rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673bcff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\n",
      "NCBI API key configured: True\n",
      "NCBI rate: 9 requests/sec\n",
      "CrossRef rate: 2 requests/sec\n",
      "Fuzzy match threshold: 75%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from Bio import Entrez\n",
    "from collections import Counter\n",
    "from rapidfuzz import fuzz  # Fuzzy string matching for improved reference matching\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# NCBI API configuration - with API key allows 10 requests/sec\n",
    "# Credentials loaded from .env file (never hardcode!)\n",
    "Entrez.email = os.environ.get(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.environ.get(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "# Rate limits\n",
    "NCBI_RATE = 0.11 if Entrez.api_key else 0.34  # seconds between NCBI requests\n",
    "CROSSREF_RATE = 0.5  # seconds between CrossRef requests (polite)\n",
    "\n",
    "# Fuzzy matching configuration\n",
    "FUZZY_TITLE_THRESHOLD = 75  # Minimum title similarity score (0-100) to accept a CrossRef match\n",
    "FUZZY_AUTHOR_WEIGHT = 0.3   # How much to weight author match vs title match\n",
    "\n",
    "# Setup paths\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "\n",
    "# Input\n",
    "REFS_CSV = DATA_DIR / \"categorized_references.csv\"\n",
    "META_CSV = DATA_DIR / \"review_metadata.csv\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_CSV = DATA_DIR / \"referenced_paper_abstracts.csv\"\n",
    "PROGRESS_CSV = DATA_DIR / \"crossref_matching_progress.csv\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"NCBI API key configured: {bool(Entrez.api_key)}\")\n",
    "print(f\"NCBI rate: {1/NCBI_RATE:.0f} requests/sec\")\n",
    "print(f\"CrossRef rate: {1/CROSSREF_RATE:.0f} requests/sec\")\n",
    "print(f\"Fuzzy match threshold: {FUZZY_TITLE_THRESHOLD}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e564d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total references: 630,032\n",
      "Total reviews in metadata: 16,618\n",
      "After version dedup: 9,968 reviews (removed 6,650 superseded)\n",
      "\n",
      "Public health reviews (latest only): 61\n",
      "Groups: Public Health (61)\n",
      "\n",
      "References after PH filter: 5,876\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load and Filter to Public Health Reviews (Latest Versions Only)\n",
    "# =============================================================================\n",
    "\n",
    "# Load all references\n",
    "refs_df = pd.read_csv(REFS_CSV)\n",
    "print(f\"Total references: {len(refs_df):,}\")\n",
    "\n",
    "# Load metadata for filtering\n",
    "meta_df = pd.read_csv(META_CSV)\n",
    "print(f\"Total reviews in metadata: {len(meta_df):,}\")\n",
    "\n",
    "# ── Version deduplication ──────────────────────────────────────────────────────\n",
    "# Cochrane DOIs: 10.1002/14651858.CD000004.pub2 → CD000004, version 2\n",
    "# Keep only the most recent version of each review\n",
    "\n",
    "if 'cd_number' not in meta_df.columns or 'version' not in meta_df.columns:\n",
    "    # Compute version info if not already in the CSV (backward compatible)\n",
    "    _vp = meta_df['doi'].str.extract(r'(CD\\d+)(?:\\.pub(\\d+))?', flags=re.I)\n",
    "    meta_df['cd_number'] = _vp[0].str.upper()\n",
    "    meta_df['version'] = _vp[1].fillna(1).astype(int)\n",
    "\n",
    "has_cd = meta_df[meta_df['cd_number'].notna()]\n",
    "latest_idx = has_cd.groupby('cd_number')['version'].idxmax()\n",
    "no_cd = meta_df[meta_df['cd_number'].isna()]\n",
    "\n",
    "# Keep latest versions + any non-CD reviews\n",
    "meta_df = pd.concat([meta_df.loc[latest_idx], no_cd], ignore_index=True)\n",
    "print(f\"After version dedup: {len(meta_df):,} reviews (removed {len(has_cd) - len(latest_idx):,} superseded)\")\n",
    "\n",
    "# ── Filter to Public Health group ─────────────────────────────────────────────\n",
    "# Strict filter: only the Cochrane \"Public Health\" group, aligned with UKHSA's remit\n",
    "PUBLIC_HEALTH_GROUPS = [\n",
    "    'Public Health',\n",
    "]\n",
    "\n",
    "# Filter to PH reviews\n",
    "ph_reviews = meta_df[meta_df['cochrane_group'].isin(PUBLIC_HEALTH_GROUPS)]\n",
    "ph_review_dois = set(ph_reviews['doi'].dropna())\n",
    "\n",
    "print(f\"\\nPublic health reviews (latest only): {len(ph_reviews):,}\")\n",
    "print(\"Groups:\", \", \".join([f\"{g} ({(ph_reviews['cochrane_group']==g).sum()})\" \n",
    "                            for g in PUBLIC_HEALTH_GROUPS if (ph_reviews['cochrane_group']==g).sum() > 0]))\n",
    "\n",
    "# Filter references\n",
    "refs_df = refs_df[refs_df['review_doi'].isin(ph_review_dois)].copy()\n",
    "print(f\"\\nReferences after PH filter: {len(refs_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46a4d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total references: 5,876\n",
      "Unique references: 5,690\n",
      "Deduplication ratio: 1.0x\n",
      "\n",
      "Categories:\n",
      "category\n",
      "excluded    4173\n",
      "included    1108\n",
      "awaiting     268\n",
      "ongoing      141\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Deduplicate References\n",
    "# =============================================================================\n",
    "\n",
    "def create_ref_signature(row):\n",
    "    \"\"\"Create a normalized signature for deduplication.\"\"\"\n",
    "    title = str(row.get('title', '')).lower().strip()[:100]\n",
    "    year = str(row.get('year', ''))\n",
    "    first_author = str(row.get('authors', '')).split()[0].lower() if row.get('authors') else ''\n",
    "    return f\"{first_author}|{year}|{title[:50]}\"\n",
    "\n",
    "refs_df['signature'] = refs_df.apply(create_ref_signature, axis=1)\n",
    "unique_refs = refs_df.drop_duplicates(subset='signature').copy()\n",
    "\n",
    "print(f\"Total references: {len(refs_df):,}\")\n",
    "print(f\"Unique references: {len(unique_refs):,}\")\n",
    "print(f\"Deduplication ratio: {len(refs_df)/len(unique_refs):.1f}x\")\n",
    "print(f\"\\nCategories:\")\n",
    "print(unique_refs['category'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8ae3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 1: Direct Extraction\n",
      "============================================================\n",
      "References with DOI: 742 (13.0%)\n",
      "References with PMID: 32 (0.6%)\n",
      "Need CrossRef lookup: 4,948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanx\\AppData\\Local\\Temp\\ipykernel_27088\\1551778175.py:41: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  unique_refs['final_pmid'] = unique_refs['extracted_pmid'].combine_first(unique_refs.get('pmid'))\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: Extract DOI/PMID directly from reference text\n",
    "# =============================================================================\n",
    "# Many references already have DOI or PMID embedded in the text\n",
    "\n",
    "def extract_pmid(text):\n",
    "    \"\"\"Extract PMID from reference text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = re.search(r\"(PMID|PUBMED|MEDLINE)[:\\s]*(\\d+)\", str(text), flags=re.I)\n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    return None\n",
    "\n",
    "def extract_doi(text):\n",
    "    \"\"\"Extract DOI from reference text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = re.search(\n",
    "        r\"(?:DOI[:\\s]*|HTTPS?://(?:DX\\.)?DOI\\.ORG/)?(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\", \n",
    "        str(text), flags=re.I\n",
    "    )\n",
    "    if match:\n",
    "        doi = match.group(1)\n",
    "        return doi.rstrip(\".,;)\")  # remove trailing punctuation\n",
    "    return None\n",
    "\n",
    "# Build full reference text for extraction\n",
    "unique_refs['full_ref'] = (\n",
    "    unique_refs['title'].fillna('') + ' ' + \n",
    "    unique_refs['authors'].fillna('') + ' ' +\n",
    "    unique_refs['year'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "# Extract DOI and PMID\n",
    "unique_refs['extracted_doi'] = unique_refs['full_ref'].apply(extract_doi)\n",
    "unique_refs['extracted_pmid'] = unique_refs['full_ref'].apply(extract_pmid)\n",
    "\n",
    "# Also use ref_doi and pmid columns if available\n",
    "unique_refs['final_doi'] = unique_refs['extracted_doi'].combine_first(unique_refs.get('ref_doi'))\n",
    "unique_refs['final_pmid'] = unique_refs['extracted_pmid'].combine_first(unique_refs.get('pmid'))\n",
    "\n",
    "has_doi = unique_refs['final_doi'].notna().sum()\n",
    "has_pmid = unique_refs['final_pmid'].notna().sum()\n",
    "\n",
    "print(\"PHASE 1: Direct Extraction\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"References with DOI: {has_doi:,} ({has_doi/len(unique_refs)*100:.1f}%)\")\n",
    "print(f\"References with PMID: {has_pmid:,} ({has_pmid/len(unique_refs)*100:.1f}%)\")\n",
    "print(f\"Need CrossRef lookup: {len(unique_refs) - has_doi:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5ff427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API functions defined (enhanced with fuzzy matching):\n",
      "  • get_doi_from_crossref() - CrossRef search with fuzzy matching + short-title fallback\n",
      "  • fetch_crossref_abstract() - CrossRef abstract fetch (for PubMed fallback)\n",
      "  • get_pmid_from_doi() - DOI to PMID lookup\n",
      "  • fetch_abstracts_batch() - Batch PubMed fetch\n",
      "  • Fuzzy threshold: 75% title similarity required\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CrossRef and PubMed API Functions (Enhanced with fuzzy matching)\n",
    "# =============================================================================\n",
    "\n",
    "# ── Text Cleaning Utilities ────────────────────────────────────────────────────\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize unicode ligatures and special characters from PDFs.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # NFKD normalization handles ligatures (fi→fi, fl→fl) and other composed chars\n",
    "    text = unicodedata.normalize(\"NFKD\", str(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text_for_matching(text):\n",
    "    \"\"\"Clean text for fuzzy matching comparison.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = normalize_text(text)\n",
    "    # Remove punctuation except hyphens (keep compound words)\n",
    "    text = re.sub(r\"[^\\w\\s\\-]\", \"\", text.lower())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_reference(ref):\n",
    "    \"\"\"Clean reference text for CrossRef query.\"\"\"\n",
    "    if pd.isna(ref):\n",
    "        return \"\"\n",
    "    ref = str(ref)\n",
    "    # Unicode normalization (handles PDF ligatures)\n",
    "    ref = normalize_text(ref)\n",
    "    # Remove Cochrane-specific text\n",
    "    ref = re.sub(r\"\\(REVIEW\\).*\", \"\", ref, flags=re.I)\n",
    "    ref = re.sub(r\"TRUSTED EVIDENCE.*\", \"\", ref, flags=re.I)\n",
    "    ref = re.sub(r\"COCHRANE.*?LIBRARY\", \"\", ref, flags=re.I)\n",
    "    # Remove page markers\n",
    "    ref = re.sub(r\"---\\s*Page\\s*\\d+\\s*---\", \" \", ref)\n",
    "    # Remove excessive whitespace\n",
    "    ref = re.sub(r\"\\s{2,}\", \" \", ref)\n",
    "    return ref.strip()\n",
    "\n",
    "\n",
    "def clean_title_for_search(title):\n",
    "    \"\"\"Normalize title for CrossRef API search.\"\"\"\n",
    "    title = normalize_text(title)\n",
    "    title = title.replace(\"/\", \" \")\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "    return title\n",
    "\n",
    "\n",
    "def extract_jats_abstract(text):\n",
    "    \"\"\"Strip JATS/HTML tags from CrossRef abstract text.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text if text else None\n",
    "\n",
    "\n",
    "# ── CrossRef Utilities ─────────────────────────────────────────────────────────\n",
    "\n",
    "def format_crossref_authors(work):\n",
    "    \"\"\"Format author names from CrossRef response.\"\"\"\n",
    "    authors = work.get(\"author\", [])\n",
    "    parts = []\n",
    "    for a in authors:\n",
    "        last = a.get(\"family\", \"\")\n",
    "        given = a.get(\"given\", \"\")\n",
    "        initials = \"\".join(w[0] for w in given.split()) if given else \"\"\n",
    "        parts.append(f\"{last} {initials}\".strip())\n",
    "    return \", \".join(parts) if parts else None\n",
    "\n",
    "\n",
    "def score_crossref_candidates(candidates, query_title, query_authors):\n",
    "    \"\"\"\n",
    "    Score CrossRef candidates using fuzzy matching.\n",
    "    Returns (best_work, title_score, author_score) or (None, None, None).\n",
    "    \"\"\"\n",
    "    clean_query_title = clean_text_for_matching(query_title)\n",
    "    clean_query_authors = clean_text_for_matching(query_authors)\n",
    "    \n",
    "    scored = []\n",
    "    for item in candidates:\n",
    "        # Get title from CrossRef (usually a list with one element)\n",
    "        cr_titles = item.get(\"title\", [])\n",
    "        cr_title = cr_titles[0] if cr_titles else \"\"\n",
    "        \n",
    "        # Score title similarity\n",
    "        title_score = fuzz.token_sort_ratio(clean_query_title, clean_text_for_matching(cr_title))\n",
    "        \n",
    "        # Score author similarity\n",
    "        cr_authors = format_crossref_authors(item) or \"\"\n",
    "        author_score = fuzz.token_sort_ratio(clean_query_authors, clean_text_for_matching(cr_authors))\n",
    "        \n",
    "        scored.append((item, title_score, author_score))\n",
    "    \n",
    "    # Filter to candidates above threshold\n",
    "    above_threshold = [(item, ts, aus) for item, ts, aus in scored if ts >= FUZZY_TITLE_THRESHOLD]\n",
    "    \n",
    "    if not above_threshold:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Return best match: highest title score, break ties with author score\n",
    "    best = max(above_threshold, key=lambda x: (x[1], x[2]))\n",
    "    return best\n",
    "\n",
    "\n",
    "# ── Main CrossRef Function (Enhanced) ──────────────────────────────────────────\n",
    "\n",
    "def get_doi_from_crossref(ref, title=None, authors=None, timeout=(5, 30)):\n",
    "    \"\"\"\n",
    "    Query CrossRef API to get DOI from bibliographic reference.\n",
    "    \n",
    "    Enhanced with:\n",
    "    1. Fuzzy matching to validate results\n",
    "    2. Short-title fallback (searches before ':' or '-')\n",
    "    3. Returns match metadata for quality assessment\n",
    "    \n",
    "    Returns: (doi, match_method, title_score, author_score)\n",
    "    \n",
    "    For backward compatibility, can also be called with just `ref` and will\n",
    "    return just the DOI (legacy behavior).\n",
    "    \"\"\"\n",
    "    # Legacy mode: if only ref is provided, return just the DOI\n",
    "    legacy_mode = (title is None and authors is None)\n",
    "    \n",
    "    if not ref or len(ref) < 20:\n",
    "        return None if legacy_mode else (None, None, None, None)\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": f\"LSE-UKHSA-Project/1.0 (mailto:{Entrez.email})\" if Entrez.email else \"LSE-UKHSA-Project/1.0\"\n",
    "    }\n",
    "    \n",
    "    # Extract title/authors from ref if not provided\n",
    "    if title is None:\n",
    "        title = ref\n",
    "    if authors is None:\n",
    "        authors = \"\"\n",
    "    \n",
    "    clean_title = clean_title_for_search(title)\n",
    "    \n",
    "    def try_search(search_term, method_name):\n",
    "        \"\"\"Helper to search CrossRef and score results.\"\"\"\n",
    "        try:\n",
    "            url = \"https://api.crossref.org/works\"\n",
    "            params = {\n",
    "                \"query.bibliographic\": search_term,\n",
    "                \"rows\": 10,  # Get top 10 for fuzzy matching\n",
    "                \"select\": \"DOI,title,author,abstract\"\n",
    "            }\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            items = r.json().get(\"message\", {}).get(\"items\", [])\n",
    "            if not items:\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # In legacy mode, just return first result (backward compatible)\n",
    "            if legacy_mode:\n",
    "                return items[0].get(\"DOI\"), None, None, None\n",
    "            \n",
    "            # Score candidates\n",
    "            best_item, title_score, author_score = score_crossref_candidates(items, title, authors)\n",
    "            \n",
    "            if best_item:\n",
    "                return best_item.get(\"DOI\"), method_name, title_score, author_score\n",
    "            \n",
    "            # If no fuzzy match above threshold, check if first result is exact-ish match\n",
    "            first_title = (items[0].get(\"title\") or [\"\"])[0]\n",
    "            if clean_text_for_matching(first_title) == clean_text_for_matching(title):\n",
    "                return items[0].get(\"DOI\"), f\"{method_name}_exact\", 100, None\n",
    "                \n",
    "        except Exception:\n",
    "            pass\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Strategy 1: Full bibliographic search\n",
    "    doi, method, ts, aus = try_search(ref, \"crossref_fuzzy\")\n",
    "    if doi:\n",
    "        return doi if legacy_mode else (doi, method, ts, aus)\n",
    "    \n",
    "    # Strategy 2: Title-only search (sometimes more effective)\n",
    "    doi, method, ts, aus = try_search(clean_title, \"crossref_title\")\n",
    "    if doi:\n",
    "        return doi if legacy_mode else (doi, method, ts, aus)\n",
    "    \n",
    "    # Strategy 3: Short title (before : or -)\n",
    "    short_title = re.split(r\"\\s*[:\\-]\\s*\", clean_title)[0].strip()\n",
    "    if short_title.lower() != clean_title.lower() and len(short_title) > 20:\n",
    "        doi, method, ts, aus = try_search(short_title, \"crossref_short\")\n",
    "        if doi:\n",
    "            return doi if legacy_mode else (doi, method, ts, aus)\n",
    "    \n",
    "    return None if legacy_mode else (None, \"no_match\", None, None)\n",
    "\n",
    "\n",
    "# ── CrossRef Abstract Fetch ────────────────────────────────────────────────────\n",
    "\n",
    "def fetch_crossref_abstract(doi, timeout=(5, 30)):\n",
    "    \"\"\"\n",
    "    Fetch abstract from CrossRef for a given DOI.\n",
    "    Returns cleaned abstract text or None.\n",
    "    \"\"\"\n",
    "    if not doi:\n",
    "        return None\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": f\"LSE-UKHSA-Project/1.0 (mailto:{Entrez.email})\" if Entrez.email else \"LSE-UKHSA-Project/1.0\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://api.crossref.org/works/{doi.strip()}\"\n",
    "        r = requests.get(url, headers=headers, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            work = r.json().get(\"message\", {})\n",
    "            raw_abstract = work.get(\"abstract\", \"\")\n",
    "            return extract_jats_abstract(raw_abstract)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# ── PubMed Functions ───────────────────────────────────────────────────────────\n",
    "\n",
    "def get_pmid_from_doi(doi, api_key=None):\n",
    "    \"\"\"Look up PMID from DOI via PubMed.\"\"\"\n",
    "    if not doi:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"term\": f\"{doi}[DOI]\",\n",
    "            \"retmode\": \"json\"\n",
    "        }\n",
    "        if api_key:\n",
    "            params[\"api_key\"] = api_key\n",
    "            \n",
    "        r = requests.get(\n",
    "            \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
    "            params=params, timeout=(5, 30)\n",
    "        )\n",
    "        data = r.json()\n",
    "        idlist = data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "        return idlist[0] if idlist else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_abstracts_batch(pmids, batch_size=200, max_retries=3):\n",
    "    \"\"\"Batch fetch PubMed records (200 per request) with retry logic.\"\"\"\n",
    "    results = {}\n",
    "    pmid_list = [str(int(p)) for p in pmids if pd.notna(p) and str(p).isdigit()]\n",
    "    failed_batches = []\n",
    "    \n",
    "    for i in range(0, len(pmid_list), batch_size):\n",
    "        batch = pmid_list[i:i + batch_size]\n",
    "        success = False\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(NCBI_RATE * (attempt + 1))  # Exponential backoff\n",
    "                handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(batch), rettype=\"xml\", retmode=\"xml\")\n",
    "                records = Entrez.read(handle)\n",
    "                handle.close()\n",
    "                \n",
    "                for article in records.get('PubmedArticle', []):\n",
    "                    data = extract_record_data(article)\n",
    "                    if data:\n",
    "                        results[data['pmid']] = data\n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Batch {i//batch_size + 1} attempt {attempt + 1} failed: {e}, retrying...\")\n",
    "                else:\n",
    "                    print(f\"Batch {i//batch_size + 1} failed after {max_retries} attempts: {e}\")\n",
    "                    failed_batches.append(batch)\n",
    "    \n",
    "    if failed_batches:\n",
    "        print(f\"Warning: {len(failed_batches)} batch(es) failed permanently ({sum(len(b) for b in failed_batches)} PMIDs)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_record_data(record):\n",
    "    \"\"\"Extract fields from PubMed record.\"\"\"\n",
    "    try:\n",
    "        article = record['MedlineCitation']['Article']\n",
    "        pmid = str(record['MedlineCitation']['PMID'])\n",
    "        title = str(article.get('ArticleTitle', ''))\n",
    "        \n",
    "        # Abstract\n",
    "        abstract = ''\n",
    "        if 'Abstract' in article and 'AbstractText' in article['Abstract']:\n",
    "            parts = article['Abstract']['AbstractText']\n",
    "            abstract = ' '.join([str(p) for p in parts]) if isinstance(parts, list) else str(parts)\n",
    "        \n",
    "        # Year\n",
    "        year = ''\n",
    "        if 'Journal' in article and 'JournalIssue' in article['Journal']:\n",
    "            year = article['Journal']['JournalIssue'].get('PubDate', {}).get('Year', '')\n",
    "        \n",
    "        # Authors\n",
    "        authors = []\n",
    "        if 'AuthorList' in article:\n",
    "            for auth in article['AuthorList']:\n",
    "                if 'LastName' in auth:\n",
    "                    name = auth['LastName'] + (' ' + auth.get('Initials', ''))\n",
    "                    authors.append(name.strip())\n",
    "        \n",
    "        # DOI\n",
    "        doi = ''\n",
    "        if 'ELocationID' in article:\n",
    "            for loc in article['ELocationID']:\n",
    "                if loc.attributes.get('EIdType') == 'doi':\n",
    "                    doi = str(loc)\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'pmid': pmid, 'title': title, 'abstract': abstract,\n",
    "            'year': year, 'authors': '; '.join(authors), 'doi': doi\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"API functions defined (enhanced with fuzzy matching):\")\n",
    "print(\"  • get_doi_from_crossref() - CrossRef search with fuzzy matching + short-title fallback\")\n",
    "print(\"  • fetch_crossref_abstract() - CrossRef abstract fetch (for PubMed fallback)\")\n",
    "print(\"  • get_pmid_from_doi() - DOI to PMID lookup\")\n",
    "print(\"  • fetch_abstracts_batch() - Batch PubMed fetch\")\n",
    "print(f\"  • Fuzzy threshold: {FUZZY_TITLE_THRESHOLD}% title similarity required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3d0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 2: CrossRef DOI Lookup\n",
      "============================================================\n",
      "References needing CrossRef: 4,948\n",
      "Estimated time: ~0.7 hours\n",
      "References to process: 4,948\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: CrossRef lookup for missing DOIs\n",
    "# =============================================================================\n",
    "# Query CrossRef API for references without DOI\n",
    "\n",
    "print(\"PHASE 2: CrossRef DOI Lookup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# References that need CrossRef lookup\n",
    "refs_need_crossref = unique_refs[unique_refs['final_doi'].isna()].copy()\n",
    "print(f\"References needing CrossRef: {len(refs_need_crossref):,}\")\n",
    "\n",
    "# Time estimate\n",
    "est_hours = len(refs_need_crossref) * CROSSREF_RATE / 3600\n",
    "print(f\"Estimated time: ~{est_hours:.1f} hours\")\n",
    "\n",
    "refs_to_process = refs_need_crossref.copy()\n",
    "print(f\"References to process: {len(refs_to_process):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52c2964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cd6277f9e04768848cd93e74b2c9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CrossRef:   0%|          | 0/4948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100/4,948] Match rate: 83.0% (2635/hr)\n",
      "\n",
      "[200/4,948] Match rate: 86.0% (2705/hr)\n",
      "\n",
      "[300/4,948] Match rate: 86.3% (2754/hr)\n",
      "\n",
      "[400/4,948] Match rate: 86.2% (2742/hr)\n",
      "\n",
      "[500/4,948] Match rate: 85.8% (2731/hr)\n",
      "\n",
      "[600/4,948] Match rate: 86.5% (2728/hr)\n",
      "\n",
      "[700/4,948] Match rate: 86.6% (2741/hr)\n",
      "\n",
      "[800/4,948] Match rate: 86.4% (2728/hr)\n",
      "\n",
      "[900/4,948] Match rate: 85.8% (2692/hr)\n",
      "\n",
      "[1,000/4,948] Match rate: 84.7% (2667/hr)\n",
      "\n",
      "[1,100/4,948] Match rate: 83.9% (2650/hr)\n",
      "\n",
      "[1,200/4,948] Match rate: 81.9% (2608/hr)\n",
      "\n",
      "[1,300/4,948] Match rate: 82.5% (2629/hr)\n",
      "\n",
      "[1,400/4,948] Match rate: 80.2% (2578/hr)\n",
      "\n",
      "[1,500/4,948] Match rate: 79.1% (2558/hr)\n",
      "\n",
      "[1,600/4,948] Match rate: 77.9% (2528/hr)\n",
      "\n",
      "[1,700/4,948] Match rate: 78.0% (2531/hr)\n",
      "\n",
      "[1,800/4,948] Match rate: 77.9% (2526/hr)\n",
      "\n",
      "[1,900/4,948] Match rate: 78.2% (2525/hr)\n",
      "\n",
      "[2,000/4,948] Match rate: 78.5% (2519/hr)\n",
      "\n",
      "[2,100/4,948] Match rate: 78.6% (2516/hr)\n",
      "\n",
      "[2,200/4,948] Match rate: 78.4% (2511/hr)\n",
      "\n",
      "[2,300/4,948] Match rate: 78.5% (2518/hr)\n",
      "\n",
      "[2,400/4,948] Match rate: 78.3% (2512/hr)\n",
      "\n",
      "[2,500/4,948] Match rate: 78.2% (2507/hr)\n",
      "\n",
      "[2,600/4,948] Match rate: 77.5% (2496/hr)\n",
      "\n",
      "[2,700/4,948] Match rate: 75.6% (2461/hr)\n",
      "\n",
      "[2,800/4,948] Match rate: 74.0% (2432/hr)\n",
      "\n",
      "[2,900/4,948] Match rate: 73.2% (2418/hr)\n",
      "\n",
      "[3,000/4,948] Match rate: 73.6% (2426/hr)\n",
      "\n",
      "[3,100/4,948] Match rate: 73.8% (2426/hr)\n",
      "\n",
      "[3,200/4,948] Match rate: 74.1% (2431/hr)\n",
      "\n",
      "[3,300/4,948] Match rate: 74.3% (2436/hr)\n",
      "\n",
      "[3,400/4,948] Match rate: 73.3% (2410/hr)\n",
      "\n",
      "[3,500/4,948] Match rate: 73.6% (2414/hr)\n",
      "\n",
      "[3,600/4,948] Match rate: 73.7% (2412/hr)\n",
      "\n",
      "[3,700/4,948] Match rate: 73.7% (2413/hr)\n",
      "\n",
      "[3,800/4,948] Match rate: 74.0% (2416/hr)\n",
      "\n",
      "[3,900/4,948] Match rate: 74.3% (2415/hr)\n",
      "\n",
      "[4,000/4,948] Match rate: 74.3% (2412/hr)\n",
      "\n",
      "[4,100/4,948] Match rate: 74.6% (2414/hr)\n",
      "\n",
      "[4,200/4,948] Match rate: 74.9% (2414/hr)\n",
      "\n",
      "[4,300/4,948] Match rate: 75.2% (2410/hr)\n",
      "\n",
      "[4,400/4,948] Match rate: 75.5% (2407/hr)\n",
      "\n",
      "[4,500/4,948] Match rate: 75.6% (2401/hr)\n",
      "\n",
      "[4,600/4,948] Match rate: 76.0% (2403/hr)\n",
      "\n",
      "[4,700/4,948] Match rate: 76.3% (2407/hr)\n",
      "\n",
      "[4,800/4,948] Match rate: 76.4% (2405/hr)\n",
      "\n",
      "[4,900/4,948] Match rate: 76.5% (2396/hr)\n",
      "\n",
      "✓ CrossRef complete: 3,795 DOIs found (76.7%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2 EXECUTION: CrossRef DOI Lookup\n",
    "# =============================================================================\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "matched_count = 0\n",
    "\n",
    "for idx, (_, row) in enumerate(tqdm(refs_to_process.iterrows(), total=len(refs_to_process), desc=\"CrossRef\")):\n",
    "    # Build reference string for CrossRef query\n",
    "    ref_text = clean_reference(f\"{row['title']} {row['authors']} {row['year']}\")\n",
    "    \n",
    "    # Query CrossRef with fuzzy matching (pass title + authors explicitly)\n",
    "    crossref_doi, match_method, title_score, author_score = get_doi_from_crossref(\n",
    "        ref_text, title=row['title'], authors=row.get('authors', '')\n",
    "    )\n",
    "    time.sleep(CROSSREF_RATE)\n",
    "    \n",
    "    # Record result\n",
    "    results.append({\n",
    "        'study_id': row['study_id'],\n",
    "        'category': row['category'],\n",
    "        'original_title': row['title'],\n",
    "        'original_authors': row['authors'],\n",
    "        'original_year': row['year'],\n",
    "        'crossref_doi': crossref_doi,\n",
    "        'match_method': match_method or 'no_match',\n",
    "        'title_score': title_score,\n",
    "        'author_score': author_score\n",
    "    })\n",
    "    \n",
    "    if crossref_doi:\n",
    "        matched_count += 1\n",
    "    \n",
    "    # Print progress every 100 refs\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (idx + 1) / elapsed * 3600\n",
    "        print(f\"\\n[{idx+1:,}/{len(refs_to_process):,}] Match rate: {matched_count/(idx+1)*100:.1f}% ({rate:.0f}/hr)\")\n",
    "\n",
    "# Save final results\n",
    "crossref_results_df = pd.DataFrame(results)\n",
    "crossref_results_df.to_csv(PROGRESS_CSV, index=False)\n",
    "\n",
    "print(f\"\\n✓ CrossRef complete: {matched_count:,} DOIs found ({matched_count/len(refs_to_process)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aae098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 3: DOI → PMID Conversion\n",
      "============================================================\n",
      "Total unique DOIs: 3,633\n",
      "DOIs to look up: 3,633\n",
      "Converting DOIs to PMIDs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e34e95026b456b9bcb34f324a799a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DOI→PMID:   0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Found 2,744 PMIDs, saved to doi_pmid_cache.csv\n",
      "\n",
      "✓ PMIDs found: 2,744 / 3,633 (75.5%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 3: Convert DOIs to PMIDs\n",
    "# =============================================================================\n",
    "\n",
    "print(\"PHASE 3: DOI → PMID Conversion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reload unique_refs fresh (to avoid column conflicts from previous runs)\n",
    "unique_refs_fresh = refs_df.drop_duplicates(subset='signature').copy()\n",
    "\n",
    "# Re-extract DOIs (Phase 1 logic)\n",
    "unique_refs_fresh['full_ref'] = (\n",
    "    unique_refs_fresh['title'].fillna('') + ' ' + \n",
    "    unique_refs_fresh['authors'].fillna('') + ' ' +\n",
    "    unique_refs_fresh['year'].fillna('').astype(str)\n",
    ")\n",
    "unique_refs_fresh['extracted_doi'] = unique_refs_fresh['full_ref'].apply(extract_doi)\n",
    "unique_refs_fresh['final_doi'] = unique_refs_fresh['extracted_doi']\n",
    "\n",
    "# Combine with CrossRef DOIs\n",
    "crossref_results = pd.read_csv(PROGRESS_CSV) if PROGRESS_CSV.exists() else pd.DataFrame()\n",
    "\n",
    "if len(crossref_results) > 0:\n",
    "    # Merge on normalized title (not study_id!) to correctly match different papers\n",
    "    crossref_results['title_normalized'] = crossref_results['original_title'].str.lower().str.strip()\n",
    "    unique_refs_fresh['title_normalized'] = unique_refs_fresh['title'].str.lower().str.strip()\n",
    "    \n",
    "    crossref_doi_map = crossref_results[['title_normalized', 'crossref_doi']].drop_duplicates()\n",
    "    unique_refs_fresh = unique_refs_fresh.merge(\n",
    "        crossref_doi_map,\n",
    "        on='title_normalized',\n",
    "        how='left'\n",
    "    )\n",
    "    unique_refs_fresh['final_doi'] = unique_refs_fresh['final_doi'].combine_first(unique_refs_fresh['crossref_doi'])\n",
    "\n",
    "# Update the main variable\n",
    "unique_refs = unique_refs_fresh\n",
    "\n",
    "# Get all unique DOIs\n",
    "all_dois = unique_refs[unique_refs['final_doi'].notna()]['final_doi'].str.lower().unique()\n",
    "print(f\"Total unique DOIs: {len(all_dois):,}\")\n",
    "\n",
    "# DOI→PMID output file (used by notebook 05)\n",
    "DOI_PMID_CACHE = DATA_DIR / \"doi_pmid_cache.csv\"\n",
    "\n",
    "# Look up ALL DOIs fresh\n",
    "doi_to_pmid = {}\n",
    "print(f\"DOIs to look up: {len(all_dois):,}\")\n",
    "\n",
    "if len(all_dois) > 0:\n",
    "    print(\"Converting DOIs to PMIDs...\")\n",
    "    new_mappings = 0\n",
    "    for doi in tqdm(all_dois, desc=\"DOI→PMID\"):\n",
    "        pmid = get_pmid_from_doi(doi, Entrez.api_key)\n",
    "        if pmid:\n",
    "            doi_to_pmid[doi.lower()] = pmid\n",
    "            new_mappings += 1\n",
    "        else:\n",
    "            doi_to_pmid[doi.lower()] = None\n",
    "        time.sleep(NCBI_RATE)\n",
    "    \n",
    "    # Save results for use by notebook 05\n",
    "    cache_rows = [{'doi': k, 'pmid': v if v else 'NO_PMID'} for k, v in doi_to_pmid.items()]\n",
    "    cache_df = pd.DataFrame(cache_rows)\n",
    "    cache_df.to_csv(DOI_PMID_CACHE, index=False)\n",
    "    print(f\"\\n✓ Found {new_mappings:,} PMIDs, saved to {DOI_PMID_CACHE.name}\")\n",
    "\n",
    "# Count PMIDs found\n",
    "pmids_for_current = sum(1 for d in all_dois if doi_to_pmid.get(d.lower()) is not None)\n",
    "print(f\"\\n✓ PMIDs found: {pmids_for_current:,} / {len(all_dois):,} ({pmids_for_current/len(all_dois)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e7a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 4: Fetch Abstracts\n",
      "============================================================\n",
      "Total unique PMIDs: 2,744\n",
      "Fetching abstracts in batches...\n",
      "\n",
      "✓ Fetched 2,744 records in 98s\n",
      "  With abstracts: 2,632 (95.9%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 4: Batch Fetch Abstracts\n",
    "# =============================================================================\n",
    "\n",
    "print(\"PHASE 4: Fetch Abstracts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Map DOIs to PMIDs\n",
    "unique_refs['doi_lower'] = unique_refs['final_doi'].str.lower()\n",
    "unique_refs['pmid_from_doi'] = unique_refs['doi_lower'].map(doi_to_pmid)\n",
    "\n",
    "# Initialize final_pmid if it doesn't exist (fresh dataframe)\n",
    "if 'final_pmid' not in unique_refs.columns:\n",
    "    unique_refs['final_pmid'] = None\n",
    "unique_refs['final_pmid'] = unique_refs['final_pmid'].combine_first(unique_refs['pmid_from_doi'])\n",
    "\n",
    "# Get all unique PMIDs\n",
    "all_pmids = unique_refs[unique_refs['final_pmid'].notna()]['final_pmid'].unique()\n",
    "print(f\"Total unique PMIDs: {len(all_pmids):,}\")\n",
    "\n",
    "# Batch fetch abstracts\n",
    "print(\"Fetching abstracts in batches...\")\n",
    "start = time.time()\n",
    "abstract_records = fetch_abstracts_batch(all_pmids, batch_size=200)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n✓ Fetched {len(abstract_records):,} records in {elapsed:.0f}s\")\n",
    "with_abstract = sum(1 for r in abstract_records.values() if r.get('abstract'))\n",
    "print(f\"  With abstracts: {with_abstract:,} ({with_abstract/len(abstract_records)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8798a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 4b: CrossRef Abstract Fallback\n",
      "============================================================\n",
      "DOIs needing CrossRef abstract lookup: 1,001\n",
      "Fetching abstracts from CrossRef...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf43581d6d54d0c89ef91a64e57118f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CrossRef abstracts:   0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Found 228 abstracts from CrossRef (22.8%)\n",
      "\n",
      "Compiling final results...\n",
      "============================================================\n",
      "Abstract data built for 5,690 unique signatures\n",
      "  Abstracts from PubMed: 2,781\n",
      "  Abstracts from CrossRef (fallback): 235\n",
      "\n",
      "Total references: 5,876\n",
      "Unique (study_id, review_doi) pairs: 5,874\n",
      "\n",
      "Match methods:\n",
      "match_method\n",
      "crossref      3237\n",
      "no_match      2608\n",
      "doi_direct      31\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 4b: CrossRef Abstract Fallback + Compile Final Results\n",
    "# =============================================================================\n",
    "# For papers with DOI but no PubMed abstract, try fetching from CrossRef\n",
    "\n",
    "print(\"PHASE 4b: CrossRef Abstract Fallback\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find DOIs with no abstract\n",
    "dois_needing_fallback = []\n",
    "for _, row in unique_refs.iterrows():\n",
    "    doi = str(row.get('final_doi', '')).lower() if pd.notna(row.get('final_doi')) else None\n",
    "    if not doi or doi == 'nan':\n",
    "        continue\n",
    "    \n",
    "    pmid = str(row.get('final_pmid')) if pd.notna(row.get('final_pmid')) else None\n",
    "    record = abstract_records.get(pmid, {}) if pmid else {}\n",
    "    \n",
    "    # If no abstract from PubMed, this DOI needs CrossRef fallback\n",
    "    if not record.get('abstract'):\n",
    "        dois_needing_fallback.append(doi)\n",
    "\n",
    "dois_needing_fallback = list(set(dois_needing_fallback))\n",
    "print(f\"DOIs needing CrossRef abstract lookup: {len(dois_needing_fallback):,}\")\n",
    "\n",
    "# Fetch abstracts from CrossRef\n",
    "doi_to_crossref_abstract = {}\n",
    "if dois_needing_fallback:\n",
    "    print(\"Fetching abstracts from CrossRef...\")\n",
    "    for doi in tqdm(dois_needing_fallback, desc=\"CrossRef abstracts\"):\n",
    "        abstract = fetch_crossref_abstract(doi)\n",
    "        if abstract and len(abstract) > 50:  # Only keep meaningful abstracts\n",
    "            doi_to_crossref_abstract[doi.lower()] = abstract\n",
    "        time.sleep(0.2)  # Polite rate limit\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(doi_to_crossref_abstract):,} abstracts from CrossRef ({len(doi_to_crossref_abstract)/len(dois_needing_fallback)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"✓ No references need CrossRef abstract fallback\")\n",
    "\n",
    "# =============================================================================\n",
    "# Compile Final Results\n",
    "# =============================================================================\n",
    "# Re-expand abstract data to ALL (study_id, review_doi) pairs via signature\n",
    "\n",
    "print(\"\\nCompiling final results...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build abstract data keyed by signature (from unique_refs)\n",
    "signature_to_abstract = {}\n",
    "\n",
    "for _, row in unique_refs.iterrows():\n",
    "    sig = row['signature']\n",
    "    pmid = str(row['final_pmid']) if pd.notna(row['final_pmid']) else None\n",
    "    record = abstract_records.get(pmid, {}) if pmid else {}\n",
    "    doi = str(row['final_doi']).lower() if pd.notna(row['final_doi']) else None\n",
    "    \n",
    "    # Get abstract: prefer PubMed, fallback to CrossRef\n",
    "    abstract = record.get('abstract', '')\n",
    "    abstract_source = 'pubmed' if abstract else None\n",
    "    \n",
    "    if not abstract and doi and doi in doi_to_crossref_abstract:\n",
    "        abstract = doi_to_crossref_abstract[doi]\n",
    "        abstract_source = 'crossref'\n",
    "    \n",
    "    # Determine match method\n",
    "    if pd.notna(row.get('extracted_pmid')):\n",
    "        method = 'pmid_direct'\n",
    "    elif pd.notna(row.get('extracted_doi')) or pd.notna(row.get('ref_doi')):\n",
    "        method = 'doi_direct'\n",
    "    elif pd.notna(row.get('crossref_doi')):\n",
    "        method = 'crossref'\n",
    "    else:\n",
    "        method = 'no_match'\n",
    "    \n",
    "    signature_to_abstract[sig] = {\n",
    "        'pmid': pmid,\n",
    "        'doi': row['final_doi'],\n",
    "        'matched_title': record.get('title', ''),\n",
    "        'matched_authors': record.get('authors', ''),\n",
    "        'matched_year': record.get('year', ''),\n",
    "        'abstract': abstract,\n",
    "        'abstract_source': abstract_source,\n",
    "        'match_method': method if pmid or (doi and abstract) else 'no_match'\n",
    "    }\n",
    "\n",
    "print(f\"Abstract data built for {len(signature_to_abstract):,} unique signatures\")\n",
    "\n",
    "# Count abstracts by source\n",
    "pubmed_abstracts = sum(1 for v in signature_to_abstract.values() if v.get('abstract_source') == 'pubmed')\n",
    "crossref_abstracts_found = sum(1 for v in signature_to_abstract.values() if v.get('abstract_source') == 'crossref')\n",
    "print(f\"  Abstracts from PubMed: {pubmed_abstracts:,}\")\n",
    "print(f\"  Abstracts from CrossRef (fallback): {crossref_abstracts_found:,}\")\n",
    "\n",
    "# Re-expand to ALL original (study_id, review_doi) pairs\n",
    "output_rows = []\n",
    "\n",
    "for _, row in refs_df.iterrows():\n",
    "    sig = row['signature']\n",
    "    abstract_data = signature_to_abstract.get(sig, {})\n",
    "    \n",
    "    output_rows.append({\n",
    "        'study_id': row['study_id'],\n",
    "        'review_doi': row['review_doi'],\n",
    "        'category': row['category'],\n",
    "        'original_title': row['title'],\n",
    "        'original_authors': row['authors'],\n",
    "        'original_year': row['year'],\n",
    "        'pmid': abstract_data.get('pmid'),\n",
    "        'doi': abstract_data.get('doi'),\n",
    "        'matched_title': abstract_data.get('matched_title', ''),\n",
    "        'matched_authors': abstract_data.get('matched_authors', ''),\n",
    "        'matched_year': abstract_data.get('matched_year', ''),\n",
    "        'abstract': abstract_data.get('abstract', ''),\n",
    "        'abstract_source': abstract_data.get('abstract_source', ''),\n",
    "        'match_method': abstract_data.get('match_method', 'no_match')\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(output_rows)\n",
    "\n",
    "print(f\"\\nTotal references: {len(results_df):,}\")\n",
    "print(f\"Unique (study_id, review_doi) pairs: {results_df.groupby(['study_id', 'review_doi']).ngroups:,}\")\n",
    "print(f\"\\nMatch methods:\")\n",
    "print(results_df['match_method'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d332275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS\n",
      "============================================================\n",
      "Total matched: 3,268 / 5,876 (55.6%)\n",
      "Unique (study_id, review_doi) pairs: 3,267\n",
      "\n",
      "By category:\n",
      "category\n",
      "excluded    2367\n",
      "included     660\n",
      "awaiting     157\n",
      "ongoing       84\n",
      "\n",
      "With abstracts: 3,152 (96.5%)\n",
      "\n",
      "✓ Columns in output: ['study_id', 'review_doi', 'category', 'original_title', 'original_authors', 'original_year', 'pmid', 'doi', 'matched_title', 'matched_authors', 'matched_year', 'abstract', 'abstract_source', 'match_method']\n",
      "\n",
      "✓ Saved to c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\referenced_paper_abstracts.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Save Final Output (with review_doi preserved!)\n",
    "# =============================================================================\n",
    "\n",
    "# Filter to matched only\n",
    "matched_refs = results_df[results_df['match_method'] != 'no_match'].copy()\n",
    "\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total matched: {len(matched_refs):,} / {len(results_df):,} ({len(matched_refs)/len(results_df)*100:.1f}%)\")\n",
    "print(f\"Unique (study_id, review_doi) pairs: {matched_refs.groupby(['study_id', 'review_doi']).ngroups:,}\")\n",
    "print(f\"\\nBy category:\")\n",
    "print(matched_refs['category'].value_counts().to_string())\n",
    "\n",
    "has_abstract = matched_refs['abstract'].str.len() > 0\n",
    "print(f\"\\nWith abstracts: {has_abstract.sum():,} ({has_abstract.mean()*100:.1f}%)\")\n",
    "\n",
    "# Verify review_doi is in output\n",
    "print(f\"\\n✓ Columns in output: {matched_refs.columns.tolist()}\")\n",
    "\n",
    "# Save\n",
    "matched_refs.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n✓ Saved to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16ea7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Input: 5,694 unique references from public health reviews\n",
      "\n",
      "Phase 1 - Direct extraction:\n",
      "  DOIs extracted: 12\n",
      "\n",
      "Phase 2 - CrossRef DOI lookup (with fuzzy matching):\n",
      "  DOIs found: 3,795 / 4,948 (76.7%)\n",
      "\n",
      "Phase 3 - DOI → PMID:\n",
      "  PMIDs found: 2,744\n",
      "\n",
      "Phase 4 - Abstract fetch:\n",
      "  PubMed records fetched: 2,744\n",
      "  With PubMed abstracts: 2,632\n",
      "\n",
      "Phase 4b - CrossRef abstract fallback:\n",
      "  Additional abstracts from CrossRef: 228\n",
      "\n",
      "Total abstracts recovered: 2,860\n",
      "\n",
      "Final output: referenced_paper_abstracts.csv\n",
      "  Matched: 3,268 (57.4%)\n",
      "  By category: {'excluded': np.int64(2367), 'included': np.int64(660), 'awaiting': np.int64(157), 'ongoing': np.int64(84)}\n",
      "\n",
      "Abstract coverage:\n",
      "  References with abstracts: 3,152 / 3,268 (96.5%)\n",
      "  Source breakdown: {'pubmed': np.int64(2914), 'crossref': np.int64(238)}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Summary Statistics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput: {len(unique_refs):,} unique references from public health reviews\")\n",
    "\n",
    "print(f\"\\nPhase 1 - Direct extraction:\")\n",
    "direct_doi = unique_refs['extracted_doi'].notna().sum() if 'extracted_doi' in unique_refs.columns else 0\n",
    "print(f\"  DOIs extracted: {direct_doi:,}\")\n",
    "\n",
    "if PROGRESS_CSV.exists():\n",
    "    crossref_df = pd.read_csv(PROGRESS_CSV)\n",
    "    crossref_found = crossref_df['crossref_doi'].notna().sum()\n",
    "    print(f\"\\nPhase 2 - CrossRef DOI lookup (with fuzzy matching):\")\n",
    "    print(f\"  DOIs found: {crossref_found:,} / {len(crossref_df):,} ({crossref_found/len(crossref_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPhase 3 - DOI → PMID:\")\n",
    "actual_pmids = sum(1 for v in doi_to_pmid.values() if v is not None)\n",
    "print(f\"  PMIDs found: {actual_pmids:,}\")\n",
    "\n",
    "print(f\"\\nPhase 4 - Abstract fetch:\")\n",
    "print(f\"  PubMed records fetched: {len(abstract_records):,}\")\n",
    "pubmed_with_abstract = sum(1 for r in abstract_records.values() if r.get('abstract'))\n",
    "print(f\"  With PubMed abstracts: {pubmed_with_abstract:,}\")\n",
    "\n",
    "print(f\"\\nPhase 4b - CrossRef abstract fallback:\")\n",
    "print(f\"  Additional abstracts from CrossRef: {len(doi_to_crossref_abstract):,}\")\n",
    "\n",
    "total_abstracts = pubmed_with_abstract + len(doi_to_crossref_abstract)\n",
    "print(f\"\\nTotal abstracts recovered: {total_abstracts:,}\")\n",
    "\n",
    "print(f\"\\nFinal output: {OUTPUT_CSV.name}\")\n",
    "print(f\"  Matched: {len(matched_refs):,} ({len(matched_refs)/len(unique_refs)*100:.1f}%)\")\n",
    "print(f\"  By category: {dict(matched_refs['category'].value_counts())}\")\n",
    "\n",
    "# Abstract coverage\n",
    "with_abstract = matched_refs['abstract'].str.len() > 0 \n",
    "print(f\"\\nAbstract coverage:\")\n",
    "print(f\"  References with abstracts: {with_abstract.sum():,} / {len(matched_refs):,} ({with_abstract.mean()*100:.1f}%)\")\n",
    "if 'abstract_source' in matched_refs.columns:\n",
    "    print(f\"  Source breakdown: {dict(matched_refs[matched_refs['abstract'].str.len() > 0]['abstract_source'].value_counts())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
