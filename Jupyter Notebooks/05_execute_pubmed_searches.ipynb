{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ab9e7b",
   "metadata": {},
   "source": [
    "# 05: Execute PubMed Searches & Identify Excluded Papers\n",
    "\n",
    "## Objective\n",
    "Execute the translated PubMed search queries (from notebook 03) against the PubMed API\n",
    "to identify papers that were retrieved by systematic review searches but are **NOT** referenced\n",
    "in any Cochrane review. These form the **excluded** set for ground truth evaluation.\n",
    "\n",
    "## Revised Ground Truth Logic\n",
    "- **Included (label=1)**: ALL papers referenced in any Cochrane review\n",
    "  (regardless of internal categorization as included/excluded/awaiting/ongoing)\n",
    "- **Excluded (label=0)**: Papers that appear in PubMed search results but are\n",
    "  NOT referenced in any Cochrane review\n",
    "\n",
    "## Pipeline\n",
    "1. Load translated search strategies from `search_strategies.csv`\n",
    "2. Build \"known papers\" set from Cochrane reviews (PMIDs from notebooks 03 + 04)\n",
    "3. Execute each PubMed query via NCBI Entrez API\n",
    "4. Collect PMIDs from search results, track source review\n",
    "5. Filter out papers that appear in any Cochrane review\n",
    "6. Fetch abstracts for excluded papers\n",
    "7. Save results\n",
    "\n",
    "## Input Files\n",
    "- `Data/search_strategies.csv` — Translated PubMed queries (from notebook 03)\n",
    "- `Data/categorized_references.csv` — References extracted from PDFs (notebook 03)\n",
    "- `Data/referenced_paper_abstracts.csv` — Matched references with PMIDs (notebook 04)\n",
    "- `Data/doi_pmid_cache.csv` — DOI→PMID mappings (notebook 04)\n",
    "\n",
    "## Output Files\n",
    "- `Data/pubmed_search_results.csv` — Search execution log (counts, errors, PMIDs per query)\n",
    "- `Data/pubmed_excluded_abstracts.csv` — Abstracts for excluded-only papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fbad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q biopython pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87454cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\n",
      "NCBI API key configured: True\n",
      "Rate limit: 9 requests/sec\n",
      "Max PMIDs per query: 10,000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from Bio import Entrez\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# NCBI API — with API key allows 10 requests/sec\n",
    "Entrez.email = os.environ.get(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.environ.get(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "NCBI_RATE = 0.11 if Entrez.api_key else 0.34  # seconds between requests\n",
    "SEARCH_RETMAX = 10000  # max PMIDs to retrieve per query\n",
    "\n",
    "# Paths\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "\n",
    "# Input files\n",
    "STRATEGIES_CSV  = DATA_DIR / \"search_strategies.csv\"\n",
    "REFS_CSV        = DATA_DIR / \"categorized_references.csv\"\n",
    "ABSTRACTS_CSV   = DATA_DIR / \"referenced_paper_abstracts.csv\"\n",
    "DOI_PMID_CACHE  = DATA_DIR / \"doi_pmid_cache.csv\"\n",
    "\n",
    "# Output files\n",
    "SEARCH_RESULTS_CSV    = DATA_DIR / \"pubmed_search_results.csv\"\n",
    "SEARCH_PROGRESS_CSV   = DATA_DIR / \"pubmed_search_progress.csv\"\n",
    "EXCLUDED_ABSTRACTS_CSV = DATA_DIR / \"pubmed_excluded_abstracts.csv\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"NCBI API key configured: {bool(Entrez.api_key)}\")\n",
    "print(f\"Rate limit: {1/NCBI_RATE:.0f} requests/sec\")\n",
    "print(f\"Max PMIDs per query: {SEARCH_RETMAX:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a50a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total strategies: 16,906\n",
      "\n",
      "Translation notes:\n",
      "translation_notes\n",
      "narrative_only                                       8785\n",
      "success                                              1921\n",
      "adjacency_converted_to_AND                           1657\n",
      "filtered_not_a_query                                 1329\n",
      "mp_approximated; adjacency_converted_to_AND           651\n",
      "mp_approximated                                       561\n",
      "no_numbered_lines                                     548\n",
      "wildcard_approximated; adjacency_converted_to_AND     249\n",
      "adjacency_converted_to_AND; wildcard_approximated     213\n",
      "wildcard_approximated                                 187\n",
      "\n",
      "Translated queries: 6,197\n",
      "Unique queries after dedup: 4,869\n",
      "  (covers 6,197 review-strategy pairs)\n",
      "\n",
      "Query length stats:\n",
      "count    4869.000000\n",
      "mean      462.298008\n",
      "std       485.262759\n",
      "min         6.000000\n",
      "25%       117.000000\n",
      "50%       275.000000\n",
      "75%       674.000000\n",
      "max      3970.000000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Search Strategies\n",
    "# =============================================================================\n",
    "\n",
    "strategies = pd.read_csv(STRATEGIES_CSV)\n",
    "print(f\"Total strategies: {len(strategies):,}\")\n",
    "print(f\"\\nTranslation notes:\")\n",
    "print(strategies['translation_notes'].value_counts().head(10).to_string())\n",
    "\n",
    "# Filter to successfully translated queries\n",
    "translated = strategies[strategies['pubmed_query'].notna()].copy()\n",
    "print(f\"\\nTranslated queries: {len(translated):,}\")\n",
    "\n",
    "# Deduplicate identical queries (different versions of the same review often share a search)\n",
    "translated['query_hash'] = translated['pubmed_query'].apply(hash)\n",
    "unique_queries = translated.drop_duplicates(subset='query_hash').copy()\n",
    "\n",
    "# Build mapping: query_hash → list of review DOIs\n",
    "query_to_reviews = translated.groupby('query_hash')['doi'].apply(list).to_dict()\n",
    "\n",
    "print(f\"Unique queries after dedup: {len(unique_queries):,}\")\n",
    "print(f\"  (covers {len(translated):,} review-strategy pairs)\")\n",
    "print(f\"\\nQuery length stats:\")\n",
    "print(unique_queries['pubmed_query'].str.len().describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eaeb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 1 — Direct extraction:      34,881 PMIDs\n",
      "Source 2 — Matched abstracts:      23,182 PMIDs\n",
      "Source 3 — DOI→PMID cache:        23,182 PMIDs\n",
      "\n",
      "==================================================\n",
      "Total known PMIDs (in Cochrane reviews): 56,814\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Build Known PMID Set (papers already in Cochrane reviews)\n",
    "# =============================================================================\n",
    "# Union of all PMIDs from: direct extraction (notebook 03), CrossRef matching\n",
    "# (notebook 04), and DOI→PMID cache (notebook 04).\n",
    "\n",
    "known_pmids = set()\n",
    "\n",
    "# Source 1: Direct PMIDs from reference extraction\n",
    "if REFS_CSV.exists():\n",
    "    refs = pd.read_csv(REFS_CSV, usecols=['pmid'], dtype=str)\n",
    "    direct = {p.strip() for p in refs['pmid'].dropna() if p.strip().isdigit()}\n",
    "    known_pmids |= direct\n",
    "    print(f\"Source 1 — Direct extraction:    {len(direct):>8,} PMIDs\")\n",
    "\n",
    "# Source 2: Matched PMIDs from abstract fetching\n",
    "if ABSTRACTS_CSV.exists():\n",
    "    abs_df = pd.read_csv(ABSTRACTS_CSV, usecols=['pmid'], dtype=str)\n",
    "    matched = {p.strip() for p in abs_df['pmid'].dropna() if p.strip().isdigit()}\n",
    "    known_pmids |= matched\n",
    "    print(f\"Source 2 — Matched abstracts:    {len(matched):>8,} PMIDs\")\n",
    "\n",
    "# Source 3: DOI→PMID cache\n",
    "if DOI_PMID_CACHE.exists():\n",
    "    cache = pd.read_csv(DOI_PMID_CACHE, dtype=str)\n",
    "    cached = {p.strip() for p in cache[cache['pmid'] != 'NO_PMID']['pmid'].dropna() if p.strip().isdigit()}\n",
    "    known_pmids |= cached\n",
    "    print(f\"Source 3 — DOI→PMID cache:      {len(cached):>8,} PMIDs\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Total known PMIDs (in Cochrane reviews): {len(known_pmids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf068f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined:\n",
      "  • execute_pubmed_search() — Run a PubMed query, get PMIDs\n",
      "  • fetch_abstracts_batch()  — Batch fetch abstracts by PMID\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PubMed Search & Abstract Fetch Functions\n",
    "# =============================================================================\n",
    "\n",
    "def execute_pubmed_search(query, retmax=10000, max_retries=3):\n",
    "    \"\"\"Execute a PubMed search and return PMIDs + total count.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: pmids (list[str]), count (int), error (str|None),\n",
    "              retmax_hit (bool — True if count > retmax, results truncated)\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=query,\n",
    "                retmax=retmax,\n",
    "                usehistory=\"y\",\n",
    "            )\n",
    "            results = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            count = int(results.get('Count', 0))\n",
    "            pmids = [str(p) for p in results.get('IdList', [])]\n",
    "\n",
    "            return {\n",
    "                'pmids': pmids,\n",
    "                'count': count,\n",
    "                'error': None,\n",
    "                'retmax_hit': count > retmax,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 1))\n",
    "            else:\n",
    "                return {'pmids': [], 'count': 0, 'error': str(e), 'retmax_hit': False}\n",
    "\n",
    "\n",
    "def extract_pubmed_record(record):\n",
    "    \"\"\"Extract key fields from a PubMed XML article record.\"\"\"\n",
    "    try:\n",
    "        cit = record['MedlineCitation']\n",
    "        art = cit['Article']\n",
    "        pmid = str(cit['PMID'])\n",
    "        title = str(art.get('ArticleTitle', ''))\n",
    "\n",
    "        # Abstract\n",
    "        abstract = ''\n",
    "        if 'Abstract' in art and 'AbstractText' in art['Abstract']:\n",
    "            parts = art['Abstract']['AbstractText']\n",
    "            abstract = ' '.join(str(p) for p in parts) if isinstance(parts, list) else str(parts)\n",
    "\n",
    "        # Year\n",
    "        year = ''\n",
    "        if 'Journal' in art and 'JournalIssue' in art['Journal']:\n",
    "            year = art['Journal']['JournalIssue'].get('PubDate', {}).get('Year', '')\n",
    "\n",
    "        # Authors\n",
    "        authors = []\n",
    "        if 'AuthorList' in art:\n",
    "            for auth in art['AuthorList']:\n",
    "                if 'LastName' in auth:\n",
    "                    name = auth['LastName']\n",
    "                    if auth.get('Initials'):\n",
    "                        name += ' ' + auth['Initials']\n",
    "                    authors.append(name)\n",
    "\n",
    "        # DOI\n",
    "        doi = ''\n",
    "        if 'ELocationID' in art:\n",
    "            for loc in art['ELocationID']:\n",
    "                if hasattr(loc, 'attributes') and loc.attributes.get('EIdType') == 'doi':\n",
    "                    doi = str(loc)\n",
    "                    break\n",
    "\n",
    "        return {'pmid': pmid, 'title': title, 'abstract': abstract,\n",
    "                'year': year, 'authors': '; '.join(authors), 'doi': doi}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_abstracts_batch(pmids, batch_size=200, max_retries=3):\n",
    "    \"\"\"Batch-fetch PubMed records. Returns {pmid: record_dict}.\"\"\"\n",
    "    results = {}\n",
    "    pmid_list = [str(p) for p in pmids if str(p).isdigit()]\n",
    "\n",
    "    batches = range(0, len(pmid_list), batch_size)\n",
    "    for i in tqdm(batches, desc=\"Fetching abstracts\"):\n",
    "        batch = pmid_list[i:i + batch_size]\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(NCBI_RATE * (attempt + 1))\n",
    "                handle = Entrez.efetch(\n",
    "                    db=\"pubmed\", id=\",\".join(batch),\n",
    "                    rettype=\"xml\", retmode=\"xml\"\n",
    "                )\n",
    "                records = Entrez.read(handle)\n",
    "                handle.close()\n",
    "\n",
    "                for article in records.get('PubmedArticle', []):\n",
    "                    data = extract_pubmed_record(article)\n",
    "                    if data:\n",
    "                        results[data['pmid']] = data\n",
    "                break  # success\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"  Batch {i//batch_size + 1} failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Functions defined:\")\n",
    "print(\"  • execute_pubmed_search() — Run a PubMed query, get PMIDs\")\n",
    "print(\"  • fetch_abstracts_batch()  — Batch fetch abstracts by PMID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44418324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING PUBMED SEARCHES\n",
      "============================================================\n",
      "Queries to execute: 4,869\n",
      "Estimated time: ~9 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95396cc785504edcac887f901219acaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PubMed Search:   0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/4,869] 576,029 PMIDs | 0 errors | 53 capped | 26 q/min\n",
      "  [200/4,869] 1,107,753 PMIDs | 0 errors | 96 capped | 26 q/min\n",
      "  [300/4,869] 1,563,219 PMIDs | 0 errors | 136 capped | 25 q/min\n",
      "  [400/4,869] 2,075,111 PMIDs | 0 errors | 180 capped | 24 q/min\n",
      "  [500/4,869] 2,442,827 PMIDs | 0 errors | 212 capped | 25 q/min\n",
      "  [600/4,869] 2,922,256 PMIDs | 0 errors | 252 capped | 24 q/min\n",
      "  [700/4,869] 3,491,760 PMIDs | 0 errors | 301 capped | 24 q/min\n",
      "  [800/4,869] 4,074,792 PMIDs | 0 errors | 354 capped | 24 q/min\n",
      "  [900/4,869] 4,621,398 PMIDs | 0 errors | 399 capped | 24 q/min\n",
      "  [1,000/4,869] 5,035,869 PMIDs | 0 errors | 430 capped | 24 q/min\n",
      "  [1,100/4,869] 5,551,841 PMIDs | 0 errors | 471 capped | 24 q/min\n",
      "  [1,200/4,869] 5,979,161 PMIDs | 0 errors | 508 capped | 24 q/min\n",
      "  [1,300/4,869] 6,459,396 PMIDs | 0 errors | 549 capped | 25 q/min\n",
      "  [1,400/4,869] 6,982,257 PMIDs | 0 errors | 594 capped | 25 q/min\n",
      "  [1,500/4,869] 7,493,611 PMIDs | 0 errors | 637 capped | 24 q/min\n",
      "  [1,600/4,869] 8,001,462 PMIDs | 0 errors | 681 capped | 25 q/min\n",
      "  [1,700/4,869] 8,521,754 PMIDs | 0 errors | 728 capped | 25 q/min\n",
      "  [1,800/4,869] 9,118,265 PMIDs | 0 errors | 778 capped | 25 q/min\n",
      "  [1,900/4,869] 9,613,968 PMIDs | 0 errors | 817 capped | 25 q/min\n",
      "  [2,000/4,869] 10,177,173 PMIDs | 0 errors | 865 capped | 25 q/min\n",
      "  [2,100/4,869] 10,779,887 PMIDs | 0 errors | 918 capped | 25 q/min\n",
      "  [2,200/4,869] 11,238,362 PMIDs | 0 errors | 956 capped | 25 q/min\n",
      "  [2,300/4,869] 11,624,306 PMIDs | 0 errors | 988 capped | 25 q/min\n",
      "  [2,400/4,869] 12,157,779 PMIDs | 0 errors | 1034 capped | 25 q/min\n",
      "  [2,500/4,869] 12,761,930 PMIDs | 0 errors | 1082 capped | 25 q/min\n",
      "  [2,600/4,869] 13,213,333 PMIDs | 0 errors | 1119 capped | 25 q/min\n",
      "  [2,700/4,869] 13,742,103 PMIDs | 0 errors | 1162 capped | 25 q/min\n",
      "  [2,800/4,869] 14,243,794 PMIDs | 0 errors | 1204 capped | 25 q/min\n",
      "  [2,900/4,869] 14,786,984 PMIDs | 0 errors | 1254 capped | 25 q/min\n",
      "  [3,000/4,869] 15,152,822 PMIDs | 0 errors | 1283 capped | 25 q/min\n",
      "  [3,100/4,869] 15,690,910 PMIDs | 0 errors | 1329 capped | 25 q/min\n",
      "  [3,200/4,869] 16,174,809 PMIDs | 0 errors | 1372 capped | 25 q/min\n",
      "  [3,300/4,869] 16,631,476 PMIDs | 0 errors | 1410 capped | 25 q/min\n",
      "  [3,400/4,869] 17,107,692 PMIDs | 0 errors | 1452 capped | 25 q/min\n",
      "  [3,500/4,869] 17,562,278 PMIDs | 0 errors | 1491 capped | 25 q/min\n",
      "  [3,600/4,869] 18,037,523 PMIDs | 0 errors | 1532 capped | 25 q/min\n",
      "  [3,700/4,869] 18,665,897 PMIDs | 0 errors | 1585 capped | 26 q/min\n",
      "  [3,800/4,869] 19,249,725 PMIDs | 0 errors | 1637 capped | 25 q/min\n",
      "  [3,900/4,869] 19,803,467 PMIDs | 0 errors | 1687 capped | 25 q/min\n",
      "  [4,000/4,869] 20,305,546 PMIDs | 0 errors | 1732 capped | 25 q/min\n",
      "  [4,100/4,869] 20,751,585 PMIDs | 0 errors | 1765 capped | 25 q/min\n",
      "  [4,200/4,869] 21,294,317 PMIDs | 0 errors | 1813 capped | 26 q/min\n",
      "  [4,300/4,869] 21,806,472 PMIDs | 0 errors | 1857 capped | 26 q/min\n",
      "  [4,400/4,869] 22,427,428 PMIDs | 0 errors | 1913 capped | 26 q/min\n",
      "  [4,500/4,869] 23,020,526 PMIDs | 0 errors | 1967 capped | 26 q/min\n",
      "  [4,600/4,869] 23,545,809 PMIDs | 0 errors | 2012 capped | 25 q/min\n",
      "  [4,700/4,869] 24,178,505 PMIDs | 0 errors | 2068 capped | 25 q/min\n",
      "  [4,800/4,869] 24,751,047 PMIDs | 0 errors | 2122 capped | 25 q/min\n",
      "\n",
      "============================================================\n",
      "Complete: 4,869 queries in 193.7 min\n",
      "  Total PMIDs retrieved: 25,168,669\n",
      "  Errors: 0\n",
      "  Queries exceeding retmax (10,000): 2,159\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Execute PubMed Searches (with checkpoint every 100 queries)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"EXECUTING PUBMED SEARCHES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "if SEARCH_PROGRESS_CSV.exists():\n",
    "    progress = pd.read_csv(SEARCH_PROGRESS_CSV)\n",
    "    done_hashes = set(progress['query_hash'])\n",
    "    print(f\"Resuming from checkpoint: {len(done_hashes):,} queries already done\")\n",
    "else:\n",
    "    progress = pd.DataFrame()\n",
    "    done_hashes = set()\n",
    "\n",
    "remaining = unique_queries[~unique_queries['query_hash'].isin(done_hashes)]\n",
    "print(f\"Queries to execute: {len(remaining):,}\")\n",
    "print(f\"Estimated time: ~{len(remaining) * NCBI_RATE / 60:.0f} min\")\n",
    "\n",
    "# --- Main loop ---\n",
    "batch_results = []\n",
    "total_pmids = 0\n",
    "errors = 0\n",
    "capped = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, (_, row) in enumerate(tqdm(remaining.iterrows(),\n",
    "                                     total=len(remaining), desc=\"PubMed Search\")):\n",
    "    query = row['pubmed_query']\n",
    "    review_dois = query_to_reviews.get(row['query_hash'], [row['doi']])\n",
    "\n",
    "    result = execute_pubmed_search(query, retmax=SEARCH_RETMAX)\n",
    "    time.sleep(NCBI_RATE)\n",
    "\n",
    "    batch_results.append({\n",
    "        'query_hash': row['query_hash'],\n",
    "        'review_doi': row['doi'],\n",
    "        'all_review_dois': '|'.join(review_dois),\n",
    "        'query_length': len(query),\n",
    "        'result_count': result['count'],\n",
    "        'pmids_retrieved': len(result['pmids']),\n",
    "        'retmax_hit': result['retmax_hit'],\n",
    "        'error': result['error'],\n",
    "        'pmids_json': json.dumps(result['pmids']),\n",
    "    })\n",
    "\n",
    "    total_pmids += len(result['pmids'])\n",
    "    if result['error']:\n",
    "        errors += 1\n",
    "    if result['retmax_hit']:\n",
    "        capped += 1\n",
    "\n",
    "    # Checkpoint every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        batch_df = pd.DataFrame(batch_results)\n",
    "        combined = pd.concat([progress, batch_df], ignore_index=True)\n",
    "        combined.to_csv(SEARCH_PROGRESS_CSV, index=False)\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (idx + 1) / elapsed * 60\n",
    "        print(f\"  [{idx+1:,}/{len(remaining):,}] \"\n",
    "              f\"{total_pmids:,} PMIDs | {errors} errors | \"\n",
    "              f\"{capped} capped | {rate:.0f} q/min\")\n",
    "\n",
    "# Final save\n",
    "if batch_results:\n",
    "    batch_df = pd.DataFrame(batch_results)\n",
    "    combined = pd.concat([progress, batch_df], ignore_index=True)\n",
    "    combined.to_csv(SEARCH_PROGRESS_CSV, index=False)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Complete: {len(remaining):,} queries in {elapsed/60:.1f} min\")\n",
    "print(f\"  Total PMIDs retrieved: {total_pmids:,}\")\n",
    "print(f\"  Errors: {errors:,}\")\n",
    "print(f\"  Queries exceeding retmax ({SEARCH_RETMAX:,}): {capped:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c3cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTIFYING EXCLUDED PAPERS\n",
      "============================================================\n",
      "Total queries executed: 4,869\n",
      "  With results: 4,308\n",
      "  Errors: 0\n",
      "  Hit retmax cap (10,000): 2,159\n",
      "\n",
      "After quality filter (non-capped, with results):\n",
      "  Retained queries: 2,149\n",
      "  Dropped (capped/empty): 2,720\n",
      "\n",
      "Result count stats for retained queries:\n",
      "count    2149.000000\n",
      "mean     1666.276408\n",
      "std      2319.338814\n",
      "min         1.000000\n",
      "25%        66.000000\n",
      "50%       541.000000\n",
      "75%      2363.000000\n",
      "max      9995.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88084df97abc4dae9a11ff2de9edcf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting PMIDs:   0%|          | 0/2149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique PMIDs from filtered searches: 2,661,996\n",
      "\n",
      "Overlap with Cochrane reviews:  19,244 PMIDs\n",
      "Excluded (search-only):         2,642,752 PMIDs\n",
      "Exclusion rate:                 99.3%\n",
      "\n",
      "Excluded (review, pmid) pairs:  4,146,832\n",
      "Unique reviews with excluded:   2,402\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Filter Queries & Identify Excluded PMIDs\n",
    "# =============================================================================\n",
    "\n",
    "print(\"IDENTIFYING EXCLUDED PAPERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load full search results\n",
    "results_df = pd.read_csv(SEARCH_PROGRESS_CSV)\n",
    "print(f\"Total queries executed: {len(results_df):,}\")\n",
    "print(f\"  With results: {(results_df['result_count'] > 0).sum():,}\")\n",
    "print(f\"  Errors: {results_df['error'].notna().sum():,}\")\n",
    "print(f\"  Hit retmax cap ({SEARCH_RETMAX:,}): {results_df['retmax_hit'].sum():,}\")\n",
    "\n",
    "# ---- QUALITY FILTER ----\n",
    "# Queries that hit the retmax cap are overly broad (100K–35M results).\n",
    "# Real systematic review searches typically return a few hundred to a few\n",
    "# thousand results. Restrict to queries that returned ALL their results\n",
    "# (i.e., result_count <= retmax) and have at least 1 result.\n",
    "MAX_RESULTS = 10_000  # match SEARCH_RETMAX\n",
    "\n",
    "successful = results_df[\n",
    "    (results_df['error'].isna()) &\n",
    "    (results_df['result_count'] > 0) &\n",
    "    (~results_df['retmax_hit'])\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nAfter quality filter (non-capped, with results):\")\n",
    "print(f\"  Retained queries: {len(successful):,}\")\n",
    "print(f\"  Dropped (capped/empty): {len(results_df) - len(successful):,}\")\n",
    "print(f\"\\nResult count stats for retained queries:\")\n",
    "print(successful['result_count'].describe().to_string())\n",
    "\n",
    "# Collect all unique PMIDs and track source reviews\n",
    "all_search_pmids = set()\n",
    "pmid_to_reviews = defaultdict(set)  # pmid → set of review DOIs\n",
    "\n",
    "for _, row in tqdm(successful.iterrows(), total=len(successful), desc=\"Collecting PMIDs\"):\n",
    "    if pd.isna(row['pmids_json']) or row['pmids_json'] == '[]':\n",
    "        continue\n",
    "    pmids = json.loads(row['pmids_json'])\n",
    "    review_dois = row['all_review_dois'].split('|')\n",
    "    for pmid in pmids:\n",
    "        all_search_pmids.add(pmid)\n",
    "        for rdoi in review_dois:\n",
    "            pmid_to_reviews[pmid].add(rdoi)\n",
    "\n",
    "print(f\"\\nTotal unique PMIDs from filtered searches: {len(all_search_pmids):,}\")\n",
    "\n",
    "# Partition into included (in Cochrane) and excluded (search-only)\n",
    "overlap_pmids  = all_search_pmids & known_pmids\n",
    "excluded_pmids = all_search_pmids - known_pmids\n",
    "\n",
    "print(f\"\\nOverlap with Cochrane reviews:  {len(overlap_pmids):,} PMIDs\")\n",
    "print(f\"Excluded (search-only):         {len(excluded_pmids):,} PMIDs\")\n",
    "print(f\"Exclusion rate:                 {len(excluded_pmids)/max(len(all_search_pmids),1)*100:.1f}%\")\n",
    "\n",
    "# Build excluded (review_doi, pmid) pairs for ground truth\n",
    "excluded_pairs = []\n",
    "for pmid in excluded_pmids:\n",
    "    for rdoi in pmid_to_reviews[pmid]:\n",
    "        excluded_pairs.append({'review_doi': rdoi, 'pmid': pmid})\n",
    "\n",
    "excluded_pairs_df = pd.DataFrame(excluded_pairs).drop_duplicates()\n",
    "print(f\"\\nExcluded (review, pmid) pairs:  {len(excluded_pairs_df):,}\")\n",
    "print(f\"Unique reviews with excluded:   {excluded_pairs_df['review_doi'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b18540ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FETCHING ABSTRACTS FOR EXCLUDED PAPERS\n",
      "============================================================\n",
      "Excluded PMIDs (2,642,752) exceeds cap (200,000).\n",
      "Randomly sampling 200,000 for abstract fetching.\n",
      "Estimated: 1,001 batches, ~2 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d6739d7853426ebb3bdd261b9a51bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching abstracts:   0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetched 199,645 records in 141.1 min\n",
      "  With abstracts:    175,578 (87.9%)\n",
      "  Without abstracts: 24,067\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Fetch Abstracts for Excluded Papers\n",
    "# =============================================================================\n",
    "\n",
    "print(\"FETCHING ABSTRACTS FOR EXCLUDED PAPERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# If the excluded set is very large, sample to keep manageable\n",
    "MAX_ABSTRACT_FETCH = 200_000  # adjust as needed\n",
    "pmids_to_fetch = list(excluded_pmids)\n",
    "\n",
    "if len(pmids_to_fetch) > MAX_ABSTRACT_FETCH:\n",
    "    print(f\"Excluded PMIDs ({len(pmids_to_fetch):,}) exceeds cap ({MAX_ABSTRACT_FETCH:,}).\")\n",
    "    print(f\"Randomly sampling {MAX_ABSTRACT_FETCH:,} for abstract fetching.\")\n",
    "    rng = np.random.default_rng(42)\n",
    "    pmids_to_fetch = list(rng.choice(pmids_to_fetch, size=MAX_ABSTRACT_FETCH, replace=False))\n",
    "else:\n",
    "    print(f\"Fetching abstracts for all {len(pmids_to_fetch):,} excluded PMIDs\")\n",
    "\n",
    "est_batches = len(pmids_to_fetch) // 200 + 1\n",
    "est_min = est_batches * NCBI_RATE / 60\n",
    "print(f\"Estimated: {est_batches:,} batches, ~{max(est_min, 1):.0f} min\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "excluded_records = fetch_abstracts_batch(pmids_to_fetch, batch_size=200)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "with_abstract = sum(1 for r in excluded_records.values() if r.get('abstract'))\n",
    "print(f\"\\nFetched {len(excluded_records):,} records in {elapsed/60:.1f} min\")\n",
    "print(f\"  With abstracts:    {with_abstract:,} ({with_abstract/max(len(excluded_records),1)*100:.1f}%)\")\n",
    "print(f\"  Without abstracts: {len(excluded_records) - with_abstract:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40723ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING EXCLUDED PAPERS DATASET\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dd3dbe40e8492fb5738971d96cfb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building output:   0%|          | 0/4146832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total excluded pairs with records: 313,024\n",
      "With abstracts (>50 chars):        279,373\n",
      "Skipped (no PubMed record):        3,833,808\n",
      "\n",
      "Unique excluded PMIDs with abstract: 175,562\n",
      "Unique reviews represented:          2,147\n",
      "\n",
      "✓ Saved to pubmed_excluded_abstracts.csv\n",
      "✓ Search summary saved to pubmed_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Build & Save Final Output\n",
    "# =============================================================================\n",
    "\n",
    "print(\"BUILDING EXCLUDED PAPERS DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Join excluded pairs with fetched abstracts\n",
    "output_rows = []\n",
    "no_record = 0\n",
    "\n",
    "for _, pair in tqdm(excluded_pairs_df.iterrows(), total=len(excluded_pairs_df),\n",
    "                     desc=\"Building output\"):\n",
    "    pmid = pair['pmid']\n",
    "    rec = excluded_records.get(pmid)\n",
    "    if rec is None:\n",
    "        no_record += 1\n",
    "        continue\n",
    "\n",
    "    output_rows.append({\n",
    "        'review_doi': pair['review_doi'],\n",
    "        'pmid': pmid,\n",
    "        'title': rec.get('title', ''),\n",
    "        'abstract': rec.get('abstract', ''),\n",
    "        'authors': rec.get('authors', ''),\n",
    "        'year': rec.get('year', ''),\n",
    "        'doi': rec.get('doi', ''),\n",
    "    })\n",
    "\n",
    "excluded_df = pd.DataFrame(output_rows)\n",
    "\n",
    "# Filter to rows that actually have an abstract (required for LLM evaluation)\n",
    "excluded_with_abs = excluded_df[\n",
    "    excluded_df['abstract'].notna() &\n",
    "    (excluded_df['abstract'].str.len() > 50)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nTotal excluded pairs with records: {len(excluded_df):,}\")\n",
    "print(f\"With abstracts (>50 chars):        {len(excluded_with_abs):,}\")\n",
    "print(f\"Skipped (no PubMed record):        {no_record:,}\")\n",
    "print(f\"\\nUnique excluded PMIDs with abstract: {excluded_with_abs['pmid'].nunique():,}\")\n",
    "print(f\"Unique reviews represented:          {excluded_with_abs['review_doi'].nunique():,}\")\n",
    "\n",
    "# Save\n",
    "excluded_with_abs.to_csv(EXCLUDED_ABSTRACTS_CSV, index=False)\n",
    "print(f\"\\n✓ Saved to {EXCLUDED_ABSTRACTS_CSV.name}\")\n",
    "\n",
    "# Also save the search results summary (without PMIDs JSON for smaller file)\n",
    "search_summary = results_df.drop(columns=['pmids_json'], errors='ignore')\n",
    "search_summary.to_csv(SEARCH_RESULTS_CSV, index=False)\n",
    "print(f\"✓ Search summary saved to {SEARCH_RESULTS_CSV.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84fc6d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Input:\n",
      "  Translated search queries:     4,869\n",
      "  Known PMIDs (Cochrane refs):   56,814\n",
      "\n",
      "PubMed Searches:\n",
      "  Queries executed:              4,869\n",
      "  Successful:                    2,149\n",
      "  Unique PMIDs found:            2,661,996\n",
      "\n",
      "Excluded Papers:\n",
      "  Excluded PMIDs (total):        2,642,752\n",
      "  With abstracts fetched:        175,562\n",
      "  (review, paper) pairs:         279,373\n",
      "\n",
      "Output Files:\n",
      "  pubmed_excluded_abstracts.csv\n",
      "  pubmed_search_results.csv\n",
      "\n",
      "✓ Ready for notebook 06 (build ground truth)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Summary Statistics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput:\")\n",
    "print(f\"  Translated search queries:     {len(unique_queries):,}\")\n",
    "print(f\"  Known PMIDs (Cochrane refs):   {len(known_pmids):,}\")\n",
    "\n",
    "print(f\"\\nPubMed Searches:\")\n",
    "print(f\"  Queries executed:              {len(results_df):,}\")\n",
    "print(f\"  Successful:                    {len(successful):,}\")\n",
    "print(f\"  Unique PMIDs found:            {len(all_search_pmids):,}\")\n",
    "\n",
    "print(f\"\\nExcluded Papers:\")\n",
    "print(f\"  Excluded PMIDs (total):        {len(excluded_pmids):,}\")\n",
    "print(f\"  With abstracts fetched:        {excluded_with_abs['pmid'].nunique():,}\")\n",
    "print(f\"  (review, paper) pairs:         {len(excluded_with_abs):,}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  {EXCLUDED_ABSTRACTS_CSV.name}\")\n",
    "print(f\"  {SEARCH_RESULTS_CSV.name}\")\n",
    "\n",
    "print(f\"\\n✓ Ready for notebook 06 (build ground truth)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
