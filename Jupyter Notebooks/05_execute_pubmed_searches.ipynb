{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ab9e7b",
   "metadata": {},
   "source": [
    "# 05: Execute PubMed Searches & Identify Excluded Papers\n",
    "\n",
    "## Objective\n",
    "Execute the translated PubMed search queries (from notebook 03) against the PubMed API\n",
    "to identify papers that were retrieved by systematic review searches but are **NOT** referenced\n",
    "in any Cochrane review. These form the **excluded** set for ground truth evaluation.\n",
    "\n",
    "## Revised Ground Truth Logic\n",
    "- **Included (label=1)**: ALL papers referenced in any Cochrane review\n",
    "  (regardless of internal categorization as included/excluded/awaiting/ongoing)\n",
    "- **Excluded (label=0)**: Papers that appear in PubMed search results but are\n",
    "  NOT referenced in any Cochrane review\n",
    "\n",
    "## Pipeline\n",
    "1. Load translated search strategies from `search_strategies.csv`\n",
    "2. Build \"known papers\" set from Cochrane reviews (PMIDs from notebooks 03 + 04)\n",
    "3. Execute each PubMed query via NCBI Entrez API\n",
    "4. Collect PMIDs from search results, track source review\n",
    "5. Filter out papers that appear in any Cochrane review\n",
    "6. Fetch abstracts for excluded papers\n",
    "7. Save results\n",
    "\n",
    "## Input Files\n",
    "- `Data/search_strategies.csv` — Translated PubMed queries (from notebook 03)\n",
    "- `Data/categorized_references.csv` — References extracted from PDFs (notebook 03)\n",
    "- `Data/referenced_paper_abstracts.csv` — Matched references with PMIDs (notebook 04)\n",
    "- `Data/doi_pmid_cache.csv` — DOI→PMID mappings (notebook 04)\n",
    "\n",
    "## Output Files\n",
    "- `Data/pubmed_search_results.csv` — Search execution log (counts, errors, PMIDs per query)\n",
    "- `Data/pubmed_excluded_abstracts.csv` — Abstracts for excluded-only papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fbad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q biopython pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87454cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\n",
      "NCBI API key configured: True\n",
      "Rate limit: 9 requests/sec\n",
      "Max PMIDs per query: 10,000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from Bio import Entrez\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# NCBI API — with API key allows 10 requests/sec\n",
    "Entrez.email = os.environ.get(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.environ.get(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "NCBI_RATE = 0.11 if Entrez.api_key else 0.34  # seconds between requests\n",
    "SEARCH_RETMAX = 100000  # max PMIDs to retrieve per query\n",
    "\n",
    "# Paths\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "\n",
    "# Input files\n",
    "STRATEGIES_CSV  = DATA_DIR / \"search_strategies.csv\"\n",
    "REFS_CSV        = DATA_DIR / \"categorized_references.csv\"\n",
    "ABSTRACTS_CSV   = DATA_DIR / \"referenced_paper_abstracts.csv\"\n",
    "DOI_PMID_CACHE  = DATA_DIR / \"doi_pmid_cache.csv\"\n",
    "\n",
    "# Output files\n",
    "SEARCH_RESULTS_CSV    = DATA_DIR / \"pubmed_search_results.csv\"\n",
    "SEARCH_PROGRESS_CSV   = DATA_DIR / \"pubmed_search_progress.csv\"\n",
    "EXCLUDED_ABSTRACTS_CSV = DATA_DIR / \"pubmed_excluded_abstracts.csv\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"NCBI API key configured: {bool(Entrez.api_key)}\")\n",
    "print(f\"Rate limit: {1/NCBI_RATE:.0f} requests/sec\")\n",
    "print(f\"Max PMIDs per query: {SEARCH_RETMAX:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a50a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total strategies: 16,929\n",
      "\n",
      "Translation notes:\n",
      "translation_notes\n",
      "narrative_only                                       8801\n",
      "success                                              1922\n",
      "adjacency_converted_to_AND                           1657\n",
      "filtered_not_a_query                                 1334\n",
      "mp_approximated; adjacency_converted_to_AND           651\n",
      "mp_approximated                                       561\n",
      "no_numbered_lines                                     548\n",
      "wildcard_approximated; adjacency_converted_to_AND     249\n",
      "adjacency_converted_to_AND; wildcard_approximated     213\n",
      "wildcard_approximated                                 187\n",
      "\n",
      "After Public Health filter: 65 strategies (61 reviews)\n",
      "Translated queries: 19\n",
      "Unique queries after dedup: 17\n",
      "  (covers 19 review-strategy pairs)\n",
      "\n",
      "Query length stats:\n",
      "count      17.000000\n",
      "mean      362.352941\n",
      "std       478.586061\n",
      "min        13.000000\n",
      "25%        47.000000\n",
      "50%       129.000000\n",
      "75%       639.000000\n",
      "max      1514.000000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Search Strategies (filtered to Public Health group)\n",
    "# =============================================================================\n",
    "\n",
    "strategies = pd.read_csv(STRATEGIES_CSV)\n",
    "print(f\"Total strategies: {len(strategies):,}\")\n",
    "print(f\"\\nTranslation notes:\")\n",
    "print(strategies['translation_notes'].value_counts().head(10).to_string())\n",
    "\n",
    "# ── Filter to Public Health reviews only ──────────────────────────────────────\n",
    "META_CSV = DATA_DIR / \"review_metadata.csv\"\n",
    "meta = pd.read_csv(META_CSV)\n",
    "\n",
    "# Version deduplication (keep latest version of each Cochrane review)\n",
    "if 'cd_number' not in meta.columns or 'version' not in meta.columns:\n",
    "    _vp = meta['doi'].str.extract(r'(CD\\d+)(?:\\.pub(\\d+))?', flags=re.I)\n",
    "    meta['cd_number'] = _vp[0].str.upper()\n",
    "    meta['version'] = _vp[1].fillna(1).astype(int)\n",
    "\n",
    "_has_cd = meta[meta['cd_number'].notna()]\n",
    "_latest_idx = _has_cd.groupby('cd_number')['version'].idxmax()\n",
    "_no_cd = meta[meta['cd_number'].isna()]\n",
    "meta = pd.concat([meta.loc[_latest_idx], _no_cd], ignore_index=True)\n",
    "\n",
    "ph_reviews = meta[meta['cochrane_group'] == 'Public Health']\n",
    "ph_dois = set(ph_reviews['doi'].dropna())\n",
    "strategies = strategies[strategies['doi'].isin(ph_dois)].copy()\n",
    "print(f\"\\nAfter Public Health filter: {len(strategies):,} strategies ({len(ph_dois):,} reviews)\")\n",
    "\n",
    "# Filter to successfully translated queries\n",
    "translated = strategies[strategies['pubmed_query'].notna()].copy()\n",
    "print(f\"Translated queries: {len(translated):,}\")\n",
    "\n",
    "# Deduplicate identical queries (different versions of the same review often share a search)\n",
    "translated['query_hash'] = translated['pubmed_query'].apply(hash)\n",
    "unique_queries = translated.drop_duplicates(subset='query_hash').copy()\n",
    "\n",
    "# Build mapping: query_hash → list of review DOIs\n",
    "query_to_reviews = translated.groupby('query_hash')['doi'].apply(list).to_dict()\n",
    "\n",
    "print(f\"Unique queries after dedup: {len(unique_queries):,}\")\n",
    "print(f\"  (covers {len(translated):,} review-strategy pairs)\")\n",
    "print(f\"\\nQuery length stats:\")\n",
    "print(unique_queries['pubmed_query'].str.len().describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eaeb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 1 — Direct extraction:      34,883 PMIDs\n",
      "Source 2 — Matched abstracts:       2,715 PMIDs\n",
      "Source 3 — DOI→PMID cache:        23,268 PMIDs\n",
      "\n",
      "==================================================\n",
      "Total known PMIDs (in Cochrane reviews): 56,898\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Build Known PMID Set (papers already in Cochrane reviews)\n",
    "# =============================================================================\n",
    "# Union of all PMIDs from: direct extraction (notebook 03), CrossRef matching\n",
    "# (notebook 04), and DOI→PMID cache (notebook 04).\n",
    "\n",
    "known_pmids = set()\n",
    "\n",
    "# Source 1: Direct PMIDs from reference extraction\n",
    "if REFS_CSV.exists():\n",
    "    refs = pd.read_csv(REFS_CSV, usecols=['pmid'], dtype=str)\n",
    "    direct = {p.strip() for p in refs['pmid'].dropna() if p.strip().isdigit()}\n",
    "    known_pmids |= direct\n",
    "    print(f\"Source 1 — Direct extraction:    {len(direct):>8,} PMIDs\")\n",
    "\n",
    "# Source 2: Matched PMIDs from abstract fetching\n",
    "if ABSTRACTS_CSV.exists():\n",
    "    abs_df = pd.read_csv(ABSTRACTS_CSV, usecols=['pmid'], dtype=str)\n",
    "    matched = {p.strip() for p in abs_df['pmid'].dropna() if p.strip().isdigit()}\n",
    "    known_pmids |= matched\n",
    "    print(f\"Source 2 — Matched abstracts:    {len(matched):>8,} PMIDs\")\n",
    "\n",
    "# Source 3: DOI→PMID cache\n",
    "if DOI_PMID_CACHE.exists():\n",
    "    cache = pd.read_csv(DOI_PMID_CACHE, dtype=str)\n",
    "    cached = {p.strip() for p in cache[cache['pmid'] != 'NO_PMID']['pmid'].dropna() if p.strip().isdigit()}\n",
    "    known_pmids |= cached\n",
    "    print(f\"Source 3 — DOI→PMID cache:      {len(cached):>8,} PMIDs\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Total known PMIDs (in Cochrane reviews): {len(known_pmids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf068f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined:\n",
      "  • execute_pubmed_search() — Run a PubMed query, get PMIDs\n",
      "  • fetch_abstracts_batch()  — Batch fetch abstracts by PMID\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PubMed Search & Abstract Fetch Functions\n",
    "# =============================================================================\n",
    "\n",
    "def execute_pubmed_search(query, retmax=10000, max_retries=3):\n",
    "    \"\"\"Execute a PubMed search and return PMIDs + total count.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: pmids (list[str]), count (int), error (str|None),\n",
    "              retmax_hit (bool — True if count > retmax, results truncated)\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=query,\n",
    "                retmax=retmax,\n",
    "                usehistory=\"y\",\n",
    "            )\n",
    "            results = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            count = int(results.get('Count', 0))\n",
    "            pmids = [str(p) for p in results.get('IdList', [])]\n",
    "\n",
    "            return {\n",
    "                'pmids': pmids,\n",
    "                'count': count,\n",
    "                'error': None,\n",
    "                'retmax_hit': count > retmax,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 1))\n",
    "            else:\n",
    "                return {'pmids': [], 'count': 0, 'error': str(e), 'retmax_hit': False}\n",
    "\n",
    "\n",
    "def extract_pubmed_record(record):\n",
    "    \"\"\"Extract key fields from a PubMed XML article record.\"\"\"\n",
    "    try:\n",
    "        cit = record['MedlineCitation']\n",
    "        art = cit['Article']\n",
    "        pmid = str(cit['PMID'])\n",
    "        title = str(art.get('ArticleTitle', ''))\n",
    "\n",
    "        # Abstract\n",
    "        abstract = ''\n",
    "        if 'Abstract' in art and 'AbstractText' in art['Abstract']:\n",
    "            parts = art['Abstract']['AbstractText']\n",
    "            abstract = ' '.join(str(p) for p in parts) if isinstance(parts, list) else str(parts)\n",
    "\n",
    "        # Year\n",
    "        year = ''\n",
    "        if 'Journal' in art and 'JournalIssue' in art['Journal']:\n",
    "            year = art['Journal']['JournalIssue'].get('PubDate', {}).get('Year', '')\n",
    "\n",
    "        # Authors\n",
    "        authors = []\n",
    "        if 'AuthorList' in art:\n",
    "            for auth in art['AuthorList']:\n",
    "                if 'LastName' in auth:\n",
    "                    name = auth['LastName']\n",
    "                    if auth.get('Initials'):\n",
    "                        name += ' ' + auth['Initials']\n",
    "                    authors.append(name)\n",
    "\n",
    "        # DOI\n",
    "        doi = ''\n",
    "        if 'ELocationID' in art:\n",
    "            for loc in art['ELocationID']:\n",
    "                if hasattr(loc, 'attributes') and loc.attributes.get('EIdType') == 'doi':\n",
    "                    doi = str(loc)\n",
    "                    break\n",
    "\n",
    "        return {'pmid': pmid, 'title': title, 'abstract': abstract,\n",
    "                'year': year, 'authors': '; '.join(authors), 'doi': doi}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_abstracts_batch(pmids, batch_size=200, max_retries=3):\n",
    "    \"\"\"Batch-fetch PubMed records. Returns {pmid: record_dict}.\"\"\"\n",
    "    results = {}\n",
    "    pmid_list = [str(p) for p in pmids if str(p).isdigit()]\n",
    "\n",
    "    batches = range(0, len(pmid_list), batch_size)\n",
    "    for i in tqdm(batches, desc=\"Fetching abstracts\"):\n",
    "        batch = pmid_list[i:i + batch_size]\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(NCBI_RATE * (attempt + 1))\n",
    "                handle = Entrez.efetch(\n",
    "                    db=\"pubmed\", id=\",\".join(batch),\n",
    "                    rettype=\"xml\", retmode=\"xml\"\n",
    "                )\n",
    "                records = Entrez.read(handle)\n",
    "                handle.close()\n",
    "\n",
    "                for article in records.get('PubmedArticle', []):\n",
    "                    data = extract_pubmed_record(article)\n",
    "                    if data:\n",
    "                        results[data['pmid']] = data\n",
    "                break  # success\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"  Batch {i//batch_size + 1} failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Functions defined:\")\n",
    "print(\"  • execute_pubmed_search() — Run a PubMed query, get PMIDs\")\n",
    "print(\"  • fetch_abstracts_batch()  — Batch fetch abstracts by PMID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44418324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING PUBMED SEARCHES\n",
      "============================================================\n",
      "Resuming from checkpoint: 17 queries already done\n",
      "Queries to execute: 17\n",
      "Estimated time: ~0 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05287ba4800945258c5482d95bca1c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PubMed Search:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Complete: 17 queries in 0.5 min\n",
      "  Total PMIDs retrieved: 151,695\n",
      "  Errors: 0\n",
      "  Queries exceeding retmax (10,000): 14\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Execute PubMed Searches (with checkpoint every 100 queries)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"EXECUTING PUBMED SEARCHES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "if SEARCH_PROGRESS_CSV.exists():\n",
    "    progress = pd.read_csv(SEARCH_PROGRESS_CSV)\n",
    "    done_hashes = set(progress['query_hash'])\n",
    "    print(f\"Resuming from checkpoint: {len(done_hashes):,} queries already done\")\n",
    "else:\n",
    "    progress = pd.DataFrame()\n",
    "    done_hashes = set()\n",
    "\n",
    "remaining = unique_queries[~unique_queries['query_hash'].isin(done_hashes)]\n",
    "print(f\"Queries to execute: {len(remaining):,}\")\n",
    "print(f\"Estimated time: ~{len(remaining) * NCBI_RATE / 60:.0f} min\")\n",
    "\n",
    "# --- Main loop ---\n",
    "batch_results = []\n",
    "total_pmids = 0\n",
    "errors = 0\n",
    "capped = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, (_, row) in enumerate(tqdm(remaining.iterrows(),\n",
    "                                     total=len(remaining), desc=\"PubMed Search\")):\n",
    "    query = row['pubmed_query']\n",
    "    review_dois = query_to_reviews.get(row['query_hash'], [row['doi']])\n",
    "\n",
    "    result = execute_pubmed_search(query, retmax=SEARCH_RETMAX)\n",
    "    time.sleep(NCBI_RATE)\n",
    "\n",
    "    batch_results.append({\n",
    "        'query_hash': row['query_hash'],\n",
    "        'review_doi': row['doi'],\n",
    "        'all_review_dois': '|'.join(review_dois),\n",
    "        'query_length': len(query),\n",
    "        'result_count': result['count'],\n",
    "        'pmids_retrieved': len(result['pmids']),\n",
    "        'retmax_hit': result['retmax_hit'],\n",
    "        'error': result['error'],\n",
    "        'pmids_json': json.dumps(result['pmids']),\n",
    "    })\n",
    "\n",
    "    total_pmids += len(result['pmids'])\n",
    "    if result['error']:\n",
    "        errors += 1\n",
    "    if result['retmax_hit']:\n",
    "        capped += 1\n",
    "\n",
    "    # Checkpoint every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        batch_df = pd.DataFrame(batch_results)\n",
    "        combined = pd.concat([progress, batch_df], ignore_index=True)\n",
    "        combined.to_csv(SEARCH_PROGRESS_CSV, index=False)\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (idx + 1) / elapsed * 60\n",
    "        print(f\"  [{idx+1:,}/{len(remaining):,}] \"\n",
    "              f\"{total_pmids:,} PMIDs | {errors} errors | \"\n",
    "              f\"{capped} capped | {rate:.0f} q/min\")\n",
    "\n",
    "# Final save\n",
    "if batch_results:\n",
    "    batch_df = pd.DataFrame(batch_results)\n",
    "    combined = pd.concat([progress, batch_df], ignore_index=True)\n",
    "    combined.to_csv(SEARCH_PROGRESS_CSV, index=False)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Complete: {len(remaining):,} queries in {elapsed/60:.1f} min\")\n",
    "print(f\"  Total PMIDs retrieved: {total_pmids:,}\")\n",
    "print(f\"  Errors: {errors:,}\")\n",
    "print(f\"  Queries exceeding retmax ({SEARCH_RETMAX:,}): {capped:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c3cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTIFYING EXCLUDED PAPERS\n",
      "============================================================\n",
      "Total queries executed: 34\n",
      "  With results: 34\n",
      "  Errors: 0\n",
      "  Hit retmax cap (10,000): 28\n",
      "\n",
      "After quality filter (non-empty, no errors):\n",
      "  Retained queries: 34\n",
      "  Of which hit retmax cap: 28\n",
      "  Dropped (empty/errored): 0\n",
      "\n",
      "Result count stats for retained queries:\n",
      "count    3.400000e+01\n",
      "mean     4.465462e+05\n",
      "std      1.052737e+06\n",
      "min      2.688000e+03\n",
      "25%      1.213100e+04\n",
      "50%      4.714600e+04\n",
      "75%      2.176930e+05\n",
      "max      4.332692e+06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528b02b39d324c5cbee4fee946d48688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting PMIDs:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique PMIDs from filtered searches: 147,667\n",
      "\n",
      "Overlap with Cochrane reviews:  386 PMIDs\n",
      "Excluded (search-only):         147,281 PMIDs\n",
      "Exclusion rate:                 99.7%\n",
      "\n",
      "Excluded (review, pmid) pairs:  161,273\n",
      "Unique reviews with excluded:   18\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Filter Queries & Identify Excluded PMIDs\n",
    "# =============================================================================\n",
    "\n",
    "print(\"IDENTIFYING EXCLUDED PAPERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load full search results\n",
    "results_df = pd.read_csv(SEARCH_PROGRESS_CSV)\n",
    "print(f\"Total queries executed: {len(results_df):,}\")\n",
    "print(f\"  With results: {(results_df['result_count'] > 0).sum():,}\")\n",
    "print(f\"  Errors: {results_df['error'].notna().sum():,}\")\n",
    "print(f\"  Hit retmax cap ({SEARCH_RETMAX:,}): {results_df['retmax_hit'].sum():,}\")\n",
    "\n",
    "# ---- QUALITY FILTER ----\n",
    "# Keep all queries that returned results (no errors).\n",
    "# Queries that hit the retmax cap ARE still useful — the 10K PMIDs we\n",
    "# retrieved are valid search results that the review chose not to include.\n",
    "# Discarding them would leave us with almost no excluded papers.\n",
    "\n",
    "successful = results_df[\n",
    "    (results_df['error'].isna()) &\n",
    "    (results_df['result_count'] > 0)\n",
    "].copy()\n",
    "\n",
    "n_capped = successful['retmax_hit'].sum()\n",
    "print(f\"\\nAfter quality filter (non-empty, no errors):\")\n",
    "print(f\"  Retained queries: {len(successful):,}\")\n",
    "print(f\"  Of which hit retmax cap: {n_capped:,}\")\n",
    "print(f\"  Dropped (empty/errored): {len(results_df) - len(successful):,}\")\n",
    "print(f\"\\nResult count stats for retained queries:\")\n",
    "print(successful['result_count'].describe().to_string())\n",
    "\n",
    "# Collect all unique PMIDs and track source reviews\n",
    "all_search_pmids = set()\n",
    "pmid_to_reviews = defaultdict(set)  # pmid → set of review DOIs\n",
    "\n",
    "for _, row in tqdm(successful.iterrows(), total=len(successful), desc=\"Collecting PMIDs\"):\n",
    "    if pd.isna(row['pmids_json']) or row['pmids_json'] == '[]':\n",
    "        continue\n",
    "    pmids = json.loads(row['pmids_json'])\n",
    "    review_dois = row['all_review_dois'].split('|')\n",
    "    for pmid in pmids:\n",
    "        all_search_pmids.add(pmid)\n",
    "        for rdoi in review_dois:\n",
    "            pmid_to_reviews[pmid].add(rdoi)\n",
    "\n",
    "print(f\"\\nTotal unique PMIDs from filtered searches: {len(all_search_pmids):,}\")\n",
    "\n",
    "# Partition into included (in Cochrane) and excluded (search-only)\n",
    "overlap_pmids  = all_search_pmids & known_pmids\n",
    "excluded_pmids = all_search_pmids - known_pmids\n",
    "\n",
    "print(f\"\\nOverlap with Cochrane reviews:  {len(overlap_pmids):,} PMIDs\")\n",
    "print(f\"Excluded (search-only):         {len(excluded_pmids):,} PMIDs\")\n",
    "print(f\"Exclusion rate:                 {len(excluded_pmids)/max(len(all_search_pmids),1)*100:.1f}%\")\n",
    "\n",
    "# Build excluded (review_doi, pmid) pairs for ground truth\n",
    "excluded_pairs = []\n",
    "for pmid in excluded_pmids:\n",
    "    for rdoi in pmid_to_reviews[pmid]:\n",
    "        excluded_pairs.append({'review_doi': rdoi, 'pmid': pmid})\n",
    "\n",
    "excluded_pairs_df = pd.DataFrame(excluded_pairs).drop_duplicates()\n",
    "print(f\"\\nExcluded (review, pmid) pairs:  {len(excluded_pairs_df):,}\")\n",
    "print(f\"Unique reviews with excluded:   {excluded_pairs_df['review_doi'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18540ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FETCHING ABSTRACTS FOR EXCLUDED PAPERS\n",
      "============================================================\n",
      "Fetching abstracts for all 147,281 excluded PMIDs\n",
      "Estimated: 737 batches, ~1 min\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d531598796247e9986f7d7d1de375c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching abstracts:   0%|          | 0/737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetched 146,951 records in 65.7 min\n",
      "  With abstracts:    137,466 (93.5%)\n",
      "  Without abstracts: 9,485\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Fetch Abstracts for Excluded Papers\n",
    "# =============================================================================\n",
    "\n",
    "print(\"FETCHING ABSTRACTS FOR EXCLUDED PAPERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pmids_to_fetch = list(excluded_pmids)\n",
    "print(f\"Fetching abstracts for all {len(pmids_to_fetch):,} excluded PMIDs\")\n",
    "\n",
    "est_batches = len(pmids_to_fetch) // 200 + 1\n",
    "est_min = est_batches * NCBI_RATE / 60\n",
    "print(f\"Estimated: {est_batches:,} batches, ~{max(est_min, 1):.0f} min\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "excluded_records = fetch_abstracts_batch(pmids_to_fetch, batch_size=200)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "with_abstract = sum(1 for r in excluded_records.values() if r.get('abstract'))\n",
    "print(f\"\\nFetched {len(excluded_records):,} records in {elapsed/60:.1f} min\")\n",
    "print(f\"  With abstracts:    {with_abstract:,} ({with_abstract/max(len(excluded_records),1)*100:.1f}%)\")\n",
    "print(f\"  Without abstracts: {len(excluded_records) - with_abstract:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c40723ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING EXCLUDED PAPERS DATASET\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f219c5f70a940aa9d5aa1c0b3062a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building output:   0%|          | 0/161273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total excluded pairs with records: 160,934\n",
      "With abstracts (>50 chars):        148,822\n",
      "Skipped (no PubMed record):        339\n",
      "\n",
      "Unique excluded PMIDs with abstract: 137,432\n",
      "Unique reviews represented:          18\n",
      "\n",
      "✓ Saved to pubmed_excluded_abstracts.csv\n",
      "✓ Search summary saved to pubmed_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Build & Save Final Output\n",
    "# =============================================================================\n",
    "\n",
    "print(\"BUILDING EXCLUDED PAPERS DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Join excluded pairs with fetched abstracts\n",
    "output_rows = []\n",
    "no_record = 0\n",
    "\n",
    "for _, pair in tqdm(excluded_pairs_df.iterrows(), total=len(excluded_pairs_df),\n",
    "                     desc=\"Building output\"):\n",
    "    pmid = pair['pmid']\n",
    "    rec = excluded_records.get(pmid)\n",
    "    if rec is None:\n",
    "        no_record += 1\n",
    "        continue\n",
    "\n",
    "    output_rows.append({\n",
    "        'review_doi': pair['review_doi'],\n",
    "        'pmid': pmid,\n",
    "        'title': rec.get('title', ''),\n",
    "        'abstract': rec.get('abstract', ''),\n",
    "        'authors': rec.get('authors', ''),\n",
    "        'year': rec.get('year', ''),\n",
    "        'doi': rec.get('doi', ''),\n",
    "    })\n",
    "\n",
    "excluded_df = pd.DataFrame(output_rows)\n",
    "\n",
    "# Filter to rows that actually have an abstract (required for LLM evaluation)\n",
    "excluded_with_abs = excluded_df[\n",
    "    excluded_df['abstract'].notna() &\n",
    "    (excluded_df['abstract'].str.len() > 50)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nTotal excluded pairs with records: {len(excluded_df):,}\")\n",
    "print(f\"With abstracts (>50 chars):        {len(excluded_with_abs):,}\")\n",
    "print(f\"Skipped (no PubMed record):        {no_record:,}\")\n",
    "print(f\"\\nUnique excluded PMIDs with abstract: {excluded_with_abs['pmid'].nunique():,}\")\n",
    "print(f\"Unique reviews represented:          {excluded_with_abs['review_doi'].nunique():,}\")\n",
    "\n",
    "# Save\n",
    "excluded_with_abs.to_csv(EXCLUDED_ABSTRACTS_CSV, index=False)\n",
    "print(f\"\\n✓ Saved to {EXCLUDED_ABSTRACTS_CSV.name}\")\n",
    "\n",
    "# Also save the search results summary (without PMIDs JSON for smaller file)\n",
    "search_summary = results_df.drop(columns=['pmids_json'], errors='ignore')\n",
    "search_summary.to_csv(SEARCH_RESULTS_CSV, index=False)\n",
    "print(f\"✓ Search summary saved to {SEARCH_RESULTS_CSV.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fc6d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Input:\n",
      "  Translated search queries:     17\n",
      "  Known PMIDs (Cochrane refs):   56,898\n",
      "\n",
      "PubMed Searches:\n",
      "  Queries executed:              34\n",
      "  Successful:                    34\n",
      "  Unique PMIDs found:            147,667\n",
      "\n",
      "Excluded Papers:\n",
      "  Excluded PMIDs (total):        147,281\n",
      "  With abstracts fetched:        137,432\n",
      "  (review, paper) pairs:         148,822\n",
      "\n",
      "Output Files:\n",
      "  pubmed_excluded_abstracts.csv\n",
      "  pubmed_search_results.csv\n",
      "\n",
      "✓ Ready for notebook 06 (build ground truth)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Summary Statistics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nInput:\")\n",
    "print(f\"  Translated search queries:     {len(unique_queries):,}\")\n",
    "print(f\"  Known PMIDs (Cochrane refs):   {len(known_pmids):,}\")\n",
    "\n",
    "print(f\"\\nPubMed Searches:\")\n",
    "print(f\"  Queries executed:              {len(results_df):,}\")\n",
    "print(f\"  Successful:                    {len(successful):,}\")\n",
    "print(f\"  Unique PMIDs found:            {len(all_search_pmids):,}\")\n",
    "\n",
    "print(f\"\\nExcluded Papers:\")\n",
    "print(f\"  Excluded PMIDs (total):        {len(excluded_pmids):,}\")\n",
    "print(f\"  With abstracts fetched:        {excluded_with_abs['pmid'].nunique():,}\")\n",
    "print(f\"  (review, paper) pairs:         {len(excluded_with_abs):,}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  {EXCLUDED_ABSTRACTS_CSV.name}\")\n",
    "print(f\"  {SEARCH_RESULTS_CSV.name}\")\n",
    "\n",
    "print(f\"\\n✓ Ready for notebook 06 (build ground truth)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
