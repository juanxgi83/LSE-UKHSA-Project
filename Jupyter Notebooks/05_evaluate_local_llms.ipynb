{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55fe0ca",
   "metadata": {},
   "source": [
    "# 05: Evaluate Local LLMs for Systematic Review Screening\n",
    "\n",
    "## Summary\n",
    "This notebook evaluates **local LLMs only** on the systematic review screening task. No proprietary data is sent to external APIs - all inference runs locally using Ollama.\n",
    "\n",
    "**Models Evaluated:**\n",
    "- Llama 3.2 (Meta)\n",
    "- Mistral (Mistral AI)\n",
    "- Other local models available via Ollama\n",
    "\n",
    "**Pipeline Position:** Sixth notebook - evaluates model performance on ground truth data.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads ground truth validation set\n",
    "2. Constructs prompts for screening decision\n",
    "3. Runs inference using local Ollama models\n",
    "4. Evaluates predictions against ground truth\n",
    "5. Computes metrics: accuracy, precision, recall, F1\n",
    "\n",
    "**Input:** `Data/ground_truth_validation_set.csv`\n",
    "\n",
    "**Output:** `Data/results/eval_*.csv`, model comparison metrics\n",
    "\n",
    "**Requirements:**\n",
    "- Ollama installed and running locally\n",
    "- Models pulled: `ollama pull llama3.2`, `ollama pull mistral`\n",
    "\n",
    "**IMPORTANT:** No external API calls are made. All processing is local to protect proprietary Cochrane content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc949b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for local LLM inference\n",
    "%pip install -q ollama pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and load ground truth data\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \"Data\").exists() else notebook_dir.parent\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GROUND_TRUTH_CSV = DATA_DIR / \"ground_truth_validation_set.csv\"\n",
    "\n",
    "ground_truth = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "print(f\"Loaded {len(ground_truth):,} examples\")\n",
    "print(ground_truth['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec17e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available Ollama models\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Available local models:\")\n",
    "    for model in models.get('models', []):\n",
    "        print(f\"  - {model['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d08ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates for screening task\n",
    "\n",
    "ZERO_SHOT_PROMPT = \"\"\"You are a systematic review screening assistant. Your task is to determine whether a study should be INCLUDED or EXCLUDED from a systematic review based on its abstract.\n",
    "\n",
    "Study Abstract:\n",
    "{abstract}\n",
    "\n",
    "Based on the abstract above, should this study be INCLUDED or EXCLUDED from a systematic review?\n",
    "Respond with only one word: INCLUDED or EXCLUDED\"\"\"\n",
    "\n",
    "COT_PROMPT = \"\"\"You are a systematic review screening assistant. Your task is to determine whether a study should be INCLUDED or EXCLUDED from a systematic review based on its abstract.\n",
    "\n",
    "Study Abstract:\n",
    "{abstract}\n",
    "\n",
    "Think step by step:\n",
    "1. What is the study design?\n",
    "2. What is the intervention or topic?\n",
    "3. What outcomes are measured?\n",
    "4. Is this likely to be relevant for inclusion?\n",
    "\n",
    "Based on your analysis, respond with your final decision: INCLUDED or EXCLUDED\"\"\"\n",
    "\n",
    "def create_prompt(abstract: str, use_cot: bool = False) -> str:\n",
    "    template = COT_PROMPT if use_cot else ZERO_SHOT_PROMPT\n",
    "    return template.format(abstract=abstract[:3000])  # Truncate long abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run local LLM inference\n",
    "import re\n",
    "\n",
    "def extract_decision(response: str) -> int:\n",
    "    \"\"\"Extract INCLUDED (1) or EXCLUDED (0) from LLM response.\"\"\"\n",
    "    response_lower = response.lower()\n",
    "    if 'included' in response_lower and 'excluded' not in response_lower:\n",
    "        return 1\n",
    "    elif 'excluded' in response_lower:\n",
    "        return 0\n",
    "    # Check for last occurrence if both present\n",
    "    included_pos = response_lower.rfind('included')\n",
    "    excluded_pos = response_lower.rfind('excluded')\n",
    "    if included_pos > excluded_pos:\n",
    "        return 1\n",
    "    elif excluded_pos > included_pos:\n",
    "        return 0\n",
    "    return -1  # Could not determine\n",
    "\n",
    "def run_evaluation(model_name: str, data: pd.DataFrame, use_cot: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Run evaluation on a dataset using a local Ollama model.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model_name}\"):\n",
    "        prompt = create_prompt(row['abstract'], use_cot=use_cot)\n",
    "        \n",
    "        try:\n",
    "            response = ollama.generate(model=model_name, prompt=prompt)\n",
    "            response_text = response.get('response', '')\n",
    "            prediction = extract_decision(response_text)\n",
    "        except Exception as e:\n",
    "            response_text = f\"ERROR: {e}\"\n",
    "            prediction = -1\n",
    "        \n",
    "        results.append({\n",
    "            'study_pmid': row['study_pmid'],\n",
    "            'label': row['label'],\n",
    "            'prediction': prediction,\n",
    "            'response': response_text[:500],  # Truncate for storage\n",
    "            'correct': prediction == row['label']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ea031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on local models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Models to evaluate (adjust based on what you have installed)\n",
    "MODELS = [\n",
    "    ('llama3.2', False),  # Zero-shot\n",
    "    ('llama3.2', True),   # Chain-of-thought\n",
    "    ('mistral', False),   # Zero-shot\n",
    "    ('mistral', True),    # Chain-of-thought\n",
    "]\n",
    "\n",
    "# Sample for faster testing (set to None for full evaluation)\n",
    "SAMPLE_SIZE = 100\n",
    "eval_data = ground_truth.sample(n=SAMPLE_SIZE, random_state=42) if SAMPLE_SIZE else ground_truth\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_name, use_cot in MODELS:\n",
    "    prompt_type = 'cot' if use_cot else 'zero_shot'\n",
    "    run_name = f\"{model_name}_{prompt_type}\"\n",
    "    \n",
    "    print(f\"\\nEvaluating: {run_name}\")\n",
    "    \n",
    "    try:\n",
    "        results = run_evaluation(model_name, eval_data, use_cot=use_cot)\n",
    "        all_results[run_name] = results\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = RESULTS_DIR / f\"eval_{run_name.replace('.', '_')}_{timestamp}.csv\"\n",
    "        results.to_csv(output_file, index=False)\n",
    "        print(f\"  Saved to {output_file.name}\")\n",
    "        \n",
    "        # Compute metrics\n",
    "        valid = results[results['prediction'] != -1]\n",
    "        if len(valid) > 0:\n",
    "            acc = accuracy_score(valid['label'], valid['prediction'])\n",
    "            prec = precision_score(valid['label'], valid['prediction'], zero_division=0)\n",
    "            rec = recall_score(valid['label'], valid['prediction'], zero_division=0)\n",
    "            f1 = f1_score(valid['label'], valid['prediction'], zero_division=0)\n",
    "            print(f\"  Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison summary\n",
    "comparison_rows = []\n",
    "\n",
    "for run_name, results in all_results.items():\n",
    "    valid = results[results['prediction'] != -1]\n",
    "    if len(valid) == 0:\n",
    "        continue\n",
    "    \n",
    "    comparison_rows.append({\n",
    "        'model': run_name,\n",
    "        'n_evaluated': len(valid),\n",
    "        'n_errors': len(results) - len(valid),\n",
    "        'accuracy': accuracy_score(valid['label'], valid['prediction']),\n",
    "        'precision': precision_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'recall': recall_score(valid['label'], valid['prediction'], zero_division=0),\n",
    "        'f1': f1_score(valid['label'], valid['prediction'], zero_division=0)\n",
    "    })\n",
    "\n",
    "comparison = pd.DataFrame(comparison_rows)\n",
    "comparison = comparison.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON (sorted by F1 score)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrices for best models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    fig, axes = plt.subplots(1, min(4, len(all_results)), figsize=(4*min(4, len(all_results)), 4))\n",
    "    if len(all_results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (run_name, results) in zip(axes, list(all_results.items())[:4]):\n",
    "        valid = results[results['prediction'] != -1]\n",
    "        if len(valid) > 0:\n",
    "            cm = confusion_matrix(valid['label'], valid['prediction'])\n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "            ax.set_title(run_name)\n",
    "            ax.set_ylabel('True')\n",
    "            ax.set_xlabel('Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Models evaluated: {len(all_results)}\")\n",
    "print(f\"Examples per model: {len(eval_data)}\")\n",
    "print(f\"Results saved to: {RESULTS_DIR}\")\n",
    "print(\"\\nNote: All inference was performed locally - no data sent to external APIs.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
