{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Screening Evaluation Pipeline\n",
    "\n",
    "**Summary:** This notebook evaluates how well various LLMs can screen paper abstracts for inclusion in systematic reviews. I test both open-source models (Llama 3.2 and Mistral via Ollama) and proprietary APIs (Gemini 3 Pro Preview, GPT-5.2 Thinking, Claude Opus 4.5) with two prompt strategies (zero-shot and chain-of-thought) on a ground-truth validation set of 1,000 labeled abstracts.\n",
    "\n",
    "---\n",
    "\n",
    "## Models Evaluated\n",
    "\n",
    "| Model | Type | Provider |\n",
    "|-------|------|----------|\n",
    "| Llama 3.2 3B | Open-source (via Ollama) | Meta |\n",
    "| Mistral 7B Instruct | Open-source (via Ollama) | Mistral AI |\n",
    "| Gemini 3 Pro Preview | Proprietary API | Google |\n",
    "| GPT-5.2 (Thinking/High-Reasoning) | Proprietary API | OpenAI |\n",
    "| Claude Opus 4.5 | Proprietary API | Anthropic |\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Ground Truth Dataset Construction\n",
    "The validation set consists of 1,000 paper-review pairs sampled from Cochrane systematic reviews:\n",
    "- **100 Cochrane reviews** were randomly selected from reviews that have clearly defined inclusion criteria and at least 5 included studies with available abstracts\n",
    "- For each review, I sample **5 \"included\" papers** (papers that were actually included in the review, serving as positive examples) and **5 \"excluded\" papers** (papers that were NOT cited by the review but share the same medical topic, serving as hard negative examples)\n",
    "- Final dataset: **500 included + 500 excluded = 1,000 labeled pairs**\n",
    "\n",
    "### Prompt Strategies\n",
    "\n",
    "**Zero-shot prompt:** A direct instruction asking the LLM to decide INCLUDE or EXCLUDE based on whether the paper is relevant to the review topic.\n",
    "\n",
    "**Chain-of-thought (CoT) prompt:** The LLM is asked to reason step-by-step before making a decision.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Load ground-truth validation set (500 included, 500 excluded papers)\n",
    "2. Define prompt templates (zero-shot and chain-of-thought)\n",
    "3. Set up API clients (Ollama for local models, API keys for proprietary)\n",
    "4. Run evaluation on all models\n",
    "5. Compute metrics (accuracy, precision, recall, F1, Cohen's kappa)\n",
    "6. Compare results and analyze errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\.env\n",
      "Data directory: C:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\n",
      "Results directory: C:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\n",
      "\n",
      "API Keys loaded:\n",
      "  Gemini:    ‚úÖ Set\n",
      "  OpenAI:    ‚úÖ Set\n",
      "  Anthropic: ‚úÖ Set\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: IMPORTS AND SETUP\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "\n",
    "# Load environment variables from .env file (force override)\n",
    "ENV_FILE = Path(r\"c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\.env\")\n",
    "load_dotenv(ENV_FILE, override=True)\n",
    "print(f\"Loading .env from: {ENV_FILE}\")\n",
    "\n",
    "DATA_DIR = Path(\"../Data\")\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# API Configuration\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")  # Note: underscore in env var name\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"Results directory: {RESULTS_DIR.resolve()}\")\n",
    "print(f\"\\nAPI Keys loaded:\")\n",
    "print(f\"  Gemini:    {'‚úÖ Set' if GEMINI_API_KEY else '‚ùå Missing'}\")\n",
    "print(f\"  OpenAI:    {'‚úÖ Set' if OPENAI_API_KEY else '‚ùå Missing'}\")\n",
    "print(f\"  Anthropic: {'‚úÖ Set' if ANTHROPIC_API_KEY else '‚ùå Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1,000 records\n",
      "Label distribution: {1: 500, 0: 500}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD VALIDATION DATA\n",
    "# ============================================================\n",
    "df = pd.read_csv(DATA_DIR / \"ground_truth_validation_set.csv\")\n",
    "print(f\"‚úÖ Loaded {len(df):,} records\")\n",
    "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt templates defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: PROMPT TEMPLATES\n",
    "# ============================================================\n",
    "\n",
    "ZERO_SHOT_TEMPLATE = \"\"\"You are a systematic review screener. Based on the abstract below, decide if this paper should be INCLUDED or EXCLUDED from a systematic review about:\n",
    "\"{review_title}\"\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Answer with exactly one word: INCLUDE or EXCLUDE\"\"\"\n",
    "\n",
    "COT_TEMPLATE = \"\"\"You are a systematic review screener. Your task is to decide if a paper should be included in a systematic review.\n",
    "\n",
    "Review topic: \"{review_title}\"\n",
    "\n",
    "Abstract to screen:\n",
    "{abstract}\n",
    "\n",
    "Think through this step by step:\n",
    "1. What is the main topic of this paper?\n",
    "2. Does it relate to the systematic review topic?\n",
    "3. Does it appear to provide relevant evidence?\n",
    "\n",
    "After your reasoning, give your final answer on a new line as exactly: DECISION: INCLUDE or DECISION: EXCLUDE\"\"\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"zero_shot\": ZERO_SHOT_TEMPLATE,\n",
    "    \"cot\": COT_TEMPLATE\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Prompt templates defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: OLLAMA FUNCTIONS (for local models)\n",
    "# ============================================================\n",
    "\n",
    "def call_ollama(model: str, prompt: str, timeout: int = 120) -> str:\n",
    "    \"\"\"Send a prompt to Ollama and return the response text.\"\"\"\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
    "            timeout=timeout\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        return resp.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "def parse_decision(response: str, prompt_type: str) -> str:\n",
    "    \"\"\"Extract INCLUDE/EXCLUDE from LLM response.\"\"\"\n",
    "    text = response.upper()\n",
    "    if prompt_type == \"cot\":\n",
    "        if \"DECISION: INCLUDE\" in text or \"DECISION:INCLUDE\" in text:\n",
    "            return \"include\"\n",
    "        elif \"DECISION: EXCLUDE\" in text or \"DECISION:EXCLUDE\" in text:\n",
    "            return \"exclude\"\n",
    "    if \"INCLUDE\" in text and \"EXCLUDE\" not in text:\n",
    "        return \"include\"\n",
    "    elif \"EXCLUDE\" in text and \"INCLUDE\" not in text:\n",
    "        return \"exclude\"\n",
    "    elif text.strip().startswith(\"INCLUDE\"):\n",
    "        return \"include\"\n",
    "    elif text.strip().startswith(\"EXCLUDE\"):\n",
    "        return \"exclude\"\n",
    "    return \"unclear\"\n",
    "\n",
    "print(\"‚úÖ Ollama functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini 2.0 Flash initialized\n",
      "‚úÖ OpenAI GPT-4o initialized\n",
      "‚úÖ Claude 3 Haiku initialized\n",
      "\n",
      "‚úÖ All API clients ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: PROPRIETARY API CLIENTS\n",
    "# ============================================================\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "# --- GEMINI 2.0 Flash ---\n",
    "gemini_model = None\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    gemini_model = genai.GenerativeModel('gemini-2.0-flash')  # Available model\n",
    "    print(\"‚úÖ Gemini 2.0 Flash initialized\")\n",
    "\n",
    "def generate_gemini(prompt: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate response using Gemini 2.0 Flash.\"\"\"\n",
    "    if not gemini_model:\n",
    "        return \"ERROR: Gemini API key not set\"\n",
    "    try:\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                max_output_tokens=max_tokens,\n",
    "                temperature=0.1\n",
    "            )\n",
    "        )\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "# --- GPT-4o ---\n",
    "openai_client = None\n",
    "if OPENAI_API_KEY:\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY, timeout=120.0)\n",
    "    print(\"‚úÖ OpenAI GPT-4o initialized\")\n",
    "\n",
    "def generate_gpt(prompt: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate response using GPT-4o.\"\"\"\n",
    "    if not openai_client:\n",
    "        return \"ERROR: OpenAI API key not set\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "# --- Claude 3 Haiku ---\n",
    "anthropic_client = None\n",
    "if ANTHROPIC_API_KEY:\n",
    "    anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY, timeout=120.0)\n",
    "    print(\"‚úÖ Claude 3 Haiku initialized\")\n",
    "\n",
    "def generate_claude(prompt: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate response using Claude 3 Haiku.\"\"\"\n",
    "    if not anthropic_client:\n",
    "        return \"ERROR: Anthropic API key not set\"\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",  # Most basic/available model\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "print(\"\\n‚úÖ All API clients ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking Ollama models...\n",
      "   Available: ['mistral:latest', 'llama3.2:latest']\n",
      "\n",
      "üîç Checking API models...\n",
      "   Available: ['gemini-2.0-flash', 'gpt-4o', 'claude-3-haiku-20240307']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: CHECK AVAILABLE MODELS\n",
    "# ============================================================\n",
    "\n",
    "# Check Ollama models\n",
    "print(\"üîç Checking Ollama models...\")\n",
    "try:\n",
    "    resp = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    ollama_models = [m[\"name\"] for m in resp.json().get(\"models\", [])]\n",
    "    print(f\"   Available: {ollama_models}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not connect to Ollama: {e}\")\n",
    "    print(\"   Make sure Ollama is running (ollama serve)\")\n",
    "    ollama_models = []\n",
    "\n",
    "# Check API models\n",
    "print(\"\\nüîç Checking API models...\")\n",
    "api_models = []\n",
    "if GEMINI_API_KEY:\n",
    "    api_models.append(\"gemini-2.0-flash\")\n",
    "if OPENAI_API_KEY:\n",
    "    api_models.append(\"gpt-4o\")\n",
    "if ANTHROPIC_API_KEY:\n",
    "    api_models.append(\"claude-3-haiku-20240307\")\n",
    "print(f\"   Available: {api_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: EVALUATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def run_evaluation_ollama(df: pd.DataFrame, model: str, prompt_type: str, \n",
    "                          template: str, limit: int = None):\n",
    "    \"\"\"Run evaluation using Ollama (local models).\"\"\"\n",
    "    data = df.head(limit) if limit else df\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model}/{prompt_type}\"):\n",
    "        prompt = template.format(\n",
    "            review_title=row[\"review_title\"],\n",
    "            abstract=str(row[\"paper_abstract\"])[:3000]\n",
    "        )\n",
    "        response = call_ollama(model, prompt)\n",
    "        prediction = parse_decision(response, prompt_type)\n",
    "        \n",
    "        results.append({\n",
    "            \"paper_pmid\": row[\"paper_pmid\"],\n",
    "            \"true_label\": row[\"label\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"raw_response\": response[:500] if response else \"\",\n",
    "            \"model\": model,\n",
    "            \"prompt_type\": prompt_type\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def run_evaluation_api(df: pd.DataFrame, model_name: str, generate_fn, \n",
    "                       prompt_type: str, template: str, limit: int = None,\n",
    "                       delay: float = 0.5):\n",
    "    \"\"\"Run evaluation using API models (Gemini, GPT, Claude).\"\"\"\n",
    "    data = df.head(limit) if limit else df\n",
    "    results = []\n",
    "    max_tokens = 300 if prompt_type == \"cot\" else 50\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model_name}/{prompt_type}\"):\n",
    "        prompt = template.format(\n",
    "            review_title=row[\"review_title\"],\n",
    "            abstract=str(row[\"paper_abstract\"])[:3000]\n",
    "        )\n",
    "        \n",
    "        response = generate_fn(prompt, max_tokens)\n",
    "        prediction = parse_decision(response, prompt_type)\n",
    "        \n",
    "        results.append({\n",
    "            \"paper_pmid\": row[\"paper_pmid\"],\n",
    "            \"true_label\": row[\"label\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"raw_response\": response[:500] if response else \"\",\n",
    "            \"model\": model_name,\n",
    "            \"prompt_type\": prompt_type\n",
    "        })\n",
    "        \n",
    "        if delay > 0:\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"‚úÖ Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: METRICS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def compute_metrics(results_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
    "    valid = results_df[results_df[\"prediction\"].isin([\"include\", \"exclude\"])].copy()\n",
    "    y_true = valid[\"true_label\"].astype(int)\n",
    "    y_pred = (valid[\"prediction\"] == \"include\").astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"n_valid\": len(valid),\n",
    "        \"n_unclear\": len(results_df) - len(valid),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics: dict, model: str, prompt_type: str):\n",
    "    \"\"\"Display metrics in a readable format.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model} | Prompt: {prompt_type}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.1%}\")\n",
    "    print(f\"Precision: {metrics['precision']:.1%}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.1%}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.3f}\")\n",
    "    print(f\"Kappa:     {metrics['kappa']:.3f}\")\n",
    "    print(f\"Valid:     {metrics['n_valid']} | Unclear: {metrics['n_unclear']}\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"          Pred Excl  Pred Incl\")\n",
    "    print(f\"True Excl    {cm[0][0]:4d}       {cm[0][1]:4d}\")\n",
    "    print(f\"True Incl    {cm[1][0]:4d}       {cm[1][1]:4d}\")\n",
    "\n",
    "print(\"‚úÖ Metrics functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Quick test with 10 samples each...\n",
      "\n",
      "Testing Llama 3.2 (Ollama)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/zero_shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:27<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 90.0%, F1: 0.889\n",
      "Testing Gemini 2.0 Flash...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash/zero_shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 90.0%, F1: 0.889\n",
      "Testing GPT-4o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o/zero_shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:16<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 70.0%, F1: 0.571\n",
      "Testing Claude 3 Haiku...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-haiku-20240307/zero_shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:16<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 90.0%, F1: 0.889\n",
      "\n",
      "‚úÖ Quick tests complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: QUICK TEST (10 samples each)\n",
    "# ============================================================\n",
    "\n",
    "print(\"üß™ Quick test with 10 samples each...\\n\")\n",
    "\n",
    "# Test Ollama (if available)\n",
    "if \"llama3.2\" in str(ollama_models):\n",
    "    print(\"Testing Llama 3.2 (Ollama)...\")\n",
    "    test_ollama = run_evaluation_ollama(df, \"llama3.2\", \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10)\n",
    "    metrics = compute_metrics(test_ollama)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Llama 3.2 not available in Ollama\")\n",
    "\n",
    "# Test Gemini 2.0 Flash\n",
    "if GEMINI_API_KEY:\n",
    "    print(\"Testing Gemini 2.0 Flash...\")\n",
    "    test_gemini = run_evaluation_api(df, \"gemini-2.0-flash\", generate_gemini, \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10, delay=1.0)\n",
    "    metrics = compute_metrics(test_gemini)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "\n",
    "# Test GPT-4o\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"Testing GPT-4o...\")\n",
    "    test_gpt = run_evaluation_api(df, \"gpt-4o\", generate_gpt, \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10, delay=1.0)\n",
    "    metrics = compute_metrics(test_gpt)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "\n",
    "# Test Claude 3 Haiku\n",
    "if ANTHROPIC_API_KEY:\n",
    "    print(\"Testing Claude 3 Haiku...\")\n",
    "    test_claude = run_evaluation_api(df, \"claude-3-haiku-20240307\", generate_claude, \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10, delay=1.0)\n",
    "    metrics = compute_metrics(test_claude)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Quick tests complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "üöÄ Running: llama3.2 with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/zero_shot:   1%|          | 10/1000 [00:29<49:22,  2.99s/it]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11: FULL EVALUATION - ALL MODELS\n",
    "# ============================================================\n",
    "# This evaluates all available models on all 1,000 samples.\n",
    "# Estimated time: \n",
    "#   - Ollama models: ~1-2 hours each\n",
    "#   - API models: ~30-60 min each (with rate limiting)\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# --- LOCAL MODELS (Ollama) ---\n",
    "OLLAMA_MODELS = [\"llama3.2\", \"mistral\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    if model not in str(ollama_models):\n",
    "        print(f\"‚è≠Ô∏è Skipping {model} (not in Ollama)\")\n",
    "        continue\n",
    "        \n",
    "    for prompt_type, template in PROMPTS.items():\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"üöÄ Running: {model} with {prompt_type} prompt\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        results = run_evaluation_ollama(df, model, prompt_type, template)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out_file = RESULTS_DIR / f\"eval_{model}_{prompt_type}_{timestamp}.csv\"\n",
    "        results.to_csv(out_file, index=False)\n",
    "        print(f\"üíæ Saved: {out_file}\")\n",
    "        \n",
    "        metrics = compute_metrics(results)\n",
    "        print_metrics(metrics, model, prompt_type)\n",
    "        \n",
    "        all_metrics.append({\n",
    "            \"model\": model,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            **{k: v for k, v in metrics.items() if k != \"confusion_matrix\"}\n",
    "        })\n",
    "\n",
    "# --- API MODELS ---\n",
    "API_MODELS = {\n",
    "    \"gemini-2.0-flash\": {\"fn\": generate_gemini, \"delay\": 0.5, \"enabled\": bool(GEMINI_API_KEY)},\n",
    "    \"gpt-4o\": {\"fn\": generate_gpt, \"delay\": 0.5, \"enabled\": bool(OPENAI_API_KEY)},\n",
    "    \"claude-3-haiku-20240307\": {\"fn\": generate_claude, \"delay\": 0.5, \"enabled\": bool(ANTHROPIC_API_KEY)},\n",
    "}\n",
    "\n",
    "for model_name, config in API_MODELS.items():\n",
    "    if not config[\"enabled\"]:\n",
    "        print(f\"\\n‚è≠Ô∏è Skipping {model_name} (no API key)\")\n",
    "        continue\n",
    "    \n",
    "    for prompt_type, template in PROMPTS.items():\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"üöÄ Running: {model_name} with {prompt_type} prompt\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        results = run_evaluation_api(\n",
    "            df, model_name, config[\"fn\"], \n",
    "            prompt_type, template, \n",
    "            delay=config[\"delay\"]\n",
    "        )\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        safe_name = model_name.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "        out_file = RESULTS_DIR / f\"eval_{safe_name}_{prompt_type}_{timestamp}.csv\"\n",
    "        results.to_csv(out_file, index=False)\n",
    "        print(f\"üíæ Saved: {out_file}\")\n",
    "        \n",
    "        metrics = compute_metrics(results)\n",
    "        print_metrics(metrics, model_name, prompt_type)\n",
    "        \n",
    "        all_metrics.append({\n",
    "            \"model\": model_name,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            **{k: v for k, v in metrics.items() if k != \"confusion_matrix\"}\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ FULL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison (sorted by F1 score):\n",
      "   model prompt_type  accuracy  precision   recall       f1    kappa  n_valid  n_unclear\n",
      " mistral         cot  0.836181   0.835671 0.837349 0.836510 0.672361      995          5\n",
      " mistral   zero_shot  0.845000   0.907801 0.768000 0.832069 0.690000     1000          0\n",
      "llama3.2   zero_shot  0.809000   0.764103 0.894000 0.823963 0.618000     1000          0\n",
      "llama3.2         cot  0.739487   0.900709 0.529167 0.666667 0.475573      975         25\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 12: MODEL COMPARISON TABLE\n",
    "# ============================================================\n",
    "\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "comparison_df = comparison_df.sort_values(\"f1\", ascending=False)\n",
    "comparison_df.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìä Model Comparison (sorted by F1 score):\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nüíæ Saved to:\", RESULTS_DIR / \"model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analysis function defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13: ERROR ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "def analyze_errors(results_file: str, n_samples: int = 5):\n",
    "    \"\"\"Show examples of false positives and false negatives.\"\"\"\n",
    "    df_results = pd.read_csv(results_file)\n",
    "    df_gt = pd.read_csv(DATA_DIR / \"ground_truth_validation_set.csv\")\n",
    "    merged = df_results.merge(df_gt[[\"paper_pmid\", \"paper_abstract\", \"review_title\"]], on=\"paper_pmid\")\n",
    "    \n",
    "    fp = merged[(merged[\"true_label\"] == 0) & (merged[\"prediction\"] == \"include\")]\n",
    "    fn = merged[(merged[\"true_label\"] == 1) & (merged[\"prediction\"] == \"exclude\")]\n",
    "    \n",
    "    print(f\"\\n‚ùå False Positives ({len(fp)} total) - wrongly included:\")\n",
    "    for _, row in fp.head(n_samples).iterrows():\n",
    "        print(f\"  ‚Ä¢ {row['paper_abstract'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n‚ùå False Negatives ({len(fn)} total) - wrongly excluded:\")\n",
    "    for _, row in fn.head(n_samples).iterrows():\n",
    "        print(f\"  ‚Ä¢ {row['paper_abstract'][:100]}...\")\n",
    "\n",
    "print(\"‚úÖ Error analysis function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# CELL 14: ANALYZE BEST MODEL ERRORS\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Find the best performing model based on F1 score\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m comparison_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      7\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m comparison_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìà Best model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14: ANALYZE BEST MODEL ERRORS\n",
    "# ============================================================\n",
    "\n",
    "# Find the best performing model based on F1 score\n",
    "if not comparison_df.empty:\n",
    "    best_model = comparison_df.iloc[0]\n",
    "    print(f\"üìà Best model: {best_model['model']} ({best_model['prompt_type']})\")\n",
    "    print(f\"   F1: {best_model['f1']:.3f}, Accuracy: {best_model['accuracy']:.1%}\")\n",
    "    \n",
    "    # Find the corresponding results file\n",
    "    safe_name = best_model['model'].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "    pattern = f\"eval_{safe_name}_{best_model['prompt_type']}_*.csv\"\n",
    "    result_files = list(RESULTS_DIR.glob(pattern))\n",
    "    \n",
    "    if result_files:\n",
    "        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"\\nüîç Analyzing errors from: {latest_file.name}\")\n",
    "        analyze_errors(latest_file)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No results file found matching: {pattern}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation results available. Run the full evaluation first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
