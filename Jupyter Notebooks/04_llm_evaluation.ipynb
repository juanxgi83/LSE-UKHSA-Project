{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7f37a9",
   "metadata": {},
   "source": [
    "# LLM Screening Evaluation Pipeline\n",
    "\n",
    "**Objective:** Evaluate open-source LLMs on the task of screening paper abstracts for inclusion in systematic reviews.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load ground truth validation set\n",
    "2. Design screening prompts\n",
    "3. Run LLM inference\n",
    "4. Parse model outputs\n",
    "5. Calculate metrics (Precision, Recall, F1, Cohen's Kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298776f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No additional packages needed - uses requests (built-in) for Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d07f4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\n",
      "Results directory: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Literal\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path.cwd().parent / \"Data\" if not (Path.cwd() / \"Data\").exists() else Path.cwd() / \"Data\"\n",
    "VALIDATION_CSV = DATA_DIR / \"ground_truth_validation_set.csv\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20d49f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,000 validation records\n",
      "Label distribution: {1: 500, 0: 500}\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_pmid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_objectives",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_criteria",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "paper_pmid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paper_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "paper_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e87f94fe-4398-46e2-b29a-c90186813624",
       "rows": [
        [
         "0",
         "21678351",
         "Evaluation of follow-up strategies for patients with epithelial ovarian cancer following completion of primary treatment.",
         "BACKGROUND: Ovarian cancer is the sixth most common cancer and seventh cause of cancer death in women worldwide. Traditionally, many patients who have been treated for cancer undergo long-term follow up in secondary care. Recently however it has been suggested that the use of routine review may not be effective in improving survival, quality of life (QoL), and relieving anxiety. In addition, it may not be cost effective. OBJECTIVES: To compare the potential benefits of different strategies of follow up in women with epithelial ovarian cancer following completion of primary treatment. SEARCH STRATEGY: We searched the Cochrane Gynaecological Cancer Group Trials Register, Cochrane Central Register of Controlled Trials (CENTRAL) (The Cochrane Library, 2010, Issue 4), MEDLINE and EMBASE (to November 2010). We also searched CINAHL, PsycLIT, registers of clinical trials, abstracts of scientific meetings, reference lists of review articles, and contacted experts in the field.",
         "SELECTION CRITERIA: All relevant randomised controlled trials (RCTs) that evaluated follow-up strategies for patients with epithelial ovarian cancer following completion of primary treatment.",
         "11737464",
         "A critical evaluation of current protocols for the follow-up of women treated for gynecological malignancies: a pilot study.",
         "This retrospective review was undertaken to determine the efficacy of routine follow-up in the detection and management of recurrent cancer. The case notes of all women attending a regional cancer center who were diagnosed with cancer in 1997 were reviewed. Of 81 new cancers followed up for a median of 42 months (range 36-48), 14 have recurred after curative treatment and there were six cases of persistent disease. The median number of clinic visits per patient was 3.5 (range 1-16). Eight recurrences (57.1%) were diagnosed at scheduled outpatient appointments, three (2 l.4%) presented to the general practitioner (GP), and three were seen as emergencies in hospital. Seventeen patients with persistent/recurrent disease have died and three are alive with disease. The median time from initial presentation to disease recurrence was 12 months (range 5-25) and the median time from recurrence to death was 5 months (range 1-20). The longest interval between onset of symptoms and diagnosis of recurrence (4 months) occurred in those presenting at scheduled outpatient clinics. This study demonstrates that the current follow-up protocol is associated with delays in diagnosing recurrence, because symptomatic patients postpone seeking help until their scheduled visit. We have therefore commenced a prospective study evaluating other models of follow-up.",
         "1"
        ],
        [
         "1",
         "21678351",
         "Evaluation of follow-up strategies for patients with epithelial ovarian cancer following completion of primary treatment.",
         "BACKGROUND: Ovarian cancer is the sixth most common cancer and seventh cause of cancer death in women worldwide. Traditionally, many patients who have been treated for cancer undergo long-term follow up in secondary care. Recently however it has been suggested that the use of routine review may not be effective in improving survival, quality of life (QoL), and relieving anxiety. In addition, it may not be cost effective. OBJECTIVES: To compare the potential benefits of different strategies of follow up in women with epithelial ovarian cancer following completion of primary treatment. SEARCH STRATEGY: We searched the Cochrane Gynaecological Cancer Group Trials Register, Cochrane Central Register of Controlled Trials (CENTRAL) (The Cochrane Library, 2010, Issue 4), MEDLINE and EMBASE (to November 2010). We also searched CINAHL, PsycLIT, registers of clinical trials, abstracts of scientific meetings, reference lists of review articles, and contacted experts in the field.",
         "SELECTION CRITERIA: All relevant randomised controlled trials (RCTs) that evaluated follow-up strategies for patients with epithelial ovarian cancer following completion of primary treatment.",
         "2564410",
         "[Clinical usefulness of serum sialyl Le(x)-i measurement in patients with ovarian cancer].",
         "Sialyl Le(X)-i (Sialyl SSEA-1, SLX) is one of the type 2 chain carbohydrate antigens, which is defined by a monoclonal antibody FH-6. The clinical usefulness of the measurement of serum Sialyl LeX-i levels in the follow-up study of outpatient with ovarian cancer was examined for the early detection of recurrence as the serodiagnostic test. Elevated serum Sialy Le(X)-i levels (more than 38 unit/ml) were observed before treatment in 7 of 12 patients with a good prognosis (group A) and in 15 of 26 patients with a poor prognosis (group B). Six (85.7%) in 7 patients with elevated serum Sialyl Le(X)-i levels in group A decreased to the normal range after treatment, whereas 3 (20.0%) in 15 patients with positive serum Sialyl Le(X)-i levels in group B decreased below 38 unit/ml a after treatment. In 9 (34.6%) in group B, elevated serum Sialyl Le(X)-i levels were observed with an average 3.1 weeks before clinical evidence of recurrence. With tumor progression, serum Sialyl Le(X)-i levels also rose in 25 (96.2%) in group B. In 4 (15.4%) in group B, serum Sialyl Le(X)-i levels were a useful tumor marker compared with others including CA 125, TPA, and CEA. Consequently, the measurement of serum Sialyl Le(X)-i levels may be useful to monitor the condition of the disease, presume progression, and detect recurrence early in outpatients with ovarian cancer.",
         "1"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_pmid</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_objectives</th>\n",
       "      <th>review_criteria</th>\n",
       "      <th>paper_pmid</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21678351</td>\n",
       "      <td>Evaluation of follow-up strategies for patient...</td>\n",
       "      <td>BACKGROUND: Ovarian cancer is the sixth most c...</td>\n",
       "      <td>SELECTION CRITERIA: All relevant randomised co...</td>\n",
       "      <td>11737464</td>\n",
       "      <td>A critical evaluation of current protocols for...</td>\n",
       "      <td>This retrospective review was undertaken to de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21678351</td>\n",
       "      <td>Evaluation of follow-up strategies for patient...</td>\n",
       "      <td>BACKGROUND: Ovarian cancer is the sixth most c...</td>\n",
       "      <td>SELECTION CRITERIA: All relevant randomised co...</td>\n",
       "      <td>2564410</td>\n",
       "      <td>[Clinical usefulness of serum sialyl Le(x)-i m...</td>\n",
       "      <td>Sialyl Le(X)-i (Sialyl SSEA-1, SLX) is one of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_pmid                                       review_title  \\\n",
       "0     21678351  Evaluation of follow-up strategies for patient...   \n",
       "1     21678351  Evaluation of follow-up strategies for patient...   \n",
       "\n",
       "                                   review_objectives  \\\n",
       "0  BACKGROUND: Ovarian cancer is the sixth most c...   \n",
       "1  BACKGROUND: Ovarian cancer is the sixth most c...   \n",
       "\n",
       "                                     review_criteria  paper_pmid  \\\n",
       "0  SELECTION CRITERIA: All relevant randomised co...    11737464   \n",
       "1  SELECTION CRITERIA: All relevant randomised co...     2564410   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  A critical evaluation of current protocols for...   \n",
       "1  [Clinical usefulness of serum sialyl Le(x)-i m...   \n",
       "\n",
       "                                      paper_abstract  label  \n",
       "0  This retrospective review was undertaken to de...      1  \n",
       "1  Sialyl Le(X)-i (Sialyl SSEA-1, SLX) is one of ...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load validation set\n",
    "val_df = pd.read_csv(VALIDATION_CSV)\n",
    "print(f\"Loaded {len(val_df):,} validation records\")\n",
    "print(f\"Label distribution: {val_df['label'].value_counts().to_dict()}\")\n",
    "val_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f522d3",
   "metadata": {},
   "source": [
    "## Prompt Design\n",
    "\n",
    "We'll test multiple prompt strategies:\n",
    "1. **Zero-shot**: Direct instruction with criteria\n",
    "2. **Few-shot**: Include examples\n",
    "3. **Chain-of-thought**: Ask for reasoning before decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98bbbe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample zero-shot prompt:\n",
      "## Systematic Review Selection Criteria:\n",
      "SELECTION CRITERIA: All relevant randomised controlled trials (RCTs) that evaluated follow-up strategies for patients with epithelial ovarian cancer following completion of primary treatment.\n",
      "\n",
      "## Paper to Screen:\n",
      "Title: A critical evaluation of current protocols for the follow-up of women treated for gynecological malignancies: a pilot study.\n",
      "\n",
      "Abstract: This retrospective review was undertaken to determine the efficacy of routine follow-up in the detection and management of recurrent cancer. The case notes of all women attending a regional cancer center who were diagnosed with cancer in 1997 were reviewed. Of 81 new cancers followed up for a median of 42 months (range 36-48), 14 have recurred after curative treatment and there were six cases of persistent disease. The median number of clinic visits per patient was 3.5 (range 1-16). Eight recurr...\n",
      "\n",
      "## Decision:\n",
      "Based on the selection criteria above, should this paper be INCLUDED or EXCLUDED from\n"
     ]
    }
   ],
   "source": [
    "# Prompt templates\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert systematic review screener. Your task is to determine whether a research paper should be INCLUDED or EXCLUDED from a systematic review based on the review's selection criteria.\n",
    "\n",
    "Respond with ONLY one word: INCLUDE or EXCLUDE.\"\"\"\n",
    "\n",
    "\n",
    "def create_zero_shot_prompt(criteria: str, paper_title: str, paper_abstract: str) -> str:\n",
    "    \"\"\"Create a zero-shot screening prompt.\"\"\"\n",
    "    return f\"\"\"## Systematic Review Selection Criteria:\n",
    "{criteria}\n",
    "\n",
    "## Paper to Screen:\n",
    "Title: {paper_title}\n",
    "\n",
    "Abstract: {paper_abstract}\n",
    "\n",
    "## Decision:\n",
    "Based on the selection criteria above, should this paper be INCLUDED or EXCLUDED from the systematic review?\n",
    "\n",
    "Answer with one word only: INCLUDE or EXCLUDE\"\"\"\n",
    "\n",
    "\n",
    "def create_cot_prompt(criteria: str, paper_title: str, paper_abstract: str) -> str:\n",
    "    \"\"\"Create a chain-of-thought screening prompt.\"\"\"\n",
    "    return f\"\"\"## Systematic Review Selection Criteria:\n",
    "{criteria}\n",
    "\n",
    "## Paper to Screen:\n",
    "Title: {paper_title}\n",
    "\n",
    "Abstract: {paper_abstract}\n",
    "\n",
    "## Task:\n",
    "Analyze whether this paper meets the selection criteria for the systematic review.\n",
    "\n",
    "First, briefly explain your reasoning (2-3 sentences).\n",
    "Then, provide your final decision on a new line starting with \"DECISION:\" followed by either INCLUDE or EXCLUDE.\"\"\"\n",
    "\n",
    "\n",
    "# Test prompt creation\n",
    "sample = val_df.iloc[0]\n",
    "prompt = create_zero_shot_prompt(\n",
    "    sample[\"review_criteria\"],\n",
    "    sample[\"paper_title\"],\n",
    "    sample[\"paper_abstract\"][:500] + \"...\"\n",
    ")\n",
    "print(\"Sample zero-shot prompt:\")\n",
    "print(prompt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38088c",
   "metadata": {},
   "source": [
    "## LLM Inference Setup\n",
    "\n",
    "We support multiple backends:\n",
    "1. **Ollama** (local models) - recommended for open-source models\n",
    "2. **OpenAI-compatible API** (for cloud-hosted models)\n",
    "\n",
    "### Installing Ollama\n",
    "If you haven't installed Ollama:\n",
    "1. Download from https://ollama.ai\n",
    "2. Run: `ollama pull llama3.2` or `ollama pull mistral`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc0408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDE                                            -> 1\n",
      "EXCLUDE                                            -> 0\n",
      "Based on the criteria, this paper should be INCLUD -> 1\n",
      "The paper does not meet criteria. DECISION: EXCLUD -> 0\n",
      "I think we should include it.                      -> 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "def call_ollama(prompt: str, model: str = \"llama3.2\", system: str = SYSTEM_PROMPT) -> str:\n",
    "    \"\"\"Call Ollama API for inference using requests.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"options\": {\"temperature\": 0.0},\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"message\"][\"content\"]\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Ollama not running. Start it with: ollama serve\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_decision(response: str) -> int | None:\n",
    "    \"\"\"Parse LLM response into binary decision.\"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "        \n",
    "    response_upper = response.upper()\n",
    "    \n",
    "    # Check for DECISION: prefix (for CoT prompts)\n",
    "    if \"DECISION:\" in response_upper:\n",
    "        after_decision = response_upper.split(\"DECISION:\")[-1].strip()\n",
    "        words = after_decision.split()\n",
    "        if words:\n",
    "            if \"INCLUDE\" in words[0]:\n",
    "                return 1\n",
    "            elif \"EXCLUDE\" in words[0]:\n",
    "                return 0\n",
    "    \n",
    "    # Simple keyword matching\n",
    "    if \"INCLUDE\" in response_upper and \"EXCLUDE\" not in response_upper:\n",
    "        return 1\n",
    "    elif \"EXCLUDE\" in response_upper and \"INCLUDE\" not in response_upper:\n",
    "        return 0\n",
    "    \n",
    "    # Check first word\n",
    "    first_word = response_upper.strip().split()[0] if response_upper.strip() else \"\"\n",
    "    if \"INCLUDE\" in first_word:\n",
    "        return 1\n",
    "    elif \"EXCLUDE\" in first_word:\n",
    "        return 0\n",
    "    \n",
    "    return None  # Unable to parse\n",
    "\n",
    "\n",
    "# Test the parser\n",
    "test_responses = [\n",
    "    \"INCLUDE\",\n",
    "    \"EXCLUDE\",\n",
    "    \"Based on the criteria, this paper should be INCLUDED.\",\n",
    "    \"The paper does not meet criteria. DECISION: EXCLUDE\",\n",
    "    \"I think we should include it.\"\n",
    "]\n",
    "for r in test_responses:\n",
    "    print(f\"{r[:50]:50s} -> {parse_decision(r)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ab9dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama not running!\n",
      "\n",
      "To set up Ollama:\n",
      "1. Download from https://ollama.ai\n",
      "2. Run: ollama serve\n",
      "3. In another terminal: ollama pull llama3.2\n"
     ]
    }
   ],
   "source": [
    "# Check available Ollama models\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    response.raise_for_status()\n",
    "    models = response.json().get(\"models\", [])\n",
    "    print(\"Available Ollama models:\")\n",
    "    for m in models:\n",
    "        print(f\"  - {m['name']}\")\n",
    "    if not models:\n",
    "        print(\"  No models found. Run: ollama pull llama3.2\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Ollama not running!\")\n",
    "    print(\"\\nTo set up Ollama:\")\n",
    "    print(\"1. Download from https://ollama.ai\")\n",
    "    print(\"2. Run: ollama serve\")\n",
    "    print(\"3. In another terminal: ollama pull llama3.2\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking Ollama: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b80b21",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0187b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    prompt_type: Literal[\"zero_shot\", \"cot\"] = \"zero_shot\",\n",
    "    max_samples: int | None = None,\n",
    "    save_results: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run LLM evaluation on the validation set.\n",
    "    \n",
    "    Args:\n",
    "        df: Validation DataFrame\n",
    "        model: Ollama model name\n",
    "        prompt_type: \"zero_shot\" or \"cot\"\n",
    "        max_samples: Limit number of samples (for testing)\n",
    "        save_results: Save results to CSV\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    if max_samples:\n",
    "        df = df.head(max_samples).copy()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "    \n",
    "    results = []\n",
    "    prompt_fn = create_zero_shot_prompt if prompt_type == \"zero_shot\" else create_cot_prompt\n",
    "    \n",
    "    print(f\"Running evaluation: model={model}, prompt={prompt_type}, samples={len(df)}\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating\"):\n",
    "        prompt = prompt_fn(\n",
    "            row[\"review_criteria\"],\n",
    "            row[\"paper_title\"],\n",
    "            row[\"paper_abstract\"]\n",
    "        )\n",
    "        \n",
    "        response = call_ollama(prompt, model=model)\n",
    "        prediction = parse_decision(response)\n",
    "        \n",
    "        results.append({\n",
    "            \"review_pmid\": row[\"review_pmid\"],\n",
    "            \"paper_pmid\": row[\"paper_pmid\"],\n",
    "            \"label\": row[\"label\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"response\": response[:500],  # Truncate for storage\n",
    "            \"model\": model,\n",
    "            \"prompt_type\": prompt_type\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if save_results:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"eval_{model.replace(':', '_')}_{prompt_type}_{timestamp}.csv\"\n",
    "        results_df.to_csv(RESULTS_DIR / filename, index=False)\n",
    "        print(f\"Saved results to: {RESULTS_DIR / filename}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c5c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics.\n",
    "    \n",
    "    Returns dict with:\n",
    "        - precision, recall, f1 (for \"include\" class)\n",
    "        - accuracy\n",
    "        - cohen_kappa\n",
    "        - confusion_matrix\n",
    "        - parse_failures\n",
    "    \"\"\"\n",
    "    # Filter out parse failures\n",
    "    valid = results_df[results_df[\"prediction\"].notna()].copy()\n",
    "    parse_failures = len(results_df) - len(valid)\n",
    "    \n",
    "    if len(valid) == 0:\n",
    "        return {\"error\": \"No valid predictions\"}\n",
    "    \n",
    "    y_true = valid[\"label\"].values\n",
    "    y_pred = valid[\"prediction\"].astype(int).values\n",
    "    \n",
    "    metrics = {\n",
    "        \"n_samples\": len(results_df),\n",
    "        \"n_valid\": len(valid),\n",
    "        \"parse_failures\": parse_failures,\n",
    "        \"parse_failure_rate\": parse_failures / len(results_df),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, pos_label=1),\n",
    "        \"recall\": recall_score(y_true, y_pred, pos_label=1),\n",
    "        \"f1\": f1_score(y_true, y_pred, pos_label=1),\n",
    "        \"cohen_kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "    }\n",
    "    \n",
    "    # Agreement rate (same as accuracy)\n",
    "    metrics[\"agreement_rate\"] = metrics[\"accuracy\"]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_metrics(metrics: dict, model: str = \"\", prompt_type: str = \"\"):\n",
    "    \"\"\"Pretty-print evaluation metrics.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if model:\n",
    "        print(f\"Model: {model} | Prompt: {prompt_type}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Samples: {metrics['n_samples']} | Valid: {metrics['n_valid']} | Parse failures: {metrics['parse_failures']} ({metrics['parse_failure_rate']:.1%})\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"Accuracy:       {metrics['accuracy']:.3f}\")\n",
    "    print(f\"Precision:      {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall:         {metrics['recall']:.3f}\")\n",
    "    print(f\"F1 Score:       {metrics['f1']:.3f}\")\n",
    "    print(f\"Cohen's Kappa:  {metrics['cohen_kappa']:.3f}\")\n",
    "    print(\"-\"*60)\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"                 Predicted\")\n",
    "    print(f\"                 Excl   Incl\")\n",
    "    print(f\"  Actual Excl    {cm[0][0]:4d}   {cm[0][1]:4d}\")\n",
    "    print(f\"  Actual Incl    {cm[1][0]:4d}   {cm[1][1]:4d}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329cc0c",
   "metadata": {},
   "source": [
    "## Test Run (Small Sample)\n",
    "\n",
    "First, let's test with a small sample to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89de831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: llama3.2\n",
      "Running evaluation: model=llama3.2, prompt=zero_shot, samples=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:25<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample results:\n",
      "   label  prediction response\n",
      "0      1           1  INCLUDE\n",
      "1      1           1  INCLUDE\n",
      "2      1           1  INCLUDE\n",
      "3      1           0  EXCLUDE\n",
      "4      1           1  INCLUDE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with 10 samples first\n",
    "MODEL = \"llama3.2\"  # Change to your available model\n",
    "\n",
    "print(f\"Testing with model: {MODEL}\")\n",
    "test_results = run_evaluation(\n",
    "    val_df, \n",
    "    model=MODEL, \n",
    "    prompt_type=\"zero_shot\",\n",
    "    max_samples=10,\n",
    "    save_results=False\n",
    ")\n",
    "\n",
    "print(\"\\nSample results:\")\n",
    "print(test_results[[\"label\", \"prediction\", \"response\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae880a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Model: llama3.2 | Prompt: zero_shot\n",
      "============================================================\n",
      "Samples: 10 | Valid: 10 | Parse failures: 0 (0.0%)\n",
      "------------------------------------------------------------\n",
      "Accuracy:       0.700\n",
      "Precision:      0.667\n",
      "Recall:         0.800\n",
      "F1 Score:       0.727\n",
      "Cohen's Kappa:  0.400\n",
      "------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Excl   Incl\n",
      "  Actual Excl       3      2\n",
      "  Actual Incl       1      4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics on test run\n",
    "test_metrics = calculate_metrics(test_results)\n",
    "print_metrics(test_metrics, MODEL, \"zero_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642897d7",
   "metadata": {},
   "source": [
    "## Full Evaluation\n",
    "\n",
    "Run evaluation on the complete validation set. This may take 30-60 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427c2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full evaluation...\n",
      "Running evaluation: model=llama3.2, prompt=zero_shot, samples=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [41:54<00:00,  2.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\\eval_llama3.2_zero_shot_20260115_193605.csv\n",
      "\n",
      "============================================================\n",
      "Model: llama3.2 | Prompt: zero_shot\n",
      "============================================================\n",
      "Samples: 1000 | Valid: 1000 | Parse failures: 0 (0.0%)\n",
      "------------------------------------------------------------\n",
      "Accuracy:       0.647\n",
      "Precision:      0.596\n",
      "Recall:         0.910\n",
      "F1 Score:       0.721\n",
      "Cohen's Kappa:  0.294\n",
      "------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Excl   Incl\n",
      "  Actual Excl     192    308\n",
      "  Actual Incl      45    455\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full evaluation - running on all 1000 samples\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "print(\"Running full evaluation...\")\n",
    "full_results = run_evaluation(\n",
    "    val_df, \n",
    "    model=MODEL, \n",
    "    prompt_type=\"zero_shot\",\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "metrics = calculate_metrics(full_results)\n",
    "print_metrics(metrics, MODEL, \"zero_shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83212e00",
   "metadata": {},
   "source": [
    "## Compare Multiple Models\n",
    "\n",
    "Run evaluation across different models and prompt types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da428ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for multi-model comparison\n",
    "MODELS_TO_TEST = [\n",
    "    \"llama3.2\",\n",
    "    \"mistral\",\n",
    "]\n",
    "\n",
    "PROMPT_TYPES = [\"zero_shot\", \"cot\"]\n",
    "\n",
    "# Store all results\n",
    "all_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b52086bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating: llama3.2 with zero_shot\n",
      "============================================================\n",
      "Running evaluation: model=llama3.2, prompt=zero_shot, samples=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [42:43<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\\eval_llama3.2_zero_shot_20260115_201927.csv\n",
      "\n",
      "============================================================\n",
      "Model: llama3.2 | Prompt: zero_shot\n",
      "============================================================\n",
      "Samples: 1000 | Valid: 1000 | Parse failures: 0 (0.0%)\n",
      "------------------------------------------------------------\n",
      "Accuracy:       0.649\n",
      "Precision:      0.598\n",
      "Recall:         0.912\n",
      "F1 Score:       0.722\n",
      "Cohen's Kappa:  0.298\n",
      "------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Excl   Incl\n",
      "  Actual Excl     193    307\n",
      "  Actual Incl      44    456\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: llama3.2 with cot\n",
      "============================================================\n",
      "Running evaluation: model=llama3.2, prompt=cot, samples=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|███████   | 712/1000 [1:16:14<53:17:34, 666.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=120)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [1:32:42<00:00,  5.56s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\\eval_llama3.2_cot_20260115_215209.csv\n",
      "\n",
      "============================================================\n",
      "Model: llama3.2 | Prompt: cot\n",
      "============================================================\n",
      "Samples: 1000 | Valid: 999 | Parse failures: 1 (0.1%)\n",
      "------------------------------------------------------------\n",
      "Accuracy:       0.704\n",
      "Precision:      0.809\n",
      "Recall:         0.533\n",
      "F1 Score:       0.643\n",
      "Cohen's Kappa:  0.407\n",
      "------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Excl   Incl\n",
      "  Actual Excl     437     63\n",
      "  Actual Incl     233    266\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: mistral with zero_shot\n",
      "============================================================\n",
      "Running evaluation: model=mistral, prompt=zero_shot, samples=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [1:25:52<00:00,  5.15s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\\eval_mistral_zero_shot_20260115_231802.csv\n",
      "\n",
      "============================================================\n",
      "Model: mistral | Prompt: zero_shot\n",
      "============================================================\n",
      "Samples: 1000 | Valid: 1000 | Parse failures: 0 (0.0%)\n",
      "------------------------------------------------------------\n",
      "Accuracy:       0.811\n",
      "Precision:      0.846\n",
      "Recall:         0.760\n",
      "F1 Score:       0.801\n",
      "Cohen's Kappa:  0.622\n",
      "------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Excl   Incl\n",
      "  Actual Excl     431     69\n",
      "  Actual Incl     120    380\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: mistral with cot\n",
      "============================================================\n",
      "Running evaluation: model=mistral, prompt=cot, samples=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [1:14:05<00:00,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\\eval_mistral_cot_20260116_003208.csv\n",
      "\n",
      "============================================================\n",
      "Model: mistral | Prompt: cot\n",
      "============================================================\n",
      "Samples: 1000 | Valid: 998 | Parse failures: 2 (0.2%)\n",
      "------------------------------------------------------------\n",
      "Accuracy:       0.688\n",
      "Precision:      0.902\n",
      "Recall:         0.423\n",
      "F1 Score:       0.576\n",
      "Cohen's Kappa:  0.377\n",
      "------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "                 Predicted\n",
      "                 Excl   Incl\n",
      "  Actual Excl     476     23\n",
      "  Actual Incl     288    211\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run comparison across models and prompt types\n",
    "for model in MODELS_TO_TEST:\n",
    "    for prompt_type in PROMPT_TYPES:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: {model} with {prompt_type}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = run_evaluation(\n",
    "            val_df,\n",
    "            model=model,\n",
    "            prompt_type=prompt_type,\n",
    "            save_results=True\n",
    "        )\n",
    "        \n",
    "        metrics = calculate_metrics(results)\n",
    "        metrics[\"model\"] = model\n",
    "        metrics[\"prompt_type\"] = prompt_type\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print_metrics(metrics, model, prompt_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e64e3f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cohen_kappa",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "parse_failure_rate",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1f93e6fc-d3cc-4805-9ac1-561606f50d52",
       "rows": [
        [
         "2",
         "mistral",
         "zero_shot",
         "0.811",
         "0.846",
         "0.76",
         "0.801",
         "0.622",
         "0.0"
        ],
        [
         "0",
         "llama3.2",
         "zero_shot",
         "0.649",
         "0.598",
         "0.912",
         "0.722",
         "0.298",
         "0.0"
        ],
        [
         "1",
         "llama3.2",
         "cot",
         "0.704",
         "0.809",
         "0.533",
         "0.643",
         "0.407",
         "0.001"
        ],
        [
         "3",
         "mistral",
         "cot",
         "0.688",
         "0.902",
         "0.423",
         "0.576",
         "0.377",
         "0.002"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>cohen_kappa</th>\n",
       "      <th>parse_failure_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral</td>\n",
       "      <td>zero_shot</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.2</td>\n",
       "      <td>zero_shot</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3.2</td>\n",
       "      <td>cot</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral</td>\n",
       "      <td>cot</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model prompt_type  accuracy  precision  recall     f1  cohen_kappa  \\\n",
       "2   mistral   zero_shot     0.811      0.846   0.760  0.801        0.622   \n",
       "0  llama3.2   zero_shot     0.649      0.598   0.912  0.722        0.298   \n",
       "1  llama3.2         cot     0.704      0.809   0.533  0.643        0.407   \n",
       "3   mistral         cot     0.688      0.902   0.423  0.576        0.377   \n",
       "\n",
       "   parse_failure_rate  \n",
       "2               0.000  \n",
       "0               0.000  \n",
       "1               0.001  \n",
       "3               0.002  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved comparison to: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\\model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Create comparison table\n",
    "if all_metrics:\n",
    "    comparison_df = pd.DataFrame(all_metrics)\n",
    "    comparison_df = comparison_df[[\"model\", \"prompt_type\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"cohen_kappa\", \"parse_failure_rate\"]]\n",
    "    comparison_df = comparison_df.round(3)\n",
    "    display(comparison_df.sort_values(\"f1\", ascending=False))\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_df.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)\n",
    "    print(f\"\\nSaved comparison to: {RESULTS_DIR / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c642b5",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Examine patterns in model failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d60948d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(results_df: pd.DataFrame, val_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze patterns in model errors.\n",
    "    \n",
    "    Returns:\n",
    "        - False positives (predicted include, actual exclude)\n",
    "        - False negatives (predicted exclude, actual include)\n",
    "    \"\"\"\n",
    "    # Merge with original data for context\n",
    "    merged = results_df.merge(\n",
    "        val_df[[\"review_pmid\", \"paper_pmid\", \"review_title\", \"paper_title\", \"paper_abstract\"]],\n",
    "        on=[\"review_pmid\", \"paper_pmid\"]\n",
    "    )\n",
    "    \n",
    "    valid = merged[merged[\"prediction\"].notna()].copy()\n",
    "    valid[\"prediction\"] = valid[\"prediction\"].astype(int)\n",
    "    \n",
    "    # False positives: predicted 1, actual 0\n",
    "    fp = valid[(valid[\"prediction\"] == 1) & (valid[\"label\"] == 0)]\n",
    "    \n",
    "    # False negatives: predicted 0, actual 1\n",
    "    fn = valid[(valid[\"prediction\"] == 0) & (valid[\"label\"] == 1)]\n",
    "    \n",
    "    print(f\"False Positives (wrongly included): {len(fp)}\")\n",
    "    print(f\"False Negatives (wrongly excluded): {len(fn)}\")\n",
    "    \n",
    "    return {\n",
    "        \"false_positives\": fp,\n",
    "        \"false_negatives\": fn\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage (uncomment after running evaluation):\n",
    "# errors = error_analysis(full_results, val_df)\n",
    "# \n",
    "# print(\"\\nSample False Negatives (should have been included):\")\n",
    "# for _, row in errors[\"false_negatives\"].head(3).iterrows():\n",
    "#     print(f\"\\nReview: {row['review_title'][:60]}...\")\n",
    "#     print(f\"Paper: {row['paper_title']}\")\n",
    "#     print(f\"Response: {row['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c802075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives (wrongly included): 69\n",
      "False Negatives (wrongly excluded): 120\n",
      "\n",
      "============================================================\n",
      "SAMPLE FALSE POSITIVES (Over-inclusion errors)\n",
      "============================================================\n",
      "\n",
      "📋 Review: Bias due to selective inclusion and reporting of outcomes and analyses in system...\n",
      "📄 Paper: Impact of nonfatal myocardial infarction on outcomes in patients with advanced h...\n",
      "🤖 Response:  INCLUDE...\n",
      "\n",
      "📋 Review: Bias due to selective inclusion and reporting of outcomes and analyses in system...\n",
      "📄 Paper: Obesity Increases Risk-Adjusted Morbidity, Mortality, and Cost Following Cardiac...\n",
      "🤖 Response:  INCLUDE...\n",
      "\n",
      "📋 Review: Bias due to selective inclusion and reporting of outcomes and analyses in system...\n",
      "📄 Paper: Cardiovascular risk assessment scores for people with diabetes: a systematic rev...\n",
      "🤖 Response:  INCLUDE...\n",
      "\n",
      "============================================================\n",
      "SAMPLE FALSE NEGATIVES (Missed relevant papers)\n",
      "============================================================\n",
      "\n",
      "📋 Review: Evaluation of follow-up strategies for patients with epithelial ovarian cancer f...\n",
      "📄 Paper: PET-CT in recurrent ovarian cancer: impact on treatment planning....\n",
      "🤖 Response:  EXCLUDE...\n",
      "\n",
      "📋 Review: Evaluation of follow-up strategies for patients with epithelial ovarian cancer f...\n",
      "📄 Paper: Nurse led follow up and conventional medical follow up in management of patients...\n",
      "🤖 Response:  EXCLUDE...\n",
      "\n",
      "📋 Review: Active placebos versus antidepressants for depression....\n",
      "📄 Paper: Reliability of depression and associated clinical symptoms....\n",
      "🤖 Response:  EXCLUDE...\n"
     ]
    }
   ],
   "source": [
    "# Load best model results and run error analysis\n",
    "best_model_results = pd.read_csv(RESULTS_DIR / \"eval_mistral_zero_shot_20260115_231802.csv\")\n",
    "\n",
    "errors = error_analysis(best_model_results, val_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE FALSE POSITIVES (Over-inclusion errors)\")\n",
    "print(\"=\"*60)\n",
    "for _, row in errors[\"false_positives\"].head(3).iterrows():\n",
    "    print(f\"\\n📋 Review: {row['review_title'][:80]}...\")\n",
    "    print(f\"📄 Paper: {row['paper_title'][:80]}...\")\n",
    "    if 'response' in row:\n",
    "        print(f\"🤖 Response: {str(row['response'])[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE FALSE NEGATIVES (Missed relevant papers)\")\n",
    "print(\"=\"*60)\n",
    "for _, row in errors[\"false_negatives\"].head(3).iterrows():\n",
    "    print(f\"\\n📋 Review: {row['review_title'][:80]}...\")\n",
    "    print(f\"📄 Paper: {row['paper_title'][:80]}...\")\n",
    "    if 'response' in row:\n",
    "        print(f\"🤖 Response: {str(row['response'])[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290119af",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Metrics Interpretation\n",
    "- **Recall** is critical for systematic reviews (we don't want to miss relevant papers)\n",
    "- **Precision** affects workload (false positives mean extra manual review)\n",
    "- **Cohen's Kappa** measures agreement beyond chance\n",
    "\n",
    "### Recommendations for UKHSA\n",
    "1. Prioritize **high recall** models for initial screening\n",
    "2. Use LLM as first-pass filter, with human review of borderline cases\n",
    "3. Consider ensemble approaches (multiple models voting)\n",
    "\n",
    "### Next Steps\n",
    "1. Test additional models (Mistral, Phi-3, Gemma)\n",
    "2. Experiment with few-shot prompting\n",
    "3. Fine-tune a model on Cochrane screening data\n",
    "4. Build production pipeline with logging and monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
