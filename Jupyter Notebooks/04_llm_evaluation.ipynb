{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Screening Evaluation Pipeline\n",
    "\n",
    "**Summary:** This notebook evaluates how well various LLMs can screen paper abstracts for inclusion in systematic reviews. I test both open-source models (Llama 3.2 and Mistral via Ollama) and proprietary APIs (Gemini 3 Pro Preview, GPT-5.2 Thinking, Claude Opus 4.5) with two prompt strategies (zero-shot and chain-of-thought) on a ground-truth validation set of 1,000 labeled abstracts.\n",
    "\n",
    "---\n",
    "\n",
    "## Models Evaluated\n",
    "\n",
    "| Model | Type | Provider |\n",
    "|-------|------|----------|\n",
    "| Llama 3.2 3B | Open-source (via Ollama) | Meta |\n",
    "| Mistral 7B Instruct | Open-source (via Ollama) | Mistral AI |\n",
    "| Gemini 3 Pro Preview | Proprietary API | Google |\n",
    "| GPT-5.2 (Thinking/High-Reasoning) | Proprietary API | OpenAI |\n",
    "| Claude Opus 4.5 | Proprietary API | Anthropic |\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Ground Truth Dataset Construction\n",
    "The validation set consists of 1,000 paper-review pairs sampled from Cochrane systematic reviews:\n",
    "- **100 Cochrane reviews** were randomly selected from reviews that have clearly defined inclusion criteria and at least 5 included studies with available abstracts\n",
    "- For each review, I sample **5 \"included\" papers** (papers that were actually included in the review, serving as positive examples) and **5 \"excluded\" papers** (papers that were NOT cited by the review but share the same medical topic, serving as hard negative examples)\n",
    "- Final dataset: **500 included + 500 excluded = 1,000 labeled pairs**\n",
    "\n",
    "### Prompt Strategies\n",
    "\n",
    "**Zero-shot prompt:** A direct instruction asking the LLM to decide INCLUDE or EXCLUDE based on whether the paper is relevant to the review topic.\n",
    "\n",
    "**Chain-of-thought (CoT) prompt:** The LLM is asked to reason step-by-step before making a decision.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Load ground-truth validation set (500 included, 500 excluded papers)\n",
    "2. Define prompt templates (zero-shot and chain-of-thought)\n",
    "3. Set up API clients (Ollama for local models, API keys for proprietary)\n",
    "4. Run evaluation on all models\n",
    "5. Compute metrics (accuracy, precision, recall, F1, Cohen's kappa)\n",
    "6. Compare results and analyze errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\.env\n",
      "Data directory: C:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\n",
      "Results directory: C:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\n",
      "\n",
      "API Keys loaded:\n",
      "  Gemini:    âœ… Set\n",
      "  OpenAI:    âœ… Set\n",
      "  Anthropic: âœ… Set\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: IMPORTS AND SETUP\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "\n",
    "# Load environment variables from .env file (force override)\n",
    "ENV_FILE = Path(r\"c:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\.env\")\n",
    "load_dotenv(ENV_FILE, override=True)\n",
    "print(f\"Loading .env from: {ENV_FILE}\")\n",
    "\n",
    "DATA_DIR = Path(\"../Data\")\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# API Configuration\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")  # Note: underscore in env var name\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"Results directory: {RESULTS_DIR.resolve()}\")\n",
    "print(f\"\\nAPI Keys loaded:\")\n",
    "print(f\"  Gemini:    {'âœ… Set' if GEMINI_API_KEY else 'âŒ Missing'}\")\n",
    "print(f\"  OpenAI:    {'âœ… Set' if OPENAI_API_KEY else 'âŒ Missing'}\")\n",
    "print(f\"  Anthropic: {'âœ… Set' if ANTHROPIC_API_KEY else 'âŒ Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1,000 records\n",
      "Label distribution: {1: 500, 0: 500}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD VALIDATION DATA\n",
    "# ============================================================\n",
    "df = pd.read_csv(DATA_DIR / \"ground_truth_validation_set.csv\")\n",
    "print(f\"âœ… Loaded {len(df):,} records\")\n",
    "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt templates defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: PROMPT TEMPLATES\n",
    "# ============================================================\n",
    "\n",
    "ZERO_SHOT_TEMPLATE = \"\"\"You are a systematic review screener. Based on the abstract below, decide if this paper should be INCLUDED or EXCLUDED from a systematic review about:\n",
    "\"{review_title}\"\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Answer with exactly one word: INCLUDE or EXCLUDE\"\"\"\n",
    "\n",
    "COT_TEMPLATE = \"\"\"You are a systematic review screener. Your task is to decide if a paper should be included in a systematic review.\n",
    "\n",
    "Review topic: \"{review_title}\"\n",
    "\n",
    "Abstract to screen:\n",
    "{abstract}\n",
    "\n",
    "Think through this step by step:\n",
    "1. What is the main topic of this paper?\n",
    "2. Does it relate to the systematic review topic?\n",
    "3. Does it appear to provide relevant evidence?\n",
    "\n",
    "After your reasoning, give your final answer on a new line as exactly: DECISION: INCLUDE or DECISION: EXCLUDE\"\"\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"zero_shot\": ZERO_SHOT_TEMPLATE,\n",
    "    \"cot\": COT_TEMPLATE\n",
    "}\n",
    "\n",
    "print(\"âœ… Prompt templates defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: OLLAMA FUNCTIONS (for local models)\n",
    "# ============================================================\n",
    "\n",
    "def call_ollama(model: str, prompt: str, timeout: int = 120) -> str:\n",
    "    \"\"\"Send a prompt to Ollama and return the response text.\"\"\"\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
    "            timeout=timeout\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        return resp.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "def parse_decision(response: str, prompt_type: str) -> str:\n",
    "    \"\"\"Extract INCLUDE/EXCLUDE from LLM response.\"\"\"\n",
    "    text = response.upper()\n",
    "    if prompt_type == \"cot\":\n",
    "        if \"DECISION: INCLUDE\" in text or \"DECISION:INCLUDE\" in text:\n",
    "            return \"include\"\n",
    "        elif \"DECISION: EXCLUDE\" in text or \"DECISION:EXCLUDE\" in text:\n",
    "            return \"exclude\"\n",
    "    if \"INCLUDE\" in text and \"EXCLUDE\" not in text:\n",
    "        return \"include\"\n",
    "    elif \"EXCLUDE\" in text and \"INCLUDE\" not in text:\n",
    "        return \"exclude\"\n",
    "    elif text.strip().startswith(\"INCLUDE\"):\n",
    "        return \"include\"\n",
    "    elif text.strip().startswith(\"EXCLUDE\"):\n",
    "        return \"exclude\"\n",
    "    return \"unclear\"\n",
    "\n",
    "print(\"âœ… Ollama functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini 2.0 Flash initialized\n",
      "âœ… OpenAI GPT-4o initialized\n",
      "âœ… Claude 3 Haiku initialized\n",
      "\n",
      "âœ… All API clients ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: PROPRIETARY API CLIENTS\n",
    "# ============================================================\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "# --- GEMINI 2.0 Flash ---\n",
    "gemini_model = None\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    gemini_model = genai.GenerativeModel('gemini-2.0-flash')  # Available model\n",
    "    print(\"âœ… Gemini 2.0 Flash initialized\")\n",
    "\n",
    "def generate_gemini(prompt: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate response using Gemini 2.0 Flash.\"\"\"\n",
    "    if not gemini_model:\n",
    "        return \"ERROR: Gemini API key not set\"\n",
    "    try:\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                max_output_tokens=max_tokens,\n",
    "                temperature=0.1\n",
    "            )\n",
    "        )\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "# --- GPT-4o ---\n",
    "openai_client = None\n",
    "if OPENAI_API_KEY:\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY, timeout=120.0)\n",
    "    print(\"âœ… OpenAI GPT-4o initialized\")\n",
    "\n",
    "def generate_gpt(prompt: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate response using GPT-4o.\"\"\"\n",
    "    if not openai_client:\n",
    "        return \"ERROR: OpenAI API key not set\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "# --- Claude 3 Haiku ---\n",
    "anthropic_client = None\n",
    "if ANTHROPIC_API_KEY:\n",
    "    anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY, timeout=120.0)\n",
    "    print(\"âœ… Claude 3 Haiku initialized\")\n",
    "\n",
    "def generate_claude(prompt: str, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Generate response using Claude 3 Haiku.\"\"\"\n",
    "    if not anthropic_client:\n",
    "        return \"ERROR: Anthropic API key not set\"\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",  # Most basic/available model\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "print(\"\\nâœ… All API clients ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking Ollama models...\n",
      "   Available: ['mistral:latest', 'llama3.2:latest']\n",
      "\n",
      "ðŸ” Checking API models...\n",
      "   Available: ['gemini-2.0-flash', 'gpt-4o', 'claude-3-haiku-20240307']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: CHECK AVAILABLE MODELS\n",
    "# ============================================================\n",
    "\n",
    "# Check Ollama models\n",
    "print(\"ðŸ” Checking Ollama models...\")\n",
    "try:\n",
    "    resp = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    ollama_models = [m[\"name\"] for m in resp.json().get(\"models\", [])]\n",
    "    print(f\"   Available: {ollama_models}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ Could not connect to Ollama: {e}\")\n",
    "    print(\"   Make sure Ollama is running (ollama serve)\")\n",
    "    ollama_models = []\n",
    "\n",
    "# Check API models\n",
    "print(\"\\nðŸ” Checking API models...\")\n",
    "api_models = []\n",
    "if GEMINI_API_KEY:\n",
    "    api_models.append(\"gemini-2.0-flash\")\n",
    "if OPENAI_API_KEY:\n",
    "    api_models.append(\"gpt-4o\")\n",
    "if ANTHROPIC_API_KEY:\n",
    "    api_models.append(\"claude-3-haiku-20240307\")\n",
    "print(f\"   Available: {api_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 8: EVALUATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def run_evaluation_ollama(df: pd.DataFrame, model: str, prompt_type: str, \n",
    "                          template: str, limit: int = None):\n",
    "    \"\"\"Run evaluation using Ollama (local models).\"\"\"\n",
    "    data = df.head(limit) if limit else df\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model}/{prompt_type}\"):\n",
    "        prompt = template.format(\n",
    "            review_title=row[\"review_title\"],\n",
    "            abstract=str(row[\"paper_abstract\"])[:3000]\n",
    "        )\n",
    "        response = call_ollama(model, prompt)\n",
    "        prediction = parse_decision(response, prompt_type)\n",
    "        \n",
    "        results.append({\n",
    "            \"paper_pmid\": row[\"paper_pmid\"],\n",
    "            \"true_label\": row[\"label\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"raw_response\": response[:500] if response else \"\",\n",
    "            \"model\": model,\n",
    "            \"prompt_type\": prompt_type\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def run_evaluation_api(df: pd.DataFrame, model_name: str, generate_fn, \n",
    "                       prompt_type: str, template: str, limit: int = None,\n",
    "                       delay: float = 0.5):\n",
    "    \"\"\"Run evaluation using API models (Gemini, GPT, Claude).\"\"\"\n",
    "    data = df.head(limit) if limit else df\n",
    "    results = []\n",
    "    max_tokens = 300 if prompt_type == \"cot\" else 50\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model_name}/{prompt_type}\"):\n",
    "        prompt = template.format(\n",
    "            review_title=row[\"review_title\"],\n",
    "            abstract=str(row[\"paper_abstract\"])[:3000]\n",
    "        )\n",
    "        \n",
    "        response = generate_fn(prompt, max_tokens)\n",
    "        prediction = parse_decision(response, prompt_type)\n",
    "        \n",
    "        results.append({\n",
    "            \"paper_pmid\": row[\"paper_pmid\"],\n",
    "            \"true_label\": row[\"label\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"raw_response\": response[:500] if response else \"\",\n",
    "            \"model\": model_name,\n",
    "            \"prompt_type\": prompt_type\n",
    "        })\n",
    "        \n",
    "        if delay > 0:\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"âœ… Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metrics functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 9: METRICS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def compute_metrics(results_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
    "    valid = results_df[results_df[\"prediction\"].isin([\"include\", \"exclude\"])].copy()\n",
    "    y_true = valid[\"true_label\"].astype(int)\n",
    "    y_pred = (valid[\"prediction\"] == \"include\").astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"n_valid\": len(valid),\n",
    "        \"n_unclear\": len(results_df) - len(valid),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics: dict, model: str, prompt_type: str):\n",
    "    \"\"\"Display metrics in a readable format.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model} | Prompt: {prompt_type}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.1%}\")\n",
    "    print(f\"Precision: {metrics['precision']:.1%}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.1%}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.3f}\")\n",
    "    print(f\"Kappa:     {metrics['kappa']:.3f}\")\n",
    "    print(f\"Valid:     {metrics['n_valid']} | Unclear: {metrics['n_unclear']}\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"          Pred Excl  Pred Incl\")\n",
    "    print(f\"True Excl    {cm[0][0]:4d}       {cm[0][1]:4d}\")\n",
    "    print(f\"True Incl    {cm[1][0]:4d}       {cm[1][1]:4d}\")\n",
    "\n",
    "print(\"âœ… Metrics functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Quick test with 10 samples each...\n",
      "\n",
      "Testing Llama 3.2 (Ollama)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 90.0%, F1: 0.889\n",
      "Testing Gemini 2.0 Flash...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 90.0%, F1: 0.889\n",
      "Testing GPT-4o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 70.0%, F1: 0.571\n",
      "Testing Claude 3 Haiku...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-haiku-20240307/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 90.0%, F1: 0.889\n",
      "\n",
      "âœ… Quick tests complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 10: QUICK TEST (10 samples each)\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ§ª Quick test with 10 samples each...\\n\")\n",
    "\n",
    "# Test Ollama (if available)\n",
    "if \"llama3.2\" in str(ollama_models):\n",
    "    print(\"Testing Llama 3.2 (Ollama)...\")\n",
    "    test_ollama = run_evaluation_ollama(df, \"llama3.2\", \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10)\n",
    "    metrics = compute_metrics(test_ollama)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "else:\n",
    "    print(\"â­ï¸ Llama 3.2 not available in Ollama\")\n",
    "\n",
    "# Test Gemini 2.0 Flash\n",
    "if GEMINI_API_KEY:\n",
    "    print(\"Testing Gemini 2.0 Flash...\")\n",
    "    test_gemini = run_evaluation_api(df, \"gemini-2.0-flash\", generate_gemini, \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10, delay=1.0)\n",
    "    metrics = compute_metrics(test_gemini)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "\n",
    "# Test GPT-4o\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"Testing GPT-4o...\")\n",
    "    test_gpt = run_evaluation_api(df, \"gpt-4o\", generate_gpt, \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10, delay=1.0)\n",
    "    metrics = compute_metrics(test_gpt)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "\n",
    "# Test Claude 3 Haiku\n",
    "if ANTHROPIC_API_KEY:\n",
    "    print(\"Testing Claude 3 Haiku...\")\n",
    "    test_claude = run_evaluation_api(df, \"claude-3-haiku-20240307\", generate_claude, \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10, delay=1.0)\n",
    "    metrics = compute_metrics(test_claude)\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}, F1: {metrics['f1']:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Quick tests complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "ðŸš€ Running: llama3.2 with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [47:03<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_llama3.2_zero_shot_20260127_202237.csv\n",
      "\n",
      "==================================================\n",
      "Model: llama3.2 | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  81.5%\n",
      "Precision: 76.9%\n",
      "Recall:    90.0%\n",
      "F1 Score:  0.829\n",
      "Kappa:     0.630\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     365        135\n",
      "True Incl      50        450\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: llama3.2 with cot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/cot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:22:46<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_llama3.2_cot_20260127_214524.csv\n",
      "\n",
      "==================================================\n",
      "Model: llama3.2 | Prompt: cot\n",
      "==================================================\n",
      "Accuracy:  73.7%\n",
      "Precision: 91.1%\n",
      "Recall:    52.4%\n",
      "F1 Score:  0.665\n",
      "Kappa:     0.474\n",
      "Valid:     979 | Unclear: 21\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     467         25\n",
      "True Incl     232        255\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: mistral with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistral/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [44:50<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_mistral_zero_shot_20260127_223015.csv\n",
      "\n",
      "==================================================\n",
      "Model: mistral | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  84.5%\n",
      "Precision: 90.0%\n",
      "Recall:    77.6%\n",
      "F1 Score:  0.834\n",
      "Kappa:     0.690\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     457         43\n",
      "True Incl     112        388\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: mistral with cot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistral/cot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:57:51<00:00,  7.07s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_mistral_cot_20260128_002807.csv\n",
      "\n",
      "==================================================\n",
      "Model: mistral | Prompt: cot\n",
      "==================================================\n",
      "Accuracy:  85.3%\n",
      "Precision: 85.5%\n",
      "Recall:    85.0%\n",
      "F1 Score:  0.853\n",
      "Kappa:     0.706\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     428         72\n",
      "True Incl      75        425\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: gemini-2.0-flash with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [17:25<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_gemini_2_0_flash_zero_shot_20260128_004533.csv\n",
      "\n",
      "==================================================\n",
      "Model: gemini-2.0-flash | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  76.7%\n",
      "Precision: 97.5%\n",
      "Recall:    54.8%\n",
      "F1 Score:  0.702\n",
      "Kappa:     0.534\n",
      "Valid:     999 | Unclear: 1\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     492          7\n",
      "True Incl     226        274\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: gemini-2.0-flash with cot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash/cot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [28:07<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_gemini_2_0_flash_cot_20260128_011340.csv\n",
      "\n",
      "==================================================\n",
      "Model: gemini-2.0-flash | Prompt: cot\n",
      "==================================================\n",
      "Accuracy:  78.9%\n",
      "Precision: 97.4%\n",
      "Recall:    59.4%\n",
      "F1 Score:  0.738\n",
      "Kappa:     0.578\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     492          8\n",
      "True Incl     203        297\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: gpt-4o with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [17:09<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_gpt_4o_zero_shot_20260128_013049.csv\n",
      "\n",
      "==================================================\n",
      "Model: gpt-4o | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  72.1%\n",
      "Precision: 100.0%\n",
      "Recall:    44.2%\n",
      "F1 Score:  0.613\n",
      "Kappa:     0.442\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     500          0\n",
      "True Incl     279        221\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: gpt-4o with cot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o/cot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:13:57<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_gpt_4o_cot_20260128_024447.csv\n",
      "\n",
      "==================================================\n",
      "Model: gpt-4o | Prompt: cot\n",
      "==================================================\n",
      "Accuracy:  73.8%\n",
      "Precision: 99.6%\n",
      "Recall:    47.8%\n",
      "F1 Score:  0.646\n",
      "Kappa:     0.476\n",
      "Valid:     994 | Unclear: 6\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     497          1\n",
      "True Incl     259        237\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: claude-3-haiku-20240307 with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-haiku-20240307/zero_shot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [19:59<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_claude_3_haiku_20240307_zero_shot_20260128_030446.csv\n",
      "\n",
      "==================================================\n",
      "Model: claude-3-haiku-20240307 | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  80.1%\n",
      "Precision: 95.2%\n",
      "Recall:    63.4%\n",
      "F1 Score:  0.761\n",
      "Kappa:     0.602\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     484         16\n",
      "True Incl     183        317\n",
      "\n",
      "############################################################\n",
      "ðŸš€ Running: claude-3-haiku-20240307 with cot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-haiku-20240307/cot: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [36:26<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved: ..\\Data\\results\\eval_claude_3_haiku_20240307_cot_20260128_034113.csv\n",
      "\n",
      "==================================================\n",
      "Model: claude-3-haiku-20240307 | Prompt: cot\n",
      "==================================================\n",
      "Accuracy:  83.7%\n",
      "Precision: 94.0%\n",
      "Recall:    72.0%\n",
      "F1 Score:  0.815\n",
      "Kappa:     0.674\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     477         23\n",
      "True Incl     140        360\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ FULL EVALUATION COMPLETE!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 11: FULL EVALUATION - ALL MODELS\n",
    "# ============================================================\n",
    "# This evaluates all available models on all 1,000 samples.\n",
    "# Estimated time: \n",
    "#   - Ollama models: ~1-2 hours each\n",
    "#   - API models: ~30-60 min each (with rate limiting)\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# --- LOCAL MODELS (Ollama) ---\n",
    "OLLAMA_MODELS = [\"llama3.2\", \"mistral\"]\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    if model not in str(ollama_models):\n",
    "        print(f\"â­ï¸ Skipping {model} (not in Ollama)\")\n",
    "        continue\n",
    "        \n",
    "    for prompt_type, template in PROMPTS.items():\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"ðŸš€ Running: {model} with {prompt_type} prompt\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        results = run_evaluation_ollama(df, model, prompt_type, template)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out_file = RESULTS_DIR / f\"eval_{model}_{prompt_type}_{timestamp}.csv\"\n",
    "        results.to_csv(out_file, index=False)\n",
    "        print(f\"ðŸ’¾ Saved: {out_file}\")\n",
    "        \n",
    "        metrics = compute_metrics(results)\n",
    "        print_metrics(metrics, model, prompt_type)\n",
    "        \n",
    "        all_metrics.append({\n",
    "            \"model\": model,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            **{k: v for k, v in metrics.items() if k != \"confusion_matrix\"}\n",
    "        })\n",
    "\n",
    "# --- API MODELS ---\n",
    "API_MODELS = {\n",
    "    \"gemini-2.0-flash\": {\"fn\": generate_gemini, \"delay\": 0.5, \"enabled\": bool(GEMINI_API_KEY)},\n",
    "    \"gpt-4o\": {\"fn\": generate_gpt, \"delay\": 0.5, \"enabled\": bool(OPENAI_API_KEY)},\n",
    "    \"claude-3-haiku-20240307\": {\"fn\": generate_claude, \"delay\": 0.5, \"enabled\": bool(ANTHROPIC_API_KEY)},\n",
    "}\n",
    "\n",
    "for model_name, config in API_MODELS.items():\n",
    "    if not config[\"enabled\"]:\n",
    "        print(f\"\\nâ­ï¸ Skipping {model_name} (no API key)\")\n",
    "        continue\n",
    "    \n",
    "    for prompt_type, template in PROMPTS.items():\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"ðŸš€ Running: {model_name} with {prompt_type} prompt\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        results = run_evaluation_api(\n",
    "            df, model_name, config[\"fn\"], \n",
    "            prompt_type, template, \n",
    "            delay=config[\"delay\"]\n",
    "        )\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        safe_name = model_name.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "        out_file = RESULTS_DIR / f\"eval_{safe_name}_{prompt_type}_{timestamp}.csv\"\n",
    "        results.to_csv(out_file, index=False)\n",
    "        print(f\"ðŸ’¾ Saved: {out_file}\")\n",
    "        \n",
    "        metrics = compute_metrics(results)\n",
    "        print_metrics(metrics, model_name, prompt_type)\n",
    "        \n",
    "        all_metrics.append({\n",
    "            \"model\": model_name,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            **{k: v for k, v in metrics.items() if k != \"confusion_matrix\"}\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ FULL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Model Comparison (sorted by F1 score):\n",
      "================================================================================\n",
      "                  model prompt_type  accuracy  precision   recall       f1    kappa  n_valid  n_unclear\n",
      "                mistral         cot  0.853000   0.855131 0.850000 0.852558 0.706000     1000          0\n",
      "                mistral   zero_shot  0.845000   0.900232 0.776000 0.833512 0.690000     1000          0\n",
      "               llama3.2   zero_shot  0.815000   0.769231 0.900000 0.829493 0.630000     1000          0\n",
      "claude-3-haiku-20240307         cot  0.837000   0.939948 0.720000 0.815402 0.674000     1000          0\n",
      "claude-3-haiku-20240307   zero_shot  0.801000   0.951952 0.634000 0.761104 0.602000     1000          0\n",
      "       gemini-2.0-flash         cot  0.789000   0.973770 0.594000 0.737888 0.578000     1000          0\n",
      "       gemini-2.0-flash   zero_shot  0.766767   0.975089 0.548000 0.701665 0.533738      999          1\n",
      "               llama3.2         cot  0.737487   0.910714 0.523614 0.664928 0.473824      979         21\n",
      "                 gpt-4o         cot  0.738431   0.995798 0.477823 0.645777 0.476312      994          6\n",
      "                 gpt-4o   zero_shot  0.721000   1.000000 0.442000 0.613037 0.442000     1000          0\n",
      "\n",
      "ðŸ’¾ Saved to: ..\\Data\\results\\model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 12: MODEL COMPARISON TABLE\n",
    "# ============================================================\n",
    "\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "comparison_df = comparison_df.sort_values(\"f1\", ascending=False)\n",
    "comparison_df.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Model Comparison (sorted by F1 score):\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nðŸ’¾ Saved to:\", RESULTS_DIR / \"model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Error analysis function defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 13: ERROR ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "def analyze_errors(results_file: str, n_samples: int = 5):\n",
    "    \"\"\"Show examples of false positives and false negatives.\"\"\"\n",
    "    df_results = pd.read_csv(results_file)\n",
    "    df_gt = pd.read_csv(DATA_DIR / \"ground_truth_validation_set.csv\")\n",
    "    merged = df_results.merge(df_gt[[\"paper_pmid\", \"paper_abstract\", \"review_title\"]], on=\"paper_pmid\")\n",
    "    \n",
    "    fp = merged[(merged[\"true_label\"] == 0) & (merged[\"prediction\"] == \"include\")]\n",
    "    fn = merged[(merged[\"true_label\"] == 1) & (merged[\"prediction\"] == \"exclude\")]\n",
    "    \n",
    "    print(f\"\\nâŒ False Positives ({len(fp)} total) - wrongly included:\")\n",
    "    for _, row in fp.head(n_samples).iterrows():\n",
    "        print(f\"  â€¢ {row['paper_abstract'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nâŒ False Negatives ({len(fn)} total) - wrongly excluded:\")\n",
    "    for _, row in fn.head(n_samples).iterrows():\n",
    "        print(f\"  â€¢ {row['paper_abstract'][:100]}...\")\n",
    "\n",
    "print(\"âœ… Error analysis function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Best model: mistral (cot)\n",
      "   F1: 0.853, Accuracy: 85.3%\n",
      "\n",
      "ðŸ” Analyzing errors from: eval_mistral_cot_20260128_002807.csv\n",
      "\n",
      "âŒ False Positives (72 total) - wrongly included:\n",
      "  â€¢ Ventilatory muscle function was examined at rest and during exercise on a cycle ergometer in 8 patie...\n",
      "  â€¢ BACKGROUND: Apneic mass movement of oxygen by applying continuous positive airway pressure (CPAP) is...\n",
      "  â€¢ We sought to identify the clinical characteristics and outcomes of patients who had advanced heart f...\n",
      "  â€¢ OBJECTIVES: To investigate the efficacy of a disease-specific Expert Patient Programme (EPP) compare...\n",
      "  â€¢ BACKGROUND: Despite the epidemic rise in obesity, few studies have evaluated the effect of obesity o...\n",
      "\n",
      "âŒ False Negatives (90 total) - wrongly excluded:\n",
      "  â€¢ OBJECTIVE: To assess the effectiveness of nurse led follow up in the management of patients with lun...\n",
      "  â€¢ Breslow (1981, Biometrika 68, 73-84) has shown that the Mantel-Haenszel odds ratio is a consistent e...\n",
      "  â€¢ An existing randomised controlled trial was used to investigate whether multiple ultrasound scans ma...\n",
      "  â€¢ The print component of this fifth 'Article Alerts' installment comprises 100 articles published in 2...\n",
      "  â€¢ This third installment of the 'Article Alerts' feature section highlights 100 articles published in ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 14: ANALYZE BEST MODEL ERRORS\n",
    "# ============================================================\n",
    "\n",
    "# Find the best performing model based on F1 score\n",
    "if not comparison_df.empty:\n",
    "    best_model = comparison_df.iloc[0]\n",
    "    print(f\"ðŸ“ˆ Best model: {best_model['model']} ({best_model['prompt_type']})\")\n",
    "    print(f\"   F1: {best_model['f1']:.3f}, Accuracy: {best_model['accuracy']:.1%}\")\n",
    "    \n",
    "    # Find the corresponding results file\n",
    "    safe_name = best_model['model'].replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "    pattern = f\"eval_{safe_name}_{best_model['prompt_type']}_*.csv\"\n",
    "    result_files = list(RESULTS_DIR.glob(pattern))\n",
    "    \n",
    "    if result_files:\n",
    "        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"\\nðŸ” Analyzing errors from: {latest_file.name}\")\n",
    "        analyze_errors(latest_file)\n",
    "    else:\n",
    "        print(f\"âš ï¸ No results file found matching: {pattern}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No evaluation results available. Run the full evaluation first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
