{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Screening Evaluation Pipeline\n",
    "\n",
    "**Summary:** In this notebook, I evaluate how well open-source LLMs can screen paper abstracts for inclusion in systematic reviews. I test two models (Llama 3.2 and Mistral) with two prompt strategies (zero-shot and chain-of-thought) on a ground-truth validation set of 1,000 labeled abstracts.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Ground Truth Dataset Construction\n",
    "The validation set consists of 1,000 paper-review pairs sampled from Cochrane systematic reviews:\n",
    "- **100 Cochrane reviews** were randomly selected from reviews that have clearly defined inclusion criteria and at least 5 included studies with available abstracts\n",
    "- For each review, I sample **5 \"included\" papers** (papers that were actually included in the review, serving as positive examples) and **5 \"excluded\" papers** (papers that were NOT cited by the review but share the same medical topic, serving as hard negative examples)\n",
    "- The excluded papers are sampled from the broader PubMed corpus based on topic similarity, making them realistic \"near-miss\" candidates that a human screener would need to evaluate\n",
    "- Final dataset: **500 included + 500 excluded = 1,000 labeled pairs**\n",
    "\n",
    "### Prompt Generation\n",
    "Each prompt contains two key components:\n",
    "1. **Review context:** The title of the Cochrane review, which describes the clinical question (e.g., \"Interventions for treating depression after stroke\")\n",
    "2. **Candidate abstract:** The full abstract text of the paper being screened (truncated to 3,000 characters if needed)\n",
    "\n",
    "I test two prompting strategies:\n",
    "\n",
    "**Zero-shot prompt:** A direct instruction asking the LLM to decide INCLUDE or EXCLUDE based on whether the paper is relevant to the review topic. No examples are provided.\n",
    "\n",
    "**Chain-of-thought (CoT) prompt:** The LLM is asked to reason step-by-step before making a decision:\n",
    "1. What is the main topic of this paper?\n",
    "2. Does it relate to the systematic review topic?\n",
    "3. Does it appear to provide relevant evidence?\n",
    "Then it gives a final DECISION: INCLUDE or DECISION: EXCLUDE.\n",
    "\n",
    "### Evaluation Process\n",
    "- I run each model (Llama 3.2, Mistral) with each prompt type (zero-shot, CoT) on all 1,000 samples via Ollama\n",
    "- The LLM's text response is parsed to extract the binary decision\n",
    "- I compute standard classification metrics against the ground truth labels\n",
    "\n",
    "---\n",
    "\n",
    "## Key Results\n",
    "\n",
    "| Model | Prompt | Accuracy | Precision | Recall | F1 |\n",
    "|-------|--------|----------|-----------|--------|-----|\n",
    "| Mistral | cot | 83.6% | 83.6% | 83.7% | 0.837 |\n",
    "| Mistral | zero_shot | 84.5% | 90.8% | 76.8% | 0.832 |\n",
    "| Llama 3.2 | zero_shot | 80.9% | 76.4% | 89.4% | 0.824 |\n",
    "| Llama 3.2 | cot | 73.9% | 90.1% | 52.9% | 0.667 |\n",
    "\n",
    "**Main finding:** Mistral with chain-of-thought prompting achieves the best F1 score (0.837), with balanced precision and recall. Mistral zero-shot has the highest accuracy (84.5%) and precision (90.8%). Llama 3.2 zero-shot has the highest recall (89.4%) but lower precision.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Steps\n",
    "1. I load the ground-truth validation set (500 included, 500 excluded papers)\n",
    "2. I define two prompt templates: zero-shot and chain-of-thought\n",
    "3. I run each LLM on all 1,000 samples using Ollama\n",
    "4. I compute metrics (accuracy, precision, recall, F1, Cohen's kappa)\n",
    "5. I analyze errors and compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: C:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\n",
      "Results directory: C:\\Users\\juanx\\Documents\\LSE-UKHSA Project\\Data\\results\n"
     ]
    }
   ],
   "source": [
    "# I import all libraries needed for evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "\n",
    "DATA_DIR = Path(\"../Data\")\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"Results directory: {RESULTS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,000 records\n",
      "Label distribution: {1: 500, 0: 500}\n",
      "\n",
      "Sample row:\n",
      "{'review_pmid': 21678351, 'review_title': 'Evaluation of follow-up strategies for patients with epithelial ovarian cancer following completion of primary treatment.', 'review_objectives': 'BACKGROUND: Ovarian cancer is the sixth most common cancer and seventh cause of cancer death in women worldwide. Traditionally, many patients who have been treated for cancer undergo long-term follow up in secondary care. Recently however it has been suggested that the use of routine review may not be effective in improving survival, quality of life (QoL), and relieving anxiety. In addition, it may not be cost effective. OBJECTIVES: To compare the potential benefits of different strategies of follow up in women with epithelial ovarian cancer following completion of primary treatment. SEARCH STRATEGY: We searched the Cochrane Gynaecological Cancer Group Trials Register, Cochrane Central Register of Controlled Trials (CENTRAL) (The Cochrane Library, 2010, Issue 4), MEDLINE and EMBASE (to November 2010). We also searched CINAHL, PsycLIT, registers of clinical trials, abstracts of scientific meetings, reference lists of review articles, and contacted experts in the field.', 'review_criteria': 'SELECTION CRITERIA: All relevant randomised controlled trials (RCTs) that evaluated follow-up strategies for patients with epithelial ovarian cancer following completion of primary treatment.', 'paper_pmid': 11737464, 'paper_title': 'A critical evaluation of current protocols for the follow-up of women treated for gynecological malignancies: a pilot study.', 'paper_abstract': 'This retrospective review was undertaken to determine the efficacy of routine follow-up in the detection and management of recurrent cancer. The case notes of all women attending a regional cancer center who were diagnosed with cancer in 1997 were reviewed. Of 81 new cancers followed up for a median of 42 months (range 36-48), 14 have recurred after curative treatment and there were six cases of persistent disease. The median number of clinic visits per patient was 3.5 (range 1-16). Eight recurrences (57.1%) were diagnosed at scheduled outpatient appointments, three (2 l.4%) presented to the general practitioner (GP), and three were seen as emergencies in hospital. Seventeen patients with persistent/recurrent disease have died and three are alive with disease. The median time from initial presentation to disease recurrence was 12 months (range 5-25) and the median time from recurrence to death was 5 months (range 1-20). The longest interval between onset of symptoms and diagnosis of recurrence (4 months) occurred in those presenting at scheduled outpatient clinics. This study demonstrates that the current follow-up protocol is associated with delays in diagnosing recurrence, because symptomatic patients postpone seeking help until their scheduled visit. We have therefore commenced a prospective study evaluating other models of follow-up.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# I load the validation set that contains 1,000 labeled abstracts\n",
    "df = pd.read_csv(DATA_DIR / \"ground_truth_validation_set.csv\")\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "print(f\"\\nSample row:\")\n",
    "print(df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined.\n"
     ]
    }
   ],
   "source": [
    "# I define two prompt templates for the LLMs\n",
    "\n",
    "ZERO_SHOT_TEMPLATE = \"\"\"You are a systematic review screener. Based on the abstract below, decide if this paper should be INCLUDED or EXCLUDED from a systematic review about:\n",
    "\"{review_title}\"\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Answer with exactly one word: INCLUDE or EXCLUDE\"\"\"\n",
    "\n",
    "COT_TEMPLATE = \"\"\"You are a systematic review screener. Your task is to decide if a paper should be included in a systematic review.\n",
    "\n",
    "Review topic: \"{review_title}\"\n",
    "\n",
    "Abstract to screen:\n",
    "{abstract}\n",
    "\n",
    "Think through this step by step:\n",
    "1. What is the main topic of this paper?\n",
    "2. Does it relate to the systematic review topic?\n",
    "3. Does it appear to provide relevant evidence?\n",
    "\n",
    "After your reasoning, give your final answer on a new line as exactly: DECISION: INCLUDE or DECISION: EXCLUDE\"\"\"\n",
    "\n",
    "print(\"Prompt templates defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama functions defined.\n"
     ]
    }
   ],
   "source": [
    "# I define functions to call Ollama and parse LLM responses\n",
    "\n",
    "def call_ollama(model: str, prompt: str, timeout: int = 120) -> str:\n",
    "    \"\"\"Send a prompt to Ollama and return the response text.\"\"\"\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
    "            timeout=timeout\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        return resp.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "def parse_decision(response: str, prompt_type: str) -> str:\n",
    "    \"\"\"Extract INCLUDE/EXCLUDE from LLM response.\"\"\"\n",
    "    text = response.upper()\n",
    "    if prompt_type == \"cot\":\n",
    "        if \"DECISION: INCLUDE\" in text or \"DECISION:INCLUDE\" in text:\n",
    "            return \"include\"\n",
    "        elif \"DECISION: EXCLUDE\" in text or \"DECISION:EXCLUDE\" in text:\n",
    "            return \"exclude\"\n",
    "    if \"INCLUDE\" in text and \"EXCLUDE\" not in text:\n",
    "        return \"include\"\n",
    "    elif \"EXCLUDE\" in text and \"INCLUDE\" not in text:\n",
    "        return \"exclude\"\n",
    "    elif text.strip().startswith(\"INCLUDE\"):\n",
    "        return \"include\"\n",
    "    elif text.strip().startswith(\"EXCLUDE\"):\n",
    "        return \"exclude\"\n",
    "    return \"unclear\"\n",
    "\n",
    "print(\"Ollama functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['mistral:latest', 'llama3.2:latest']\n"
     ]
    }
   ],
   "source": [
    "# I check which models are available in Ollama\n",
    "try:\n",
    "    resp = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    models = [m[\"name\"] for m in resp.json().get(\"models\", [])]\n",
    "    print(f\"Available models: {models}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is running (ollama serve)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "# I define the main evaluation function that runs the LLM on all samples\n",
    "\n",
    "def run_evaluation(df: pd.DataFrame, model: str, prompt_type: str, template: str, limit: int = None):\n",
    "    \"\"\"Run evaluation on the dataset and return results DataFrame.\"\"\"\n",
    "    data = df.head(limit) if limit else df\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model}/{prompt_type}\"):\n",
    "        prompt = template.format(\n",
    "            review_title=row[\"review_title\"],\n",
    "            abstract=row[\"paper_abstract\"][:3000]\n",
    "        )\n",
    "        response = call_ollama(model, prompt)\n",
    "        prediction = parse_decision(response, prompt_type)\n",
    "        \n",
    "        results.append({\n",
    "            \"paper_pmid\": row[\"paper_pmid\"],\n",
    "            \"true_label\": row[\"label\"],\n",
    "            \"prediction\": prediction,\n",
    "            \"raw_response\": response[:500],\n",
    "            \"model\": model,\n",
    "            \"prompt_type\": prompt_type\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics functions defined.\n"
     ]
    }
   ],
   "source": [
    "# I define functions to compute and display metrics\n",
    "\n",
    "def compute_metrics(results_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
    "    valid = results_df[results_df[\"prediction\"].isin([\"include\", \"exclude\"])].copy()\n",
    "    # true_label is already int (1=include, 0=exclude)\n",
    "    y_true = valid[\"true_label\"].astype(int)\n",
    "    y_pred = (valid[\"prediction\"] == \"include\").astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"n_valid\": len(valid),\n",
    "        \"n_unclear\": len(results_df) - len(valid),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics: dict, model: str, prompt_type: str):\n",
    "    \"\"\"Display metrics in a readable format.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model} | Prompt: {prompt_type}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.1%}\")\n",
    "    print(f\"Precision: {metrics['precision']:.1%}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.1%}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.3f}\")\n",
    "    print(f\"Kappa:     {metrics['kappa']:.3f}\")\n",
    "    print(f\"Valid:     {metrics['n_valid']} | Unclear: {metrics['n_unclear']}\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"          Pred Excl  Pred Incl\")\n",
    "    print(f\"True Excl    {cm[0][0]:4d}       {cm[0][1]:4d}\")\n",
    "    print(f\"True Incl    {cm[1][0]:4d}       {cm[1][1]:4d}\")\n",
    "\n",
    "print(\"Metrics functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/zero_shot: 100%|██████████| 10/10 [00:25<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model: llama3.2 | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  80.0%\n",
      "Precision: 80.0%\n",
      "Recall:    80.0%\n",
      "F1 Score:  0.800\n",
      "Kappa:     0.600\n",
      "Valid:     10 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl       4          1\n",
      "True Incl       1          4\n",
      "\n",
      "Quick test complete. Ready for full evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I run a quick test with 10 samples to verify everything works\n",
    "test_results = run_evaluation(df, \"llama3.2\", \"zero_shot\", ZERO_SHOT_TEMPLATE, limit=10)\n",
    "test_metrics = compute_metrics(test_results)\n",
    "print_metrics(test_metrics, \"llama3.2\", \"zero_shot\")\n",
    "print(\"\\nQuick test complete. Ready for full evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "Running: llama3.2 with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/zero_shot: 100%|██████████| 1000/1000 [45:24<00:00,  2.72s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to ..\\Data\\results\\eval_llama3.2_zero_shot_20260116_025453.csv\n",
      "\n",
      "==================================================\n",
      "Model: llama3.2 | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  80.9%\n",
      "Precision: 76.4%\n",
      "Recall:    89.4%\n",
      "F1 Score:  0.824\n",
      "Kappa:     0.618\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     362        138\n",
      "True Incl      53        447\n",
      "\n",
      "############################################################\n",
      "Running: llama3.2 with cot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama3.2/cot: 100%|██████████| 1000/1000 [1:16:43<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to ..\\Data\\results\\eval_llama3.2_cot_20260116_041136.csv\n",
      "\n",
      "==================================================\n",
      "Model: llama3.2 | Prompt: cot\n",
      "==================================================\n",
      "Accuracy:  73.9%\n",
      "Precision: 90.1%\n",
      "Recall:    52.9%\n",
      "F1 Score:  0.667\n",
      "Kappa:     0.476\n",
      "Valid:     975 | Unclear: 25\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     467         28\n",
      "True Incl     226        254\n",
      "\n",
      "############################################################\n",
      "Running: mistral with zero_shot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistral/zero_shot: 100%|██████████| 1000/1000 [55:20<00:00,  3.32s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to ..\\Data\\results\\eval_mistral_zero_shot_20260116_050656.csv\n",
      "\n",
      "==================================================\n",
      "Model: mistral | Prompt: zero_shot\n",
      "==================================================\n",
      "Accuracy:  84.5%\n",
      "Precision: 90.8%\n",
      "Recall:    76.8%\n",
      "F1 Score:  0.832\n",
      "Kappa:     0.690\n",
      "Valid:     1000 | Unclear: 0\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     461         39\n",
      "True Incl     116        384\n",
      "\n",
      "############################################################\n",
      "Running: mistral with cot prompt\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistral/cot: 100%|██████████| 1000/1000 [2:24:01<00:00,  8.64s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to ..\\Data\\results\\eval_mistral_cot_20260116_073058.csv\n",
      "\n",
      "==================================================\n",
      "Model: mistral | Prompt: cot\n",
      "==================================================\n",
      "Accuracy:  83.6%\n",
      "Precision: 83.6%\n",
      "Recall:    83.7%\n",
      "F1 Score:  0.837\n",
      "Kappa:     0.672\n",
      "Valid:     995 | Unclear: 5\n",
      "\n",
      "Confusion Matrix:\n",
      "          Pred Excl  Pred Incl\n",
      "True Excl     415         82\n",
      "True Incl      81        417\n",
      "\n",
      "Full evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I run the full evaluation on all models and prompt types\n",
    "# WARNING: This takes several hours to complete!\n",
    "\n",
    "MODELS = [\"llama3.2\", \"mistral\"]\n",
    "PROMPTS = {\n",
    "    \"zero_shot\": ZERO_SHOT_TEMPLATE,\n",
    "    \"cot\": COT_TEMPLATE\n",
    "}\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for model in MODELS:\n",
    "    for prompt_type, template in PROMPTS.items():\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"Running: {model} with {prompt_type} prompt\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        results = run_evaluation(df, model, prompt_type, template)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out_file = RESULTS_DIR / f\"eval_{model}_{prompt_type}_{timestamp}.csv\"\n",
    "        results.to_csv(out_file, index=False)\n",
    "        print(f\"Saved results to {out_file}\")\n",
    "        \n",
    "        metrics = compute_metrics(results)\n",
    "        print_metrics(metrics, model, prompt_type)\n",
    "        \n",
    "        all_metrics.append({\n",
    "            \"model\": model,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            **{k: v for k, v in metrics.items() if k != \"confusion_matrix\"}\n",
    "        })\n",
    "\n",
    "print(\"\\nFull evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison (sorted by F1 score):\n",
      "   model prompt_type  accuracy  precision   recall       f1    kappa  n_valid  n_unclear\n",
      " mistral         cot  0.836181   0.835671 0.837349 0.836510 0.672361      995          5\n",
      " mistral   zero_shot  0.845000   0.907801 0.768000 0.832069 0.690000     1000          0\n",
      "llama3.2   zero_shot  0.809000   0.764103 0.894000 0.823963 0.618000     1000          0\n",
      "llama3.2         cot  0.739487   0.900709 0.529167 0.666667 0.475573      975         25\n"
     ]
    }
   ],
   "source": [
    "# I create a comparison table of all results\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "comparison_df = comparison_df.sort_values(\"f1\", ascending=False)\n",
    "comparison_df.to_csv(RESULTS_DIR / \"model_comparison.csv\", index=False)\n",
    "\n",
    "print(\"\\nModel Comparison (sorted by F1 score):\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analysis function defined.\n"
     ]
    }
   ],
   "source": [
    "# I define a function to analyze errors and see where models fail\n",
    "\n",
    "def analyze_errors(results_file: str, n_samples: int = 5):\n",
    "    \"\"\"Show examples of false positives and false negatives.\"\"\"\n",
    "    df_results = pd.read_csv(results_file)\n",
    "    df_gt = pd.read_csv(DATA_DIR / \"ground_truth_validation_set.csv\")\n",
    "    merged = df_results.merge(df_gt[[\"paper_pmid\", \"paper_abstract\", \"review_title\"]], on=\"paper_pmid\")\n",
    "    \n",
    "    # true_label is int (1=include, 0=exclude), prediction is string\n",
    "    fp = merged[(merged[\"true_label\"] == 0) & (merged[\"prediction\"] == \"include\")]\n",
    "    fn = merged[(merged[\"true_label\"] == 1) & (merged[\"prediction\"] == \"exclude\")]\n",
    "    \n",
    "    print(f\"\\nFalse Positives ({len(fp)} total):\")\n",
    "    for _, row in fp.head(n_samples).iterrows():\n",
    "        print(f\"  - {row['paper_abstract'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nFalse Negatives ({len(fn)} total):\")\n",
    "    for _, row in fn.head(n_samples).iterrows():\n",
    "        print(f\"  - {row['paper_abstract'][:100]}...\")\n",
    "\n",
    "print(\"Error analysis function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: eval_mistral_zero_shot_20260116_050656.csv\n",
      "\n",
      "False Positives (39 total):\n",
      "  - BACKGROUND: Apneic mass movement of oxygen by applying continuous positive airway pressure (CPAP) is...\n",
      "  - BACKGROUND: The associations between homocysteine, B vitamin status, and pregnancy outcomes have not...\n",
      "  - We sought to identify the clinical characteristics and outcomes of patients who had advanced heart f...\n",
      "  - OBJECTIVES: To investigate the efficacy of a disease-specific Expert Patient Programme (EPP) compare...\n",
      "  - BACKGROUND: Despite the epidemic rise in obesity, few studies have evaluated the effect of obesity o...\n",
      "\n",
      "False Negatives (131 total):\n",
      "  - BACKGROUND: Positron emission tomography-computed tomography (PET-CT) is currently not established i...\n",
      "  - OBJECTIVE: To assess the effectiveness of nurse led follow up in the management of patients with lun...\n",
      "  - Interrater reliability assessments were undertaken for the Hamilton Depression Rating Scale, the Ras...\n",
      "  - In an open study the cholinolytic biperiden was administered to 10 severely depressed inpatients in ...\n",
      "  - An open field trial was conducted comparing desipramine and an active placebo in separate population...\n"
     ]
    }
   ],
   "source": [
    "# I analyze errors for the best-performing model (Mistral CoT had best F1, but Mistral zero-shot had highest precision)\n",
    "result_files = list(RESULTS_DIR.glob(\"eval_mistral_zero_shot_*.csv\"))\n",
    "if result_files:\n",
    "    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"Analyzing: {latest_file.name}\")\n",
    "    analyze_errors(latest_file)\n",
    "else:\n",
    "    print(\"No Mistral zero-shot results found. Run the full evaluation first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
