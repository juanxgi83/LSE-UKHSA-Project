{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e16de6",
   "metadata": {},
   "source": [
    "# 08 — Agentic Approaches: Summary\n",
    "\n",
    "This notebook briefly describes the four agentic strategies used in the unified LLM evaluation pipeline (Notebook 07) to improve systematic review screening beyond zero-shot baselines.\n",
    "\n",
    "All strategies were evaluated on **100% of the Cochrane Public Health** ground-truth dataset (no sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e4f75",
   "metadata": {},
   "source": [
    "---\n",
    "## Baseline: Zero-Shot Prompting\n",
    "\n",
    "Each model receives the review scope and the candidate paper, then responds with a single word: INCLUDE or EXCLUDE. No examples, no reasoning chain.\n",
    "\n",
    "- **Best model:** llama3.1:8b — F1 = 0.762, Precision = 0.828, Recall = 0.707\n",
    "- **Problem:** Misses 29.3% of relevant papers (false-negative rate too high for systematic reviews)\n",
    "- CoT was tested but consistently underperformed zero-shot, so was dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94127fd9",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 1: Dynamic Few-Shot Prompting\n",
    "\n",
    "For each candidate paper, inject **review-specific examples** (2 included + 2 excluded papers from the same review) into the prompt. The model learns the inclusion boundary for that particular review topic.\n",
    "\n",
    "- **Result:** F1 = 0.746, Precision = 0.636, **Recall = 0.900** (+0.193 vs baseline)\n",
    "- **Cost:** 1 LLM call per paper (longer prompt)\n",
    "- **Strength:** Massive recall boost — the model stops being overly conservative\n",
    "- **Weakness:** Precision drops because the model over-includes borderline papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67aab6",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 2: Smart Ensemble (OR / Majority / Weighted Vote)\n",
    "\n",
    "Combine Phase 1 predictions from all 7 models using voting rules — **zero extra LLM calls**.\n",
    "\n",
    "| Rule | Logic | F1 | Prec | Rec |\n",
    "|------|-------|-----|------|-----|\n",
    "| **OR-rule** | Any model says INCLUDE → INCLUDE | 0.761 | 0.703 | 0.829 |\n",
    "| **Majority** | >50% of models agree | 0.731 | 0.867 | 0.631 |\n",
    "| **Weighted** | Vote weighted by each model's F1 | 0.731 | 0.867 | 0.631 |\n",
    "\n",
    "- **Strength:** Free — reuses existing predictions\n",
    "- **Weakness:** Models are highly correlated (70–78% error overlap), limiting diversity gains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72491fd",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 3: Calibrated Recall Challenge - Best Overall\n",
    "\n",
    "The most **surgical** strategy. For each paper the best model excluded, check how many other models disagreed. If ≥2 others said INCLUDE, send the paper to a challenger model with a **recall-biased reconsideration prompt** that asks it to look for *any* reasonable argument for relevance.\n",
    "\n",
    "**Flow:**\n",
    "1. Start with best model's predictions (llama3.1:8b)\n",
    "2. For each EXCLUDE → count how many other models said INCLUDE\n",
    "3. If ≥2 disagree → challenge with mistral-nemo:12b using recall-biased prompt\n",
    "4. If challenger says INCLUDE → flip; otherwise keep EXCLUDE\n",
    "\n",
    "**Key numbers:**\n",
    "- Only **94 out of ~3,500 excludes** were challenged (2.7%)\n",
    "- **85 flipped** to INCLUDE (90.4% flip rate)\n",
    "- **F1 = 0.786** (+0.024), Recall = 0.776 (+0.069), Precision = 0.797 (−0.031)\n",
    "- Extra compute: **4.2 minutes** (94 LLM calls instead of 4,000)\n",
    "\n",
    "This was the **best strategy overall** — highest F1 with minimal precision cost and negligible extra compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88f0ac",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 4: Few-Shot Debate with Judge\n",
    "\n",
    "Two screener models each evaluate every paper using the few-shot prompt. If they **agree**, that decision is final. If they **disagree**, a third judge model resolves the conflict using a prompt that favours inclusion.\n",
    "\n",
    "**Flow:**\n",
    "1. Screener A (llama3.1:8b) + Screener B (mistral-nemo:12b) both screen with few-shot prompts\n",
    "2. Agree → take that decision (90.9% of cases)\n",
    "3. Disagree → Judge (mistral) decides, biased toward INCLUDE\n",
    "\n",
    "- **Result:** F1 = 0.738, Precision = 0.617, **Recall = 0.917** (highest recall of all strategies)\n",
    "- **Cost:** 2–3 LLM calls per paper (67 minutes total)\n",
    "- **Strength:** Highest recall — misses only 8.3% of relevant papers\n",
    "- **Weakness:** Precision suffers; many false positives from the judge's inclusion bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ef149",
   "metadata": {},
   "source": [
    "---\n",
    "## Automated Strategy Selection (Phase 2 Diagnosis)\n",
    "\n",
    "The pipeline **automatically selects** which strategies to run based on Phase 1 diagnostics:\n",
    "\n",
    "| Diagnostic Signal | Threshold | Triggers |\n",
    "|-------------------|-----------|----------|\n",
    "| Precision − Recall gap > 0.05 | Bottleneck = recall | Calibrated Recall Challenge |\n",
    "| Cross-model rescue potential > 25% | FN are recoverable | Calibrated Recall Challenge |\n",
    "| Inter-model disagreement > 4% | Models are diverse | Few-Shot Debate |\n",
    "| *(always)* | — | Dynamic Few-Shot, Smart Ensemble |\n",
    "\n",
    "In our run: gap = 0.121 (recall bottleneck), rescue = 41.6%, disagreement = 6.0% → **all 4 strategies were activated**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64782cce",
   "metadata": {},
   "source": [
    "---\n",
    "## Full Results Comparison\n",
    "\n",
    "| Approach | F1 | Precision | Recall | Extra Cost |\n",
    "|----------|-----|-----------|--------|------------|\n",
    "| Baseline: llama3.1:8b (zero-shot) | 0.762 | 0.828 | 0.707 | — |\n",
    "| **Calibrated Recall Challenge** | **0.786** | **0.797** | **0.776** | **94 calls (4 min)** |\n",
    "| Ensemble: OR-rule | 0.761 | 0.703 | 0.829 | 0 calls |\n",
    "| Dynamic Few-Shot | 0.746 | 0.636 | 0.900 | 4,000 calls (27 min) |\n",
    "| Few-Shot Debate | 0.738 | 0.617 | 0.917 | 8,000–12,000 calls (67 min) |\n",
    "| Ensemble: Majority / Weighted | 0.731 | 0.867 | 0.631 | 0 calls |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Calibrated Recall Challenge wins on F1** — best balance of precision and recall with minimal compute\n",
    "2. **Few-Shot Debate wins on recall** (0.917) — best for minimising missed papers, but expensive and imprecise\n",
    "3. **Ensembles are free** but limited by high inter-model error correlation\n",
    "4. **Dynamic Few-Shot** provides the biggest single-strategy recall boost but trades too much precision\n",
    "5. The **automated diagnosis** correctly identified recall as the bottleneck and activated all relevant strategies without human intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad78efc",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "- The **zero-shot baseline** (llama3.1:8b) achieved F1 = 0.762 but missed nearly 30% of relevant papers — unacceptable for systematic reviews\n",
    "- **Dynamic Few-Shot** injects review-specific examples into the prompt, boosting recall to 0.900 but sacrificing precision\n",
    "- **Smart Ensembles** combine the predictions already produced by all 7 models during Phase 1, requiring no additional LLM calls; the OR-rule matched baseline F1 while raising recall to 0.829\n",
    "- **Calibrated Recall Challenge** was the best overall strategy (F1 = 0.786) — it surgically re-evaluated only 94 low-confidence excludes, flipping 85 to INCLUDE, at a cost of just 4 extra minutes\n",
    "- **Few-Shot Debate** achieved the highest recall (0.917) by having two screeners + a judge, but at 3× the compute and with reduced precision\n",
    "- The pipeline's **automated diagnosis** correctly identified recall as the bottleneck and activated all four strategies without manual intervention\n",
    "- The key insight is that **targeted, confidence-aware interventions** (Calibrated Recall Challenge) outperform brute-force approaches (re-running everything with a different prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
