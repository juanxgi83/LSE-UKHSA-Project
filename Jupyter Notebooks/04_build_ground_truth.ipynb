{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead6a0ff",
   "metadata": {},
   "source": [
    "# 04: Build Ground Truth Validation Dataset\n",
    "\n",
    "## Summary\n",
    "This notebook builds the ground truth validation dataset for LLM evaluation. It uses the categorized references extracted from Cochrane PDFs to create properly labeled examples:\n",
    "- **Included studies** → Positive examples (label=1)\n",
    "- **Excluded studies** → Negative examples (label=0)\n",
    "\n",
    "Unlike the previous flawed approach using PubMed references, this dataset has verified labels based on actual Cochrane reviewer decisions.\n",
    "\n",
    "**Pipeline Position:** Fifth notebook - creates the evaluation dataset.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads categorized references from previous extraction\n",
    "2. Matches study IDs to PubMed IDs for abstract retrieval\n",
    "3. Fetches abstracts for matched studies\n",
    "4. Creates balanced training/validation sets\n",
    "5. Saves ground truth CSV with labels\n",
    "\n",
    "**Input:** `Data/categorized_references.csv`, `Data/cochrane_pubmed_abstracts.csv`\n",
    "\n",
    "**Output:** `Data/ground_truth_validation_set.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d54616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q biopython pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91701426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment and load data\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import time\n",
    "import re\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir if (notebook_dir / \".env\").exists() else notebook_dir.parent\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "Entrez.email = os.getenv(\"NCBI_EMAIL\", \"\")\n",
    "Entrez.api_key = os.getenv(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "DATA_DIR = project_root / \"Data\"\n",
    "REFS_CSV = DATA_DIR / \"categorized_references.csv\"\n",
    "ABSTRACTS_CSV = DATA_DIR / \"cochrane_pubmed_abstracts.csv\"\n",
    "OUTPUT_CSV = DATA_DIR / \"ground_truth_validation_set.csv\"\n",
    "\n",
    "print(f\"Loading categorized references...\")\n",
    "refs = pd.read_csv(REFS_CSV)\n",
    "print(f\"Total references: {len(refs):,}\")\n",
    "print(refs['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to search PubMed for study by author and year\n",
    "def search_pubmed_for_study(study_id: str) -> list:\n",
    "    \"\"\"Search PubMed for a study given Author Year format.\"\"\"\n",
    "    parts = study_id.split()\n",
    "    if len(parts) < 2:\n",
    "        return []\n",
    "    \n",
    "    author = parts[0]\n",
    "    year = re.search(r'\\d{4}', study_id)\n",
    "    if not year:\n",
    "        return []\n",
    "    year = year.group()\n",
    "    \n",
    "    query = f\"{author}[Author] AND {year}[Date - Publication]\"\n",
    "    \n",
    "    try:\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=5)\n",
    "        results = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        return results.get('IdList', [])\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Test with a sample\n",
    "sample_study = refs['study_id'].iloc[0]\n",
    "print(f\"Testing search for: {sample_study}\")\n",
    "pmids = search_pubmed_for_study(sample_study)\n",
    "print(f\"Found PMIDs: {pmids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match study IDs to PMIDs (sample for efficiency)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Filter to included and excluded only (skip awaiting)\n",
    "refs_for_matching = refs[refs['category'].isin(['included', 'excluded'])].copy()\n",
    "unique_studies = refs_for_matching['study_id'].unique()\n",
    "\n",
    "print(f\"Unique studies to match: {len(unique_studies):,}\")\n",
    "\n",
    "# Sample for manageable processing (adjust as needed)\n",
    "MAX_STUDIES = 2000\n",
    "if len(unique_studies) > MAX_STUDIES:\n",
    "    import numpy as np\n",
    "    np.random.seed(42)\n",
    "    unique_studies = np.random.choice(unique_studies, MAX_STUDIES, replace=False)\n",
    "    print(f\"Sampled to {MAX_STUDIES} studies\")\n",
    "\n",
    "# Search PubMed for each study\n",
    "study_to_pmid = {}\n",
    "for study_id in tqdm(unique_studies, desc=\"Matching to PubMed\"):\n",
    "    pmids = search_pubmed_for_study(study_id)\n",
    "    if pmids:\n",
    "        study_to_pmid[study_id] = pmids[0]  # Take first match\n",
    "    time.sleep(0.4)  # Rate limiting\n",
    "\n",
    "print(f\"\\nMatched {len(study_to_pmid):,} studies to PMIDs ({100*len(study_to_pmid)/len(unique_studies):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46123278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch abstracts for matched studies\n",
    "from io import StringIO\n",
    "from Bio import Medline\n",
    "\n",
    "def fetch_abstracts(pmids: list, batch_size: int = 50) -> dict:\n",
    "    \"\"\"Fetch abstracts for a list of PMIDs.\"\"\"\n",
    "    abstracts = {}\n",
    "    \n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch = pmids[i:i+batch_size]\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(batch), rettype=\"medline\", retmode=\"text\")\n",
    "            text = handle.read()\n",
    "            handle.close()\n",
    "            \n",
    "            for record in Medline.parse(StringIO(text)):\n",
    "                pmid = record.get(\"PMID\", \"\")\n",
    "                abstracts[pmid] = {\n",
    "                    'title': record.get(\"TI\", \"\"),\n",
    "                    'abstract': record.get(\"AB\", \"\"),\n",
    "                    'year': record.get(\"DP\", \"\").split()[0] if record.get(\"DP\") else \"\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch: {e}\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return abstracts\n",
    "\n",
    "pmids_to_fetch = list(study_to_pmid.values())\n",
    "print(f\"Fetching abstracts for {len(pmids_to_fetch)} studies...\")\n",
    "abstract_data = fetch_abstracts(pmids_to_fetch)\n",
    "print(f\"Retrieved {len(abstract_data)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f49d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ground truth dataset with labels\n",
    "ground_truth_rows = []\n",
    "\n",
    "# Get category for each study\n",
    "study_categories = refs_for_matching.groupby('study_id')['category'].first().to_dict()\n",
    "\n",
    "for study_id, pmid in study_to_pmid.items():\n",
    "    if pmid not in abstract_data:\n",
    "        continue\n",
    "    \n",
    "    info = abstract_data[pmid]\n",
    "    if not info.get('abstract'):  # Skip if no abstract\n",
    "        continue\n",
    "    \n",
    "    category = study_categories.get(study_id, 'unknown')\n",
    "    label = 1 if category == 'included' else 0\n",
    "    \n",
    "    # Get the review DOI for context\n",
    "    review_doi = refs_for_matching[refs_for_matching['study_id'] == study_id]['review_doi'].iloc[0]\n",
    "    \n",
    "    ground_truth_rows.append({\n",
    "        'study_pmid': pmid,\n",
    "        'study_id': study_id,\n",
    "        'review_doi': review_doi,\n",
    "        'title': info['title'],\n",
    "        'abstract': info['abstract'],\n",
    "        'year': info['year'],\n",
    "        'category': category,\n",
    "        'label': label\n",
    "    })\n",
    "\n",
    "ground_truth = pd.DataFrame(ground_truth_rows)\n",
    "print(f\"Ground truth dataset: {len(ground_truth):,} examples\")\n",
    "print(ground_truth['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset if needed\n",
    "included = ground_truth[ground_truth['label'] == 1]\n",
    "excluded = ground_truth[ground_truth['label'] == 0]\n",
    "\n",
    "print(f\"Before balancing: {len(included)} included, {len(excluded)} excluded\")\n",
    "\n",
    "# Undersample the majority class to match minority\n",
    "min_count = min(len(included), len(excluded))\n",
    "if len(included) != len(excluded):\n",
    "    included_sample = included.sample(n=min_count, random_state=42)\n",
    "    excluded_sample = excluded.sample(n=min_count, random_state=42)\n",
    "    ground_truth_balanced = pd.concat([included_sample, excluded_sample]).sample(frac=1, random_state=42)\n",
    "else:\n",
    "    ground_truth_balanced = ground_truth\n",
    "\n",
    "print(f\"After balancing: {len(ground_truth_balanced)} total examples\")\n",
    "print(ground_truth_balanced['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ground truth dataset\n",
    "ground_truth_balanced.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved ground truth to {OUTPUT_CSV}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GROUND TRUTH SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total examples: {len(ground_truth_balanced):,}\")\n",
    "print(f\"  Included (label=1): {(ground_truth_balanced['label'] == 1).sum():,}\")\n",
    "print(f\"  Excluded (label=0): {(ground_truth_balanced['label'] == 0).sum():,}\")\n",
    "print(f\"Unique reviews: {ground_truth_balanced['review_doi'].nunique()}\")\n",
    "print(f\"\\nNext step: Run notebook 05 to evaluate local LLMs\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
