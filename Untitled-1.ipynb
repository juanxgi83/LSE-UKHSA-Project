{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ce7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# REQUIRED: set your email and optionally an NCBI API key\n",
    "Entrez.email = \"your_email@example.com\"  # replace with your email\n",
    "Entrez.api_key = \"\"  # optional but recommended\n",
    "\n",
    "# Query to target Cochrane Database of Systematic Reviews abstracts\n",
    "QUERY = '(\"Cochrane Database Syst Rev\"[Journal]) AND hasabstract[text]'\n",
    "OUT_CSV = Path.cwd() / \"cochrane_pubmed_abstracts.csv\"\n",
    "\n",
    "# Tuning\n",
    "BATCH_SIZE = 200        # efetch batch size (200 is safe)\n",
    "SLEEP = 0.34            # ~3 req/sec with API key; use ~0.9 without a key\n",
    "MAX_RECORDS = 200       # set to None to pull everything; keep small for a test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esearch_all(query: str):\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, usehistory=\"y\", retmax=0)\n",
    "    record = Entrez.read(handle)\n",
    "    count = int(record[\"Count\"])\n",
    "    return count, record[\"WebEnv\"], record[\"QueryKey\"]\n",
    "\n",
    "\n",
    "def efetch_medline_batches(count: int, webenv: str, query_key: str, batch_size: int, max_records=None):\n",
    "    limit = count if max_records is None else min(count, max_records)\n",
    "    for start in range(0, limit, batch_size):\n",
    "        handle = Entrez.efetch(\n",
    "            db=\"pubmed\",\n",
    "            rettype=\"medline\",\n",
    "            retmode=\"text\",\n",
    "            webenv=webenv,\n",
    "            query_key=query_key,\n",
    "            retstart=start,\n",
    "            retmax=batch_size,\n",
    "        )\n",
    "        yield handle.read()\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "\n",
    "def medline_to_rows(medline_text: str):\n",
    "    from io import StringIO\n",
    "    from Bio import Medline\n",
    "\n",
    "    for record in Medline.parse(StringIO(medline_text)):\n",
    "        yield {\n",
    "            \"pmid\": record.get(\"PMID\", \"\"),\n",
    "            \"title\": record.get(\"TI\", \"\"),\n",
    "            \"abstract\": record.get(\"AB\", \"\"),\n",
    "            \"journal\": record.get(\"JT\", \"\"),\n",
    "            \"year\": record.get(\"DP\", \"\").split(\" \")[0],\n",
    "            \"authors\": \"; \".join(record.get(\"AU\", [])),\n",
    "        }\n",
    "\n",
    "\n",
    "def write_pubmed_to_csv(query: str, out_path: Path, batch_size: int, max_records=None):\n",
    "    if not Entrez.email or \"example.com\" in Entrez.email:\n",
    "        raise ValueError(\"Set Entrez.email to your email before running.\")\n",
    "\n",
    "    count, webenv, query_key = esearch_all(query)\n",
    "    limit = count if max_records is None else min(count, max_records)\n",
    "    print(f\"Found {count} records; fetching {limit}...\")\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(\n",
    "            f, fieldnames=[\"pmid\", \"title\", \"abstract\", \"journal\", \"year\", \"authors\"]\n",
    "        )\n",
    "        writer.writeheader()\n",
    "\n",
    "        for medline_chunk in efetch_medline_batches(limit, webenv, query_key, batch_size, max_records=limit):\n",
    "            for row in medline_to_rows(medline_chunk):\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f\"Saved to {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the download; set MAX_RECORDS=None to pull everything\n",
    "write_pubmed_to_csv(QUERY, OUT_CSV, BATCH_SIZE, max_records=MAX_RECORDS)\n",
    "\n",
    "# Quick peek at the first few rows\n",
    "import pandas as pd\n",
    "pd.read_csv(OUT_CSV).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
